---
title: "GEOG 6960 Causality in Geog. Studies 8"
author: 
  - name: "Simon Brewer"
    email: simon.brewer@ess.utah.edu
    affiliations:
      - name: University of Utah
        address: 260 Central Campus Drive
        city: Salt Lake City
        state: UT
        postal-code: 84112
date: last-modified
format:
  html:
    toc: true
editor: visual
---

```{r}
#| echo: false
set.seed(42)
library(reticulate)
use_condaenv("causal")
```

## Introduction

In this lab, we're going to test methods for causal discovery. We'll use a synthetic example, and a quick test with the Grace and Keeley fire/plant abundance dataset (*keeley.csv*).

## Causal discovery

There are a range of packages with algorithms for causal discovery in both R and Python. Here, we'll use:

- R: **causalDisco** - this packages builds on and integrates functions from a set of other packages, mainly from the Bioconductor repository. Installation instructions are given below. 
- Python: **causal-learn** - we'll this as it is part of a larger Python ecosystem for causal analysis. Other packages include **gcastle** and **cdt**, the Causal Discovery Toolbox 

First load (or install and load) the relevant packages. We'll need some additional packages to explore the data before model building.

::: {.panel-tabset group="language"}
# R

The **causalDisco** library has a fairly large number of dependencies, including packages that are not part of the standard CRAN system. You'll need to run (at least) the following commands *after* installing **causalDisco** to get all the appropriate functions. These are all part of the Bioconductor repository to be installed first from here:

https://www.bioconductor.org/

Once you've installed this, install the following:

- `BiocManager::install("graph")`
- `BiocManager::install("RBGL")`
- `BiocManager::install("Rgraphviz")`

Now try loading the packages. If you get errors, please let me know

```{r}
#| output: false
library(tidyverse)
library(ggpubr)
library(dagitty)
library(ggdag)
library(GGally)
library(causalDisco)
library(pcalg)
```

# Python

```{python}
import numpy as np
import pandas as pd
import seaborn as sns
```
:::

## Example data

First, we're going to create a synthetic dataset, based on the following 'true' DAG. This is the structure that we will try to discover later on:

```{r echo = FALSE}
dag <- dagitty('dag {
bb="0,0,1,1"
A [pos="0.150,0.250"]
B [pos="0.150,0.650"]
C [pos="0.450,0.450"]
D [pos="0.750,0.450"]
A -> C
B -> C
C -> D
}
'
)
# plot(dag)
ggdag(dag) + theme_dag()
```

Note that this implies the following adjacency matrix, where each row represents an origin node, and each column a destination node:

$$
A_{G} =
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

This will have 4 variables, which we will create as follows:

- `a` a randomly distributed exogenous variable: $a ~ N(0, 1)$
- `b` a randomly distributed exogenous variable: $a ~ N(0, 1)$
- `c` an endogenous variable 'caused' by `a` and `b`: $c = 0.3\times a + 0.2 \times b + N(0, 0.01)$
- `d` the outcome variable 'caused' by `c`: $d = 0.8 \times c + N(0, 0.01)$

If you want to make this more complex, uncomment the line that creates the variable `e`. This will add an additional collider to the DAG. 

::: {.panel-tabset group="language"}
# R

```{r}
set.seed(1)
n <- 10000
a <- rnorm(n) 
b <- rnorm(n) 
c <- 0.3*a + 0.2*b + rnorm(n, 0, 0.01)
d <- 0.8*c + rnorm(n, 0, 0.01)
# e <- -0.4*a + -0.4*d + rnorm(n, 0, 0.01)
df <- data.frame(a,b,c,d)
```

# Python

```{python}
np.random.seed(1)
n = 10000
a = np.random.normal(0, 1, n)
b = np.random.normal(0, 1, n)
c = 0.3*a + 0.2*b + np.random.normal(0, 0.01, n)
d = 0.8*c + np.random.normal(0, 0.01, n)

df = pd.DataFrame({'a': a,
                   'b': b,
                   'c': c,
                   'd': d})
```
:::

As usual, we'll do a little exploration of the data before moving on. 

::: {.panel-tabset group="language"}
# R

```{r}
ggpairs(df)
```

# Python

```{python}
sns.pairplot(df)
```
:::

And here's the covariance and correlation matrices:

::: {.panel-tabset group="language"}
# R

```{r}
# Covariance
cov(df)
```

```{r}
# Correlation
cor(df)
```

# Python

```{python}
df.cov()
```

```{python}
df.corr()
```
:::

## Peters-Clark algorithm

We'll first estimate the causal graph using the Peters-Clark (PC) algorithm. This starts by creating a fully connected, but undirected graph. Then:

- Edges are removed between variables that are unconditionally independent (i.e. no existing covariance)
- Edges are removed between variables that are conditionally independent (i.e. no existing covariance given other nodes)
- Colliders are identified and directed
- The direction of remaining links is established

::: {.panel-tabset group="language"}
# R

In R, the function we will use is `pc()`. Before running this, we need to create a list with summary statistics for the algorithm. For PC, we need the correlation matrix and the number of observations in the dataset. These will be used to assess the first step:

```{r}
df_stats <- list(C = cor(df), n = nrow(df))
```

With this, we can then run the PC algorithm. This takes as arguments:

- The list of summary statistics
- A predefined function to test for independence (`gaussCItest` for normally distributed variables, other functions exist for discrete or binary data)
- The threshold for inclusion (correlations below this threshold will be excluded)

```{r}
df_pc <- pc(df_stats, labels = names(df),
            indepTest = gaussCItest, alpha = 0.01)
```

# Python

```{python}
df.cov()
```

:::

Let's examine the output:

::: {.panel-tabset group="language"}
# R

The 'discovered' adjacency matrix is shown in the `summary()` output. Compare this to the known matrix above.

```{r}
summary(df_pc)
```

You can also visualize the resulting graph:

```{r message=FALSE, warning=FALSE}
plot(df_pc@graph)
```

# Python

```{python}
df.cov()
```

:::


## Greedy Equivalence Search algorithm

Next we'll use a greedy search method to find the structure, the Greedy Equivalence Search (GES) algorithm. WStarting from a graph with all the nodes but no edges, this will iterate through three stages:

- Forwards: edges are added until no further improvement is obtained
- Backwards: edges are removed until no further improvement is obtained
- Turning: edges are reversed until no further improvement is obtained

The *improvement* is measured by a score, in this case the Bayesian Information Criterion (BIC). 

::: {.panel-tabset group="language"}
# R

In R, this requires first setting up the score function. `GaussL0penObsScore` is a general function for this, which by default will estimate the Bayesian Information Criterion. This also takes an argument (`lambda`) to change the weighting (default is $log(n)/2$). Setting this to higher values will penalize against more complex graphs. 

```{r}
score <- new("GaussL0penObsScore", df)
```

Now we can estimate the causal structure using the `ges()` function (setting `verbose=TRUE` displays the progress through the different steps described above):

```{r}
df_ges <- ges(score, verbose = TRUE)
```

The resulting object contains the 'discovered' graph:

Which can be convert to an adjacency matrix for comparison:

```{r}
as(as(df_ges$essgraph,"graphNEL"),"Matrix")
```

And visualized:

```{r}
plot(df_ges$essgraph)
```

# Python

```{python}
df.cov()
```

:::

## Causal discovery of Keeley and Grace

Let's finish by running one of these algorithms with a real dataset. We'll use the Grace and Keeley data set again here. First load it (and remove the elevation column):

::: {.panel-tabset group="language"}

# R

```{r}
keeley <- read.csv("data/keeley.csv")
keeley <- keeley %>%
    select(-elev)
```

# Python

```{python}
df.cov()
```

:::

As a reminder, the DAG published in the 2009 paper looks like this:

```{r echo=FALSE}
grace_dag <- dagify(age ~ distance,
                    hetero ~ distance,
                    abiotic ~ distance,
                    firesev ~ age,
                    cover ~ firesev,
                    rich ~ cover + hetero + abiotic + distance,
                    coords = list(x = c(distance = 1,
                                        age = 2,
                                        firesev = 3,
                                        hetero = 3,
                                        abiotic = 3,
                                        cover = 4,
                                        rich = 5), 
                                  y = c(distance = 2,
                                        age = 1,
                                        firesev = 1,
                                        hetero = 3,
                                        abiotic = 4,
                                        cover = 1,
                                        rich = 2)
                    ),
                    exposure = "firesev",
                    outcome = "rich"
)

ggdag(grace_dag) +
  theme_dag()
```

So, let's see how close the PC algorithm comes to this:

::: {.panel-tabset group="language"}

# R

```{r}
keeley_stats <- list(C = cor(keeley), 
                     n = nrow(keeley))
keeley_pc <- pc(keeley_stats, labels = names(keeley),
                indepTest = gaussCItest, alpha = 0.01)
plot(keeley_pc@graph)
```

# Python

```{python}
df.cov()
```

:::

The results are a bit of a mixed bag. The returned graph is split into two. In one, the richness is correctly identified as being caused by three of the variables. However, the chain linking age to fire severity and richness, while identified, is not linked. 

## Summary

The final results show that caution is required when applying these methods. These cannot incorporate domain knowledge and so can only find the optimal structure *according to the rules of the algorithm*. With that in mind, these can provide a useful first pass through the data that can then be modified using theory and domain expertise. 


