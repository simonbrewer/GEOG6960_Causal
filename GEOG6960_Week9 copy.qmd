---
title: "GEOG 6960 Causality in Geog. Studies 10"
author: 
  - name: "Simon Brewer"
    email: simon.brewer@ess.utah.edu
    affiliations:
      - name: University of Utah
        address: 260 Central Campus Drive
        city: Salt Lake City
        state: UT
        postal-code: 84112
date: last-modified
format:
  html:
    toc: true
editor: visual
---

```{r}
#| echo: false
set.seed(42)
library(reticulate)
use_condaenv("causal")
```

## Introduction

In this lab, we're going to take a quick look at working with Bayesian networks. These are probabilistic graph models, that overlap with path models and structural equation models. The main difference is that these are based on Bayesian theory, which makes it possible to include prior information and provides probabilistic estimates that can help understand the uncertainty in a model. Note that these are not uniquely causal models - these can be used simply to model the relationships between a set of observations. However, in the absence of confounders (i.e. all known ones are accounted for), then these can be used for causal analysis.

Bayesian networks are developed in two steps: structure learning and parameter learning. Both of these can incorporate expert knowledge, and we'll see examples of this below.

We'll use two datasets:

-   Travel survey data
-   Keeley and Grace fire data set (*keeley.csv*)

## Bayesian networks

Both R and Python have several packages that allow you to create SEMs and estimate coefficients based on a dataset. We'll use **bnlearn** for both here. Other packages include:

-   R: **BayesianNetwork** and **gRain**
-   Python: **PyMC3** and **PyBN**

First load (or install and load) the relevant packages. We'll need some additional packages to explore the data before model building.

::: {.panel-tabset group="language"}
# R

```{r}
#| output: false
library(tidyverse)
library(GGally)
library(bnlearn)
library(gRain)
```

# Python

```{python}
import numpy as np
import pandas as pd
```
:::

## Discrete Bayesian Networks

We'll start with an example of a discrete Bayesian Network. In these networks, all variables are multinomial (i.e. binary or multi-category). This facilitates the analysis as all probability distributions can be described as simple conditional probability tables (more on this below).

### Data

Let's now load the data. The dataset we'll use represents information from a travel survey. This includes the following variables (listed with their categories):

-   Age (`A`): adult/young/old
-   Size of employment region (`R`): big/small
-   Education (`E`): high/uni
-   Occupation (`O`): emp/self
-   Sex (`S`): F/M
-   Travel mode (`T`): car/train/other

::: {.panel-tabset group="language"}
# R

```{r}
travel <- read.csv("./data/survey.txt", header = TRUE)
head(travel)
```

# Python

```{python}
np.random.seed(42)
n = 100
S = np.random.normal(6, 2, n)
C = np.random.choice([0,1], n, replace=True)
W = 10 + 4 * S + 5 * (S*C) + np.random.normal(0, 5, n)
T = 50 + 20 * W + np.random.normal(0, 100, n)
# e = -0.4*a + -0.4*d + np.random.normal(0, 0.01, n)

df = pd.DataFrame({'S': S,
                   'C': C,
                   'W': W,
                   'T': T})

```
:::

### Example 1

In this first example, we'll build both the structure and parameters of the model using expert knowledge.

#### Structure learning

::: {.panel-tabset group="language"}
# R

**bnlearn** has a couple of different ways to specify network structures. The one we'll use here allows you to create both the nodes (variables) and the links (paths) in one go. The function is `model2network`. Each variable is specified in `[ ]`, together with any other variables that it depends on. So `[C|A:B]` indicates a node `C` which has paths coming in from nodes ``A` and `B`. The following code specifies the DAG shown above:

```{r}
dag <- model2network("[A][S][E|A:S][O|E][R|E][T|O:R]")
dag
```

# Python

```{python}
print("Hello world")
```
:::

In the network description, there are 6 paths described (all directed). We can visualize the structure as well: 

::: {.panel-tabset group="language"}
# R

```{r}
graphviz.plot(dag)
```

# Python

```{python}
print("Hello world")
```
:::

#### Parameter learning

In a discrete Bayesian network, the parameters are the conditional probabilities for each variable, and can be set using conditional probability tables (CPTs). For an exogenous variable (one with no paths coming in), this simply represents the probability of each class. So if in our example, we expected 30% young respondents, 50% adult and 20% old, the CPT for age would be:

| `young` | `adult` | `old` |
|---------------|-----------------|--------------------------|
| 0.3        | 0.5  | 0.2 |

For endogenous variables (one with at least one path coming in) this becomes more difficult, as we need to specify the pairwise probabilities for each category of the variable of interest conditioned on each category of the parent variable. For example, occupation (`O`) is dependent on the level of education (`E`), so we might assign the following probabilities: `emp` and `high`: 96%; `emp` and `uni`: 92%; `self` and `high`: 4%; `self` and `uni`: 8%. Note the probabilites for each level of the variable of interest (`O`) have to sum to 1 (or 100%). The CPT might look something like this:

|| `emp` | `self` |
|---------------|-----------------|--------------------------|
|`high` | 0.96 | 0.92 |
|`uni`  | 0.04 | 0.08 |

For variables with two incoming paths (`E` and `T`), this becomes even more complicated as we now need the conditional probability for each combination of categories (i.e. each of `E`, `A` and `S`). As you might imagine, this becomes non-trivial for large and complex networks, where learning from data provides an easier way forward. 

Let's now make the CPTs. Generally, this requires the tables to be first defined then linked to the DAG created above. 

::: {.panel-tabset group="language"}
# R

In R, we first defined the *levels*, the categories of each variable:

```{r}
A.lv <- c("young", "adult", "old")
S.lv <- c("M", "F")
E.lv <- c("high", "uni")
O.lv <- c("emp", "self")
R.lv <- c("small", "big")
T.lv <- c("car", "train", "other")
```

Next, we define the CPTs. We'll start with the two exogenous variables (`A` and `S`). Each of these is simply a 1D vector of probabilities that should sum to 1. The vector for `A` needs three entries as there are three levels. Note that the *dimnames* are set using the levels created above (this allows the model to find the correct probability). 

```{r}
## ----cpts-A-S-----------------------------------------------------------------
A.prob <- array(c(0.30, 0.50, 0.20), dim = 3, dimnames = list(A = A.lv))
A.prob
S.prob <- array(c(0.60, 0.40), dim = 2, dimnames = list(S = S.lv))
S.prob
```

Next we'll do the variables with one incoming path (`O` and `R`). These need a 2D array of probabilities:

```{r}
## ----cpts-O-R-----------------------------------------------------------------
O.prob <- array(c(0.96, 0.04, 
                  0.92, 0.08), dim = c(2, 2),
            dimnames = list(O = O.lv, E = E.lv))
O.prob
R.prob <- array(c(0.25, 0.75, 
                  0.20, 0.80), dim = c(2, 2),
            dimnames = list(R = R.lv, E = E.lv))
R.prob
```

And finally, the two variables with 2 incoming paths. These need a 3D array (one dimension per variable). Note that the dimension definitions start to become more important here. For `E`, we have `A` and `S`, which have 3 and 2 levels respectively, and `E` has 2. The order we have chose here (`E`, `A`, `S`) needs an array with the following dimensions `[2,3,2]`. 

For the second (`T`, 3 levels), we have `O` and `R`, which both have 2 levels. The array dimension are then `[3,2,2]`. 


```{r}
## ----cpts-E-T-----------------------------------------------------------------
E.prob <- array(c(0.75, 0.25, 0.72, 0.28, 0.88, 0.12, 0.64, 0.36, 0.70,
                  0.30, 0.90, 0.10), dim = c(2, 3, 2),
            dimnames = list(E = E.lv, A = A.lv, S = S.lv))
E.prob

T.prob <- array(c(0.48, 0.42, 0.10, 0.56, 0.36, 0.08, 0.58, 0.24, 0.18,
                  0.70, 0.21, 0.09), dim = c(3, 2, 2),
            dimnames = list(T = T.lv, O = O.lv, R = R.lv))
T.prob
```

# Python

```{python}
print("Hello world")
```
:::

With these set, we can now link them to the network. 

::: {.panel-tabset group="language"}
# R

```{r}
cpt <- list(A = A.prob, S = S.prob, 
            E = E.prob, O = O.prob, 
            R = R.prob, T = T.prob)
bn <- custom.fit(dag, cpt)
```

You can print any of these from the fitted network as follows:

```{r}
bn$T
```

# Python

```{python}
print("Hello world")
```
:::

### Example 2

For the second example, we'll use the network structure that we created from expert knowledge in the first example, but we'll learn the parameters using the travel survey dataset:

#### Structure learning

We'll recreate the empty DAG first (note that we could simply update the parameters from the original if we wanted). 

::: {.panel-tabset group="language"}
# R

```{r}
dag <- model2network("[A][S][E|A:S][O|E][R|E][T|O:R]")
```

# Python

```{python}
print("Hello world")
```
:::




# Appendix: Data files

## Tavis and Grace dataset *travis.csv*

| Column header | Variable                                   |
|---------------|--------------------------------------------|
| siteno        | Site ID                                    |
| latitude      | Latitude                                   |
| geneticdist   | Genetic dissimilarity to local populations |
| stems         | Stem density                               |
| infls         | Number of infloresences                    |
| clonediam     | Clone diameter                             |
| leafht        | Leaf height                                |
| leafwdth      | Leaf width                                 |

stem density, the number of infloresences, clone diameter,
