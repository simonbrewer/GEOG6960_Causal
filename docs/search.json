[
  {
    "objectID": "GEOG6960_Week2.html",
    "href": "GEOG6960_Week2.html",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#introduction",
    "href": "GEOG6960_Week2.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#packages",
    "href": "GEOG6960_Week2.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(MatchIt)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "GEOG6960_Week2.html#simulating-data",
    "href": "GEOG6960_Week2.html#simulating-data",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Simulating data",
    "text": "Simulating data\nFirst, we’re going to create a synthetic dataset for use in the lab. Simulating these types of data can be very useful in understanding how models work, and we’ll use it here to illustrate the difference between a randomized trial and a trial where the treatment (\\(T\\)) is biased. This is particularly useful for causal inference as simulating data allows us to see both the factual (observed) and counterfactual (unobserved) outcomes.\nWe’re going to use the same example shown in the lectures: a study aiming to estimate the effect of computer tablets (\\(T\\)) on student test outcomes (\\(Y\\)). The confounding variable (\\(X\\)) is the school tuition, taken as a proxy for school wealth. One twist here is that we want to assign tablets by school, not by student, which makes this slightly more complicated.\nBefore we start, we need to decide some values for the data. You’re very welcome to change these to different values, but I’d suggest first running this with the values given here, then going back to see how changing these affects your results.\nWe’ll start by setting the random seed (to ensure we get the same results). Again, feel free to change this, but your results will differ slightly from those in this document:\n\nRPython\n\n\n\nset.seed(1242)\n\n\n\n\nnp.random.seed(42)\n\n\n\n\nNext, let’s define the number of observations:\n\nNumber of schools: 50\nNumber of students per school: 20\n\n\nRPython\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\n\nSchools\nWe’ll assign the tuition levels randomly from a normal distribution with a mean of 1000 and s.d. of 300:\n\nRPython\n\n\n\ntuition = round(rnorm(n_schools, 1000, 300))\n\n\n\n\ntuition = np.round(np.random.normal(1000, 300, n_schools))\n\n\n\n\nNow, we’ll use the tuition to decide whether or not a school assigns tablets to students. We’ll do this randomly, using a binomial distribution, where the probability of a school assign tablets is given by first converting the tuition to a \\(z\\)-score:\n\\[\n\\mbox{tuition}_z = (\\mbox{tuition} - mean(\\mbox{tuition}) / sd(\\mbox{tuition})\n\\]\nThen we get \\(p\\) for each school as:\n\\[\np_\\mbox{tablet} = exp(\\mbox{tuition}_z) / (1 + exp(\\mbox{tuition}_z))\n\\]\nPutting this into practice:\n\nRPython\n\n\n\ntuition_z = (tuition - mean(tuition)) / sd(tuition)\ntuition_p = exp(tuition_z)/(1+exp(tuition_z))\ntablet = rbinom(n_schools, 1, tuition_p)\n\nLet’s put all of this into a data frame:\n\nschool_df = data.frame(id = as.factor(1:length(tablet)), \n                       tuition = tuition, \n                       tuition_p = tuition_p,\n                       tablet = as.factor(tablet))\n\n\n\n\ntuition_z = (tuition - tuition.mean()) / tuition.std()\ntuition_p = np.exp(tuition_z)/(1+np.exp(tuition_z))\ntablet = np.random.binomial(1, tuition_p, n_schools)\n\nLet’s put all of this into a data frame:\n\nschool_df = pd.DataFrame({'id': np.arange(n_schools), \n                          'tuition': tuition,\n                          'tablet': tablet})\n\n\n\n\nAnd we can now visualize some of the results (this is a good way to check that we get what we expect):\n\nRPython\n\n\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45)\n\n\n\n\n\n\n\n\n\nggboxplot(school_df, x = \"tablet\", y = \"tuition\") +\n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\nsns.boxplot(school_df, x=\"tablet\", y=\"tuition\", hue=\"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWe can also test for differences in the tuition rates based on whether or not tablets were assigned:\n\nRPython\n\n\n\nt.test(tuition ~ tablet, school_df)\n\n\n    Welch Two Sample t-test\n\ndata:  tuition by tablet\nt = -4.3665, df = 47.53, p-value = 6.79e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -494.1193 -182.4862\nsample estimates:\nmean in group 0 mean in group 1 \n       848.9565       1187.2593 \n\n\n\n\n\nfrom statsmodels.stats.weightstats import ttest_ind\nt_stat, p_value, df = ttest_ind(school_df[school_df['tablet'] == 1]['tuition'], \n                                school_df[school_df['tablet'] == 0]['tuition'])\nprint(f'T: {t_stat}; p-value: {p_value}')\n\nT: 1.8772572008954862; p-value: 0.06656723616720626\n\n\n\n\n\n\n\nStudents\nNow we’ll create class_size students for each school. We first make a data frame of students, by simply repeating the school values for tuition and tablet:\n\nRPython\n\n\n\nstudent_df = data.frame(id = 1:(n_students),\n                        school_id = rep(school_df$id, each = class_size),\n                        tuition = rep(tuition, each = class_size),\n                        tablet = factor(rep(tablet, each = class_size)))\n\n\n\n\nstudent_df = pd.DataFrame({'id': np.arange(n_students),\n                           'school_id': np.repeat(school_df['id'], class_size),\n                           'tuition': np.repeat(school_df['tuition'], class_size),\n                           'tablet': np.repeat(school_df['tablet'], class_size)})\n\n\n\n\nNow we’ll create a test score for each student. This will again be random, but based on the tuition values of the school (to reflect that we expect students at higher funded schools to test better). Student scores will be taken from a random normal distribution with a s.d. of 200 and the mean given by \\(200 + 0.7 \\times \\mbox{tuition}\\). We’ll then rescale the scores so that the maximum is 1000.\n\nRPython\n\n\n\nstudent_df$enem_score0 = rnorm(n_students, 200 +\n                                0.7 * student_df$tuition, 200) \nstudent_df$enem_score0 =\n  (student_df$enem_score0 - min(student_df$enem_score0)) /\n  max(student_df$enem_score0) * 1000\n\n\n\n\nstudent_df['enem_score0'] = np.random.normal(200 + 0.7 * student_df['tuition'], 200, n_students) \n\nstudent_df['enem_score0'] = (student_df['enem_score0'] - student_df['enem_score0'].min()) / student_df['enem_score0'].max() * 1000.0\n\n\n\n\nNote that this score (enem_score0) is the factual for students who were not assigned a tablet, and the counterfactual for students who were.\nFinally, we’ll add a tablet effect. This is the expected change in a student’s score if they were assigned a tablet. For this exercise, we’ll assume that having a tablet reduces scores by 50 points on average, but with a s.d. of 5.\n\nRPython\n\n\n\nstudent_df$tablet_eff = rnorm(n_students, -50, 5)\ngghistogram(student_df, x = \"tablet_eff\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['tablet_eff'] = np.random.normal(-50, 5, n_students)\nsns.histplot(student_df, x = \"tablet_eff\")\n\n\n\n\n\n\n\n\n\n\n\nFinally, add the tablet effect back to the score. We multiply by the binary tablet assignation.\n\nRPython\n\n\n\nstudent_df$enem_score1 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet)-1\nggboxplot(student_df, \n          x = \"tablet\", \n          y = \"enem_score1\", \n          fill = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['enem_score1'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet']\n\nsns.boxplot(student_df, x = \"tablet\", y = \"enem_score1\", \n          hue = \"tablet\")"
  },
  {
    "objectID": "GEOG6960_Week2.html#first-test",
    "href": "GEOG6960_Week2.html#first-test",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "First test",
    "text": "First test\nWith the data in had, we can test for the causal effect of tablet on the observed scores enem_score1. Before running this, just a reminder of two points. Given the way we have created the data set, we know this is expected to be negative and around -50. But we also know that there is a bias in the tablet assignment from the tuition rates.\nAs the test scores are normally distributed, and we have two groups (treated and control), we can use a \\(t\\)-test to explore the differences (as shown above). More usefully, we replace this with a linear model (lm in R or statsmodels.OLS in Python), as this will allow us to test for significance and give us an estimate of the effect in the coefficient \\(\\beta_1\\):\n\\[\n\\mbox{enem\\_score} = \\beta_0 + \\beta_1 \\times \\mbox{tablet}\n\\]\n(As an aside, while they are often taught separately, most statistical tests are just special cases of the linear model…)\n\nRPython\n\n\n\nsummary(lm(enem_score1 ~ tablet, student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\nWhich gives us a highly significant effect of 77.64. Now you should be able to see the impact of the tuition bias: we expected an effect of around -50 and we got 77.64 instead.\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):              0.859\nTime:                        16:14:47   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nYou can also see this effect if you plot the test scores against tuition:\n\nRPython\n\n\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          col = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          hue = \"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWhere you’ll see both the influence of tuition and the asymmetric distribution of tablets."
  },
  {
    "objectID": "GEOG6960_Week2.html#randomized-trial",
    "href": "GEOG6960_Week2.html#randomized-trial",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Randomized trial",
    "text": "Randomized trial\nWe’ll now repeat this test, but by simulating a random trial of tablets across schools. To keep this comparable to the previous (biased) example, we’ll work with the same data. First we assign tablets randomly\n\nRPython\n\n\n\nschool_df$tablet_rct = as.factor(sample(rep(c(0,1), n_schools/2)))\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet_rct\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45) \n\n\n\n\n\n\n\n\n\n\n\nschool_df['tablet_rct'] = school_df['tablet'].sample(n_schools).to_numpy()\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet_rct\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\n\n\nNext we create a new set of test scores by updating the original scores (enem_score0) with tablet effect multiplied by the new tablet assignment. If we then repeat the scatter plot using the new scores and tablet assignments, you should see a more even distribution:\n\nRPython\n\n\n\nstudent_df$tablet_rct = rep(school_df$tablet_rct, each = class_size)\nstudent_df$enem_score2 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet_rct)-1\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          col = \"tablet_rct\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nsummary(lm(enem_score2 ~ tablet_rct, student_df))\n\n\nCall:\nlm(formula = enem_score2 ~ tablet_rct, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-442.06 -115.71   -5.93  118.81  486.75 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  380.065      7.399  51.367  &lt; 2e-16 ***\ntablet_rct1  -33.930     10.464  -3.243  0.00122 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.4 on 998 degrees of freedom\nMultiple R-squared:  0.01043,   Adjusted R-squared:  0.009434 \nF-statistic: 10.51 on 1 and 998 DF,  p-value: 0.001223\n\n\n\n\n\nstudent_df['tablet_rct'] = np.repeat(school_df['tablet_rct'], class_size)\nstudent_df['enem_score2'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet_rct']\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          hue = \"tablet_rct\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nmod = smf.ols(formula='enem_score2 ~ tablet_rct', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score2   R-squared:                       0.037\nModel:                            OLS   Adj. R-squared:                  0.037\nMethod:                 Least Squares   F-statistic:                     38.86\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           6.71e-10\nTime:                        16:14:48   Log-Likelihood:                -6347.3\nNo. Observations:                1000   AIC:                         1.270e+04\nDf Residuals:                     998   BIC:                         1.271e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.5017      6.312     60.281      0.000     368.115     392.888\ntablet_rct   -54.5671      8.753     -6.234      0.000     -71.744     -37.390\n==============================================================================\nOmnibus:                        3.915   Durbin-Watson:                   1.067\nProb(Omnibus):                  0.141   Jarque-Bera (JB):                3.988\nSkew:                           0.145   Prob(JB):                        0.136\nKurtosis:                       2.892   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#propensity-score-matching",
    "href": "GEOG6960_Week2.html#propensity-score-matching",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nIn the previous sections, we looked at the effect of having a randomized or biased design in our data, and how this can impact the conclusions that we draw. But what do you do when you don’t have a randomized trial? In a lot of situations, we have natural experiments; where ‘treatments’ have taken place for other reasons than our tests. This is the case with the first set of test scores - these were created to mimic a natural experiment where schools had decided themselves (and partly based on finances) whether or not to give students tablets. In this case, we can use propensity score matching to try and reduce any biases.\nThe aim here is to create a subset of data with matched treated and control samples, where the confounding variables (e.g. tuition) are used to make the matches. The idea being that if we have treatments and controls for similar tuition levels, then the remaining difference in test scores should be due to the effect of the treatment (the tablets in our example).\nHere, we’ll look briefly at how propensity scores are calculated, then use an add-on package to calculate these for our dataset. Finally, we’ll re-run our model to test for tablet-related test score differences with the new, matched set.\nLet’s remind ourselves of the data we have available:\n\nRPython\n\n\n\nhead(school_df)\n\n  id tuition tuition_p tablet tablet_rct\n1  1    1084 0.5402727      1          1\n2  2     861 0.3714199      1          0\n3  3     936 0.4268085      0          1\n4  4    1620 0.8598539      1          0\n5  5    1428 0.7724307      1          1\n6  6     784 0.3178776      1          1\n\n\n\nhead(student_df)\n\n  id school_id tuition tablet enem_score0 tablet_eff enem_score1 tablet_rct\n1  1         1    1084      1    730.6342  -50.60501    628.4242          1\n2  2         1    1084      1    585.3498  -51.11708    482.1156          1\n3  3         1    1084      1    618.7099  -44.67392    528.3621          1\n4  4         1    1084      1    513.1673  -53.44279    405.2818          1\n5  5         1    1084      1    475.5372  -46.81305    380.9111          1\n6  6         1    1084      1    639.4113  -41.48421    555.4428          1\n  enem_score2\n1    628.4242\n2    482.1156\n3    528.3621\n4    405.2818\n5    380.9111\n6    555.4428\n\n\n\n\n\nschool_df.head()\n\n   id  tuition  tablet  tablet_rct\n0   0   1149.0       1           0\n1   1    959.0       0           1\n2   2   1194.0       1           0\n3   3   1457.0       0           0\n4   4    930.0       1           1\n\n\n\nstudent_df.head()\n\n   id  school_id  tuition  ...  enem_score1  tablet_rct  enem_score2\n0   0          0   1149.0  ...   345.201552           0   396.496762\n0   1          0   1149.0  ...   449.695558           0   500.677307\n0   2          0   1149.0  ...   408.749080           0   459.107087\n0   3          0   1149.0  ...   495.975591           0   546.161703\n0   4          0   1149.0  ...   332.925810           0   379.287662\n\n[5 rows x 9 columns]\n\n\n\n\n\nAlthough we are testing for the differences in students, the assignment (and therefore propensity) needs to be calculated for the schools, so we’ll use schools_df for the next steps. Note that propensity score usually works best with larger datasets, and is somewhat limited with only 50 samples.\nPropensity scores are simply the probability that a given observation was selected for the treatment. The important part is that we want to estimate these probabilities using the same covariate(s) that we think (or know) caused the bias in the treatment. We’ll estimate this here using binomial regression in a generalized linear model, but note you can use any model that works with a binary outcome (random forests, boosted trees, etc).\n\nRPython\n\n\nIn R, we can fit this model using glm and by setting the family to binomial:\n\nfit_ps &lt;- glm(tablet ~ tuition, school_df, family = binomial())\nsummary(fit_ps)\n\n\nCall:\nglm(formula = tablet ~ tuition, family = binomial(), data = school_df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -4.114829   1.321285  -3.114  0.00184 **\ntuition      0.004234   0.001294   3.273  0.00106 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 68.994  on 49  degrees of freedom\nResidual deviance: 53.442  on 48  degrees of freedom\nAIC: 57.442\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df &lt;- data.frame(prop_score = predict(fit_ps, type = \"response\"),\n                     tablet = as.numeric(fit_ps$model$tablet)-1,\n                     tuition = fit_ps$model$tuition)\nhead(prs_df)\n\n  prop_score tablet tuition\n1  0.6165972      1    1084\n2  0.3848259      1     861\n3  0.4621865      0     936\n4  0.9396136      1    1620\n5  0.8734399      1    1428\n6  0.3110631      1     784\n\n\n\n\nIn Python, we can fit this model using the glm function from statsmodels and by setting the family to binomial:\n\nmod = smf.glm(formula='tablet ~ tuition', data=school_df, family=sm.families.Binomial())\nfit = mod.fit()\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 tablet   No. Observations:                   50\nModel:                            GLM   Df Residuals:                       48\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -32.853\nDate:                Thu, 05 Sep 2024   Deviance:                       65.706\nTime:                        16:14:48   Pearson chi2:                     50.1\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.06814\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.7868      1.077     -1.659      0.097      -3.897       0.324\ntuition        0.0020      0.001      1.794      0.073      -0.000       0.004\n==============================================================================\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df = pd.DataFrame({'prop_score': fit.predict(),\n                        'tablet': school_df['tablet'],\n                        'tuition': school_df['tuition']})\nprs_df.head()\n\n   prop_score  tablet  tuition\n0    0.627858       1   1149.0\n1    0.535210       0    959.0\n2    0.648739       1   1194.0\n3    0.758087       0   1457.0\n4    0.520682       1    930.0\n\n\n\n\n\nTo illustrate how a simple match would happen, let’s split this into a treatment and control data set:\n\nRPython\n\n\n\ntreated_df = prs_df %&gt;%\n  filter(tablet == 1)\ncontrol_df = prs_df %&gt;%\n  filter(tablet == 0)\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nmatch_id = which.min(abs(treated_df$prop_score[1] - control_df$prop_score))\nmatch_id\n\n[1] 5\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df[match_id, ]\n\n   prop_score tablet tuition\n15  0.6175977      0    1085\n\n\n\n\n\ntreated_df = prs_df[prs_df['tablet'] == 1].reset_index()\ncontrol_df = prs_df[prs_df['tablet'] == 0].reset_index()\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nabs_diff = (treated_df['prop_score'][0] - control_df['prop_score']).abs()\nmatch_id = abs_diff.idxmin()\nprint(match_id)\n\n4\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df.iloc[match_id,:]\n\nindex            9.00000\nprop_score       0.63441\ntablet           0.00000\ntuition       1163.00000\nName: 4, dtype: float64\n\n\n\n\n\nWe could obviously make this into a loop and get all the matches, but instead we’ll use an external package to carry out the full match.\n\nRPython\n\n\nIn R, the package we will use is called MatchIt. It is pretty well established and allows you to choose different method to calculate the scores and carry out matching.\nTo get an idea of the output, we’ll first run this with no matching (method = NULL). The output will show some summary statistics on the match between the treatment and control. The first line (distance) shows the difference in propensity score between the two groups and the second (and subsequent) line shows the difference in the covariate. A useful index is the standardized mean difference, which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\nmatch0 = matchit(tablet ~ tuition, data = school_df,\n                 method = NULL, distance = \"glm\")\nsummary(match0)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = NULL, \n    distance = \"glm\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.3944          1.0838     1.4057    0.2842\ntuition       1187.259      848.9565          1.0929     1.6977    0.2842\n         eCDF Max\ndistance   0.5427\ntuition    0.5427\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      27\nUnmatched       0       0\nDiscarded       0       0\n\n\nNow, we’ll re-run and use nearest neighbor matching to selected control schools.\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\")\n\nWarning: Fewer control units than treated units; not all treated units will get\na match.\n\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\")\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.7356        0.3944          1.3717     0.8313    0.3617\ntuition      1269.9130      848.9565          1.3599     1.1421    0.3617\n         eCDF Max Std. Pair Dist.\ndistance   0.6522          1.3717\ntuition    0.6522          1.3599\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      23\nUnmatched       0       4\nDiscarded       0       0\n\n\nThe results here are slightly worse (the std. differences have increased). This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\", \n                 replace = TRUE)\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\", replace = TRUE)\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.6353          0.1156     0.9800    0.0585\ntuition       1187.259     1123.9259          0.2046     1.2934    0.0585\n         eCDF Max Std. Pair Dist.\ndistance   0.3704          0.1641\ntuition    0.3704          0.2549\n\nSample Sizes:\n              Control Treated\nAll              23.       27\nMatched (ESS)     3.9      27\nMatched           8.       27\nUnmatched        15.        0\nDiscarded         0.        0\n\n\nNow we obtain a better match as shown by the decrease in std. differences. Note in the sample sizes that the total number of retained control samples is only 8, which is probably too low in practice.\nWe can see the results of the match using the plot() function. For example, this shows the histograms of treated (top) and control (bottom), before (left) and after (right) matching.\n\nplot(match1, type = \"hist\", interactive = FALSE)\n\n\n\n\n\n\n\n\nAnd this shows the same for the empirical cumulative distribution functions:\n\nplot(match1, type = \"ecdf\", interactive = FALSE)\n\n\n\n\n\n\n\n\nWe can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools. First extract the match ‘ids’ (the rows from the original school_df data frame)\n\nmatch_df = get_matches(match1, id = \"mid\")\n\nNow we can loop across these and create a new data frame by appending the students from each matched school in turn:\n\nmatch_student_df = NULL\nfor (i in 1:nrow(match_df)) {\n  tmp_df = student_df %&gt;%\n    filter(school_id == as.numeric(match_df$mid)[i])\n  match_student_df = rbind(match_student_df, tmp_df)\n}\n\nAnd finally, we can repeat our test:\n\nsummary(lm(enem_score1 ~ tablet, match_student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = match_student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -109.72    7.64   95.34  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  427.098      6.451  66.212  &lt; 2e-16 ***\ntablet1      -30.184      9.122  -3.309 0.000968 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 149.9 on 1078 degrees of freedom\nMultiple R-squared:  0.01005,   Adjusted R-squared:  0.009135 \nF-statistic: 10.95 on 1 and 1078 DF,  p-value: 0.0009681\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!\n\n\nIn Python, the package we will use is called psmpy. It is a solid and fairly widely used package, but doesn’t offer quite the same flexibility as R. The main function is PsmPy, and uses a similar format to SciKit-Learn, where methods are initialized then fit to the data. We need to specify:\n\nThe data frame holding the data\nThe treatment (this is the tablet variable)\nA column with observation IDs (these will be used in matching)\nAny varaibles that we want to exclude from the propensity score estimates\n\n\nfrom psmpy import PsmPy\npsm = PsmPy(school_df, treatment='tablet', indx='id', exclude = ['tablet_rct'])\n\nOnce we’ve set this up, we can calculate the propensity score using a binomial (logisitic) model as follows. THe resulting dataframe contains the propensity scores on both a probability and logit scale:\n\npsm.logistic_ps(balance = True)\npsm.predicted_data.head()\n\n   id  tuition  propensity_score  propensity_logit  tablet\n0   0   1149.0          0.582970          0.334979       1\n1   2   1194.0          0.595662          0.387423       1\n2   4    930.0          0.519927          0.079750       1\n3   6   1474.0          0.671227          0.713738       1\n4   8    859.0          0.499251         -0.002996       1\n\n\nOnce this is run, we can use the results to carry out nearest neighbor matching to selected control schools.\n\npsm.knn_matched(matcher='propensity_logit', replacement=False, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\nWe can explore the matches. First, we can plot a histogram of the matched propensity scores. Ideally, these histograms would roughly match, but there is still quite a lot of visible differences\n\npsm.plot_match()\n\n\n\n\n\n\n\n\nA useful index is the standardized mean difference (called the effect_size), which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\n\npsm.effect_size\n\n  Variable matching  Effect Size\n0  tuition   before     0.531394\n1  tuition    after     0.370122\n\n\nThis shows that we have reduced the difference (the after effect_size is lower), but it remains fairly high. This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\npsm.knn_matched(matcher='propensity_logit', replacement=True, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\nNow we obtain a better match as shown by the decrease in effect size. Another useful diagnostic is to plot values of covariates for the matched treated and control samples as histograms:\n\nfig, axs = plt.subplots(ncols=2)\nsns.histplot(school_df, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[0]).set(title='Before')\nsns.histplot(psm.df_matched, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nOr as empirical cumulative distribution functions:\n\nfig, axs = plt.subplots(ncols=2)\nsns.ecdfplot(school_df, x = \"tuition\", hue=\"tablet\", ax=axs[0]).set(title='Before')\nsns.ecdfplot(psm.df_matched, x = \"tuition\", hue=\"tablet\", ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nAs these now align pretty well after the matching, we can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools.\n\nmatch_df = psm.df_matched\nmatched_student_df = pd.DataFrame(columns=student_df.columns)\nfor idx, row in match_df.iterrows():\n    #print(row['id'])\n    tmp_df = student_df[student_df['school_id'] == row['id']]\n    matched_student_df = pd.concat([matched_student_df, tmp_df], ignore_index = True)\n\n&lt;string&gt;:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n    \nmatched_student_df.head()\n\n   id school_id  tuition  ... enem_score1  tablet_rct  enem_score2\n0  20         1    959.0  ...  408.713490           1   355.594645\n1  21         1    959.0  ...  375.552762           1   335.122918\n2  22         1    959.0  ...  191.336345           1   140.382933\n3  23         1    959.0  ...  380.340349           1   331.427514\n4  24         1    959.0  ...  389.004593           1   343.354932\n\n[5 rows x 9 columns]\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=matched_student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.020\nModel:                            OLS   Adj. R-squared:                  0.019\nMethod:                 Least Squares   F-statistic:                     19.11\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           1.37e-05\nTime:                        16:14:53   Log-Likelihood:                -5941.3\nNo. Observations:                 940   AIC:                         1.189e+04\nDf Residuals:                     938   BIC:                         1.190e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept     351.4540      6.146     57.187      0.000     339.393     363.515\ntablet[T.1]   -38.4086      8.785     -4.372      0.000     -55.650     -21.167\n==============================================================================\nOmnibus:                       13.303   Durbin-Watson:                   1.085\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               13.639\nSkew:                           0.295   Prob(JB):                      0.00109\nKurtosis:                       2.992   Cond. No.                         2.59\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!"
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "href": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not recieve a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           1.18e-08\nTime:                        18:12:11   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):              0.859\nTime:                        18:12:11   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-probability-weighting",
    "href": "GEOG6960_Week2.html#inverse-probability-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not receive a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           1.18e-08\nTime:                        16:14:53   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):              0.859\nTime:                        16:14:53   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "99_html_index.html",
    "href": "99_html_index.html",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "",
    "text": "Often in scientific studies we are interested in establishing a cause and effect, e.g. what is the effect of some policy on health outcomes or the effect of anthropogenic activity on a changing climate. However, most statistical texts and courses are taught using correlative methods, with the mantra that “correlation is not causation” outside of certain strict experimental conditions. Recent work by Judea Pearl and others have developed a framework (Structural Causal Modeling) for causal inference that allows causality to be inferred even when these conditions are not met, allowing this approach in a much broader range of studies. In this seminar, we will review the history and concepts of causal analysis, and go through the steps of Pearl’s causal framework using a set of hands-on examples. The class will largely follow the outline of The Book of Why (Pearl and Mackenzie, 2018; Basic Books, NY). Students will develop their own analysis over the course of the semester through a series of discussions and presentations."
  },
  {
    "objectID": "99_html_index.html#footnotes",
    "href": "99_html_index.html#footnotes",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎"
  },
  {
    "objectID": "GEOG6960_Week3.html",
    "href": "GEOG6960_Week3.html",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "",
    "text": "Warning: package 'reticulate' was built under R version 4.4.1"
  },
  {
    "objectID": "GEOG6960_Week3.html#introduction",
    "href": "GEOG6960_Week3.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week3.html#packages",
    "href": "GEOG6960_Week3.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggsci)\nlibrary(sjPlot)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "GEOG6960_Week3.html#interrupted-time-series",
    "href": "GEOG6960_Week3.html#interrupted-time-series",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Interrupted Time Series",
    "text": "Interrupted Time Series\nInterrupted time series models assess the causal effect of an intervention or treatment by examining changes in the trend of an outcome (\\(Y\\)) before and after the start of the treatment. This is quite widely used in social and economic settings, where often only one set of data can be observed (e.g. tracking GDP before and after the implementation of a fiscal policy).\nThe format for the ITS model is\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 P\n\\]\nWhere:\n\n\\(Y\\) is the outcome\n\\(T\\) is time\n\\(D\\) is a binary indicator (pre vs. post treatment)\n\\(P\\) is a index of time since treatment\n\n\nSimulated data\nFirst, we’re going to create a synthetic dataset, which will include the impact of a treatment. The time series will include a base trend, which will then be modified after the start of the intervention.\nFirst, we’ll set a random seed to make the results repeatable. As before, try changing this to see how the noise we will add changes the results.\n\nRPython\n\n\n\nset.seed(42)\n\n\n\n\nnp.random.seed(42)\n\n\n\n\nNow we’ll simulate the data. The data will represent student outcomes over a full year (365 days), and we’ll use the following equation to represent the base trend (\\(Y\\) is the outcome, \\(T\\) is the time in days). This will give a starting value of 5.4 and an upward trend of 0.5 per day:\n\\[\nY = 5.4 + 0.5 \\times T\n\\] To this we’ll add the following effects: - An immediate effect of the policy change of +20 points - A change in the slope of +1.2\nTo include these, we need to make the two vectors (\\(D\\) and \\(P\\)). With this, the equation to generate the data is:\n\\[\nY = 5.4 + 0.5 \\times T + 20 \\times D + 1.2 \\times P\n\\]\nFinally, we’ll add some noise to the trends to represent individual daily variation (\\(N(0, 50)\\)).\n\nRPython\n\n\nFirst generate the basic equation:\n\nT = rep(1:365)\nD = ifelse(T &gt; 200, 1, 0)\nP = ifelse(T &lt;= 200, 0, rep(1:200))\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\n\nNow add errors and combine everything into a data frame:\n\nerr = rnorm(365, 0, 50)\nY = Y + err\nwell_df &lt;- as.data.frame(cbind(Y, T, D, P)) \n\nAnd finally plot it:\n\nggplot(well_df, aes(x = T, y = Y)) + \ngeom_point(size = 3, alpha = 0.5) +\ngeom_vline(xintercept = 201) +\ntheme_bw() +\ntheme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\nFirst generate the basic equation:\n\nT = np.arange(365)\nD = np.where(T &gt; 200, 1, 0)\nP = T - 200\nP = np.where(T &lt;= 200, 0, P)\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\n\nNow add errors and combine everything into a data frame:\n\nerr = np.random.normal(0, 50, 365)\nY = Y + err\n\nwell_df = pd.DataFrame({'Y': Y,\n                        'T': T,\n                        'D': D,\n                        'P': P})\n\nAnd finally plot it:\n\nsns.scatterplot(well_df,\n                x = \"T\",\n                y = \"Y\", \n                alpha = 0.75)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple model\nBefore fitting the ITS model, we’ll fit a simple trend model of the outcome over time. In this case, this we’ll just use a simple OLS model.\n\nRPython\n\n\n\nfit0 &lt;- lm(Y ~ T, well_df)\ntab_model(fit0)\n\n\n\n\n\n \nY\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-49.30\n-61.27 – -37.32\n&lt;0.001\n\n\nT\n1.08\n1.03 – 1.14\n&lt;0.001\n\n\nObservations\n365\n\n\nR2 / R2 adjusted\n0.796 / 0.795\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Y ~ T', data=well_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.817\nModel:                            OLS   Adj. R-squared:                  0.816\nMethod:                 Least Squares   F-statistic:                     1618.\nDate:                Sat, 02 Nov 2024   Prob (F-statistic):          8.38e-136\nTime:                        16:58:18   Log-Likelihood:                -1986.9\nNo. Observations:                 365   AIC:                             3978.\nDf Residuals:                     363   BIC:                             3986.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -53.6683      5.862     -9.155      0.000     -65.196     -42.141\nT              1.1211      0.028     40.218      0.000       1.066       1.176\n==============================================================================\nOmnibus:                        0.840   Durbin-Watson:                   1.519\nProb(Omnibus):                  0.657   Jarque-Bera (JB):                0.939\nSkew:                          -0.068   Prob(JB):                        0.625\nKurtosis:                       2.792   Cond. No.                         420.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nNote that the slope we obtain here falls somewhere between the baseline trend (0.5) and the post-treatment trend (0.5 + 1.2), as we have not accounted for this effect as a separate term in the model.\nNow we’ll fit the full ITS model. As a reminder, this extends the basic OLS model by including the two additional vectors described above.\n\n\nITS model\n\nRPython\n\n\n\nfit1 &lt;- lm(Y ~ T + D + P, well_df)\ntab_model(fit1)\n\n\n\n\n\n \nY\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n7.64\n-5.73 – 21.02\n0.262\n\n\nT\n0.46\n0.35 – 0.58\n&lt;0.001\n\n\nD\n25.71\n5.87 – 45.54\n0.011\n\n\nP\n1.20\n1.01 – 1.39\n&lt;0.001\n\n\nObservations\n365\n\n\nR2 / R2 adjusted\n0.862 / 0.860\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Y ~ T + D + P', data=well_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.870\nModel:                            OLS   Adj. R-squared:                  0.869\nMethod:                 Least Squares   F-statistic:                     803.5\nDate:                Sat, 02 Nov 2024   Prob (F-statistic):          2.35e-159\nTime:                        16:58:18   Log-Likelihood:                -1924.6\nNo. Observations:                 365   AIC:                             3857.\nDf Residuals:                     361   BIC:                             3873.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4776      6.667     -0.372      0.710     -15.588      10.632\nT              0.5594      0.058      9.701      0.000       0.446       0.673\nD             22.9193      9.991      2.294      0.022       3.271      42.568\nP              1.0990      0.097     11.308      0.000       0.908       1.290\n==============================================================================\nOmnibus:                        4.812   Durbin-Watson:                   2.136\nProb(Omnibus):                  0.090   Jarque-Bera (JB):                5.520\nSkew:                           0.134   Prob(JB):                       0.0633\nKurtosis:                       3.539   Cond. No.                         908.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThe values we used when generating the data should now be a lot closer to the model coefficients (or at least within the confidence intervals).\nOne of the advantages of fitting these models in standard statistical frameworks (like OLS) is that we can use other diagnostics tools. For example, we can use ANOVA to compare the two model, to see if the additional complexity of the ITS model is worthwhile:\n\nRPython\n\n\n\nanova(fit0, fit1)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ T\nModel 2: Y ~ T + D + P\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    363 1222981                                  \n2    361  828505  2    394476 85.941 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(fit0, fit1)\n\n   df_resid           ssr  df_diff       ss_diff          F        Pr(&gt;F)\n0     363.0  1.142920e+06      0.0           NaN        NaN           NaN\n1     361.0  8.122294e+05      2.0  330690.71979  73.488689  1.679349e-27\n\n\n\n\n\nThe low \\(p\\)-value indicates that the more complex ITS model provides a better fit.\nLet’s now use this to visualize the model. First create a new data set to predict for, the plot the results:\n\nRPython\n\n\n\nwell_df$yhat &lt;- predict(fit1)\nhead(well_df)\n\n           Y T D P      yhat\n1  74.447922 1 0 0  8.108687\n2 -21.834909 2 0 0  8.572678\n3  25.056421 3 0 0  9.036669\n4  39.043130 4 0 0  9.500660\n5  28.113416 5 0 0  9.964650\n6   3.093774 6 0 0 10.428641\n\n\n\nggplot(well_df, aes(x = T)) + \n  geom_point(aes(y = Y), size = 3, alpha = 0.5) +\n  geom_line(data = well_df, aes(y = yhat), size = 2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nwell_df['yhat'] = fit1.predict()\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(well_df, x = \"T\", y = \"yhat\",ax=ax,\n            color=\"darkorange\", linewidth=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual\nWe can use the coefficients from the ITS model to calculate the counterfactual for the post-treatment period. The estimation of this is simple - we just set the values of \\(D\\) and \\(P\\) to zero (rather than the value we set above). In the following code, we first extract the model coefficients, then use these to estimate the factual and counterfactual for 20 days post-treatment.\n\nRPython\n\n\n\nb0 = coef(fit1)[1]\nb1 = coef(fit1)[2]\nb2 = coef(fit1)[3]\nb3 = coef(fit1)[4]\n\nFactual:\n\npost_time &lt;- 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\n(Intercept) \n   159.3938 \n\n\nCounterfactual:\n\nb0 + b1 * (200 + post_time) \n\n(Intercept) \n   109.7227 \n\n\n\n\n\nb0 = fit1.params['Intercept']\nb1 = fit1.params['T']\nb2 = fit1.params['D']\nb3 = fit1.params['P']\n\nFactual:\n\npost_time = 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\nnp.float64(165.4855648977787)\n\n\nCounterfactual:\n\nb0 + b1 * (200 + post_time)\n\nnp.float64(120.58648318541084)\n\n\n\n\n\nWhich should give you a difference of around +40 from the counterfactual. We can also predict these across a range of values, and compare with the factual values to show the effect over time. To do this we need to make a data frame that contains the values of the model variables (T, D, P) for both conditions. For the factual, we just use the values we created earlier. For the counterfactual, we repeat the time variable (T), but set both D and P to zero.\n\nRPython\n\n\n\npred_df &lt;- data.frame(T = rep(T, 2),\n                      D = c(D, rep(0, length(P))),\n                      P = c(P, rep(0, length(P))))\npred_df$yhat &lt;- predict(fit1, newdata = pred_df)\npred_df$D &lt;- as.factor(pred_df$D)\n\n\nggplot(well_df, aes(x = T)) + \n  geom_point(aes(y = Y), size = 3, alpha = 0.5) +\n  geom_line(data = pred_df, aes(y = yhat, col = D), size = 2) +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n\nT_pred = np.concatenate([T, T])\nD_pred = np.concatenate([D, np.repeat(0, len(D))])\nP_pred = np.concatenate([P, np.repeat(0, len(P))])\npred_df = pd.DataFrame({'T': T_pred,\n                        'D': D_pred,\n                        'P': P_pred\n})\n\n\npred_df['yhat'] = fit1.predict(pred_df)\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(pred_df, x = \"T\", y = \"yhat\", ax=ax, hue = \"D\",\n            linewidth=5)"
  },
  {
    "objectID": "GEOG6960_Week3.html#difference-in-differences",
    "href": "GEOG6960_Week3.html#difference-in-differences",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nDifference-in-difference models are an alternative approach to testing causality with time series data. These improve on the ITS approach by testing for changes in time and comparing these to any change in a control time series.\nThe base model for DID is:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 D\\times T\n\\]\nWhere:\n\n\\(Y\\) is the outcome\n\\(T\\) is time\n\\(D\\) is a binary indicator (control vs. treatment)\n\\(D \\times T\\) is the interaction between \\(T\\) and \\(D\\) and represents the quantity we’re interested in (i.e. the change in slope in the treated group)\n\n\nSimulated data\nAs before, we’ll start by creating a synthetic dataset. This will represent house prices for two locations. Unlike the previous example, where we had observations for multiple time steps, here we’ll just have value pre (0) and post (1) treatment. The treatment here represents the installation of subsidized housing between the two time steps, and the outcome of interest is house prices.\nTo start, we create two vectors of of 1000 binary values representing pre and post treatment (i.e. time) and control (0) or treated (1). We then estimate a house price for each of these using the following equation:\n\\[\nPrice = 50000 + 5000 \\times Treat + 43000 \\times Time +\n10000 \\times Treat \\times Time\n\\]\nThis means that: - Prices for control houses before the treatment are $50K - Prices for treated houses before the treatment are $50K + $5K = $55K - Prices for control houses increase by $43K after the treatment - Prices for treated houses increase by an additional $10K after the treatment\nFinally, we’ll add some noise to represent house-scale variability (\\(N(0, 10000)\\)).\n\nRPython\n\n\n\nTime = rep(c(0,1), 500)\n\nTreat = rep(c(0,0,1,1), 250)\n\ny = 50000 + 5000 * Treat + 43000 * Time + \n  10000 * Treat * Time\n\ne = rnorm(1000, 0, 10000)\ny = y + e\n\nAdd to data frame\n\nhouse_df = data.frame(Price = y,\n  Treat = as.factor(Treat),\n  Time = as.factor(Time))\n\nAnd plot:\n\nggline(house_df, x = \"Time\", y = \"Price\",\n       add = c(\"mean_se\", \"jitter\"), \n       color = \"Treat\", palette = \"jco\") \n\n\n\n\n\n\n\n\n\n\n\nTime = np.resize([0,1], 1000)\nTreat = np.resize([0,0,1,1], 1000)\n\ny = 50000 + 5000 * Treat + 43000 * Time + 10000 * Treat * Time\n\ne = np.random.normal(0, 10000, 1000)\ny = y + e\n\nAdd to data frame\n\nhouse_df = pd.DataFrame({'Price': y,\n                         'Treat': Treat,\n                         'Time': Time\n                         })\n\nAnd plot:\n\nfig, ax = plt.subplots()\nsns.stripplot(house_df, x = 'Time', y = 'Price', hue = 'Treat', alpha = 0.25)\nsns.pointplot(house_df, x = 'Time', y = 'Price', hue = 'Treat')\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple model\nAs before, we’ll start with simple OLS model, with the prices as a function of treatment and time. We’ll exclude the DID effect here, which makes the model:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D\n\\]\n\nRPython\n\n\n\nfit0 &lt;- lm(Price ~ Time + Treat, house_df)\ntab_model(fit0)\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n47048.67\n45919.81 – 48177.54\n&lt;0.001\n\n\nTime [1]\n48215.54\n46912.03 – 49519.04\n&lt;0.001\n\n\nTreat [1]\n10264.85\n8961.35 – 11568.36\n&lt;0.001\n\n\nObservations\n1000\n\n\nR2 / R2 adjusted\n0.847 / 0.846\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat', data=house_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.850\nMethod:                 Least Squares   F-statistic:                     2839.\nDate:                Sat, 02 Nov 2024   Prob (F-statistic):               0.00\nTime:                        16:58:21   Log-Likelihood:                -10658.\nNo. Observations:                1000   AIC:                         2.132e+04\nDf Residuals:                     997   BIC:                         2.134e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    4.78e+04    564.623     84.657      0.000    4.67e+04    4.89e+04\nTime        4.807e+04    651.970     73.733      0.000    4.68e+04    4.94e+04\nTreat       1.012e+04    651.970     15.525      0.000    8842.575    1.14e+04\n==============================================================================\nOmnibus:                        1.102   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.576   Jarque-Bera (JB):                1.153\nSkew:                           0.039   Prob(JB):                        0.562\nKurtosis:                       2.853   Cond. No.                         3.19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nWe get a pretty good model, but note that neither of the coefficients match the expected values from our simulated data (e.g. the effect of time is much larger than the base effect). Again (and I’m sure you’ve already understood this), this is because the model is merging the effects of time for the two groups together.\n\n\nDID model\nLet’s now fit the DID model to see if we get the expected coefficients. To do this, we simply need to add the interaction between Time and Treat to the model:\n\nRPython\n\n\n\nfit1 &lt;- lm(Price ~ Time * Treat, house_df)\ntab_model(fit1)\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n49804.99\n48546.68 – 51063.29\n&lt;0.001\n\n\nTime [1]\n42702.91\n40923.39 – 44482.42\n&lt;0.001\n\n\nTreat [1]\n4752.23\n2972.71 – 6531.74\n&lt;0.001\n\n\nTime [1] × Treat [1]\n11025.25\n8508.64 – 13541.87\n&lt;0.001\n\n\nObservations\n1000\n\n\nR2 / R2 adjusted\n0.857 / 0.857\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat + Time:Treat', data=house_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.859\nModel:                            OLS   Adj. R-squared:                  0.859\nMethod:                 Least Squares   F-statistic:                     2022.\nDate:                Sat, 02 Nov 2024   Prob (F-statistic):               0.00\nTime:                        16:58:21   Log-Likelihood:                -10629.\nNo. Observations:                1000   AIC:                         2.127e+04\nDf Residuals:                     996   BIC:                         2.129e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   5.023e+04    633.857     79.244      0.000     4.9e+04    5.15e+04\nTime        4.321e+04    896.410     48.205      0.000    4.15e+04     4.5e+04\nTreat       5261.6990    896.410      5.870      0.000    3502.631    7020.767\nTime:Treat  9720.5345   1267.715      7.668      0.000    7232.836    1.22e+04\n==============================================================================\nOmnibus:                        0.666   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.717   Jarque-Bera (JB):                0.746\nSkew:                           0.025   Prob(JB):                        0.689\nKurtosis:                       2.876   Cond. No.                         6.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAnd the results should be a much better match, with the model coefficients comparable to the values with used in creating the data. NB: of all the results here, the most important is the coefficient on the Time:Treat interaction. This is the casual effect in this model: the impact on house prices due to the addition of subsidized housing.\nAs in the previous section, we can also compare the two models with ANOVA, to see if including the DID term is helpful\n\nRPython\n\n\n\nanova(fit0, fit1)\n\nAnalysis of Variance Table\n\nModel 1: Price ~ Time + Treat\nModel 2: Price ~ Time * Treat\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1    997 1.0998e+11                                   \n2    996 1.0238e+11  1 7597261695 73.909 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(fit0, fit1)\n\n   df_resid           ssr  df_diff       ss_diff          F        Pr(&gt;F)\n0     997.0  1.059475e+11      0.0           NaN        NaN           NaN\n1     996.0  1.000420e+11      1.0  5.905549e+09  58.794581  4.150257e-14\n\n\n\n\n\nAnd again, the low \\(p\\)-value indicates that the DID model provides a better fit.\n\n\nCounterfactual\nEstimating the counterfactual is pretty straightforward. Here, it is the expected value of the treatment group without the DID effect, or in this case, the intercept plus the time effect plus the treatment effect.\nWe’ll now extract this, plus the estimate of the factual and the control group for plotting. As we only have two values for Time ([0,1]), we can simply work by adding together the model coefficients from the full DID model:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 D\\times T\n\\]\n\nControl at time 0: \\(\\beta_0\\)\nControl at time 1: \\(\\beta_0 + \\beta_1\\)\nTreatment at time 0: \\(\\beta_0 + \\beta_2\\)\nTreatment at time 1 (factual): \\(\\beta_0 + \\beta_2 + \\beta_1 + \\beta_3\\)\nTreatment at time 1 (counterfactual): \\(\\beta_0 + \\beta_2 + \\beta_1\\)\n\n\nRPython\n\n\nFirst extract the model coefficients:\n\ndid_coefs = coef(fit1)\ndid_coefs\n\n (Intercept)        Time1       Treat1 Time1:Treat1 \n   49804.985    42702.910     4752.228    11025.252 \n\n\nNow, we’ll make up vectors of estimates for the control, treatment, and the treatment with the counterfactual estimate at time = 1:\n\nyhat_control = c(did_coefs[1], did_coefs[1] + did_coefs[2])\nyhat_treatment = c(did_coefs[1] + did_coefs[3], \n                   did_coefs[1] + did_coefs[3] + did_coefs[2] + did_coefs[4])\nyhat_cf = c(did_coefs[1] + did_coefs[3], \n            did_coefs[1] + did_coefs[3] + did_coefs[2])\n\nCreate a data frame:\n\nplot_df = data.frame(Label = factor(rep(c(\"Control\", \"Treat\", \"CF\"), each = 2), \n                                    levels = c(\"Control\", \"Treat\", \"CF\")),\n                     Time = rep(c(0,1), 3),\n                     yhat = c(yhat_control, yhat_treatment, yhat_cf))\n\nAnd now plot:\n\nggplot(plot_df, aes(x = Time, y = yhat, col = Label)) +\n  geom_line(size = 2) +\n  scale_color_jco() +\n  theme_bw() +\n  theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\n\ndid_coefs = fit1.params.tolist()\ndid_coefs\n\n[50229.645722472276, 43211.59793472184, 5261.699019136594, 9720.534537801836]\n\n\n\nyhat_control = [did_coefs[0], did_coefs[0] + did_coefs[1]]\nyhat_treatment = [did_coefs[0] + did_coefs[2], \n                   did_coefs[0] + did_coefs[2] + did_coefs[1] + did_coefs[3]]\nyhat_cf = [did_coefs[0] + did_coefs[2], \n            did_coefs[0] + did_coefs[2] + did_coefs[1]]\n\n\npred_df = pd.DataFrame({'Label': np.repeat([\"Control\", \"Treat\", \"CF\"], 2),\n                        'Time': np.resize([0,1], 6),\n                        'yhat': np.concatenate([yhat_control, yhat_treatment, yhat_cf])\n                         })\n\n\nsns.lineplot(pred_df, x = \"Time\", y = \"yhat\", hue = \"Label\")\n\n\n\n\n\n\n\n\n\n\n\nThe DID effect can be seen here clearly as the difference in the treatment and CF estimates at time = 1."
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "Week 3: ITS and DID",
    "section": "",
    "text": "Also need scipy and the statsmodel\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nnp.random.seed(42)\nT = np.arange(365)\n\nD = np.where(T &gt; 200, 1, 0)\nP = T - 200\nP = np.where(T &lt;= 200, 0, P)\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\nY\n\narray([  5.4,   5.9,   6.4,   6.9,   7.4,   7.9,   8.4,   8.9,   9.4,\n         9.9,  10.4,  10.9,  11.4,  11.9,  12.4,  12.9,  13.4,  13.9,\n        14.4,  14.9,  15.4,  15.9,  16.4,  16.9,  17.4,  17.9,  18.4,\n        18.9,  19.4,  19.9,  20.4,  20.9,  21.4,  21.9,  22.4,  22.9,\n        23.4,  23.9,  24.4,  24.9,  25.4,  25.9,  26.4,  26.9,  27.4,\n        27.9,  28.4,  28.9,  29.4,  29.9,  30.4,  30.9,  31.4,  31.9,\n        32.4,  32.9,  33.4,  33.9,  34.4,  34.9,  35.4,  35.9,  36.4,\n        36.9,  37.4,  37.9,  38.4,  38.9,  39.4,  39.9,  40.4,  40.9,\n        41.4,  41.9,  42.4,  42.9,  43.4,  43.9,  44.4,  44.9,  45.4,\n        45.9,  46.4,  46.9,  47.4,  47.9,  48.4,  48.9,  49.4,  49.9,\n        50.4,  50.9,  51.4,  51.9,  52.4,  52.9,  53.4,  53.9,  54.4,\n        54.9,  55.4,  55.9,  56.4,  56.9,  57.4,  57.9,  58.4,  58.9,\n        59.4,  59.9,  60.4,  60.9,  61.4,  61.9,  62.4,  62.9,  63.4,\n        63.9,  64.4,  64.9,  65.4,  65.9,  66.4,  66.9,  67.4,  67.9,\n        68.4,  68.9,  69.4,  69.9,  70.4,  70.9,  71.4,  71.9,  72.4,\n        72.9,  73.4,  73.9,  74.4,  74.9,  75.4,  75.9,  76.4,  76.9,\n        77.4,  77.9,  78.4,  78.9,  79.4,  79.9,  80.4,  80.9,  81.4,\n        81.9,  82.4,  82.9,  83.4,  83.9,  84.4,  84.9,  85.4,  85.9,\n        86.4,  86.9,  87.4,  87.9,  88.4,  88.9,  89.4,  89.9,  90.4,\n        90.9,  91.4,  91.9,  92.4,  92.9,  93.4,  93.9,  94.4,  94.9,\n        95.4,  95.9,  96.4,  96.9,  97.4,  97.9,  98.4,  98.9,  99.4,\n        99.9, 100.4, 100.9, 101.4, 101.9, 102.4, 102.9, 103.4, 103.9,\n       104.4, 104.9, 105.4, 127.1, 128.8, 130.5, 132.2, 133.9, 135.6,\n       137.3, 139. , 140.7, 142.4, 144.1, 145.8, 147.5, 149.2, 150.9,\n       152.6, 154.3, 156. , 157.7, 159.4, 161.1, 162.8, 164.5, 166.2,\n       167.9, 169.6, 171.3, 173. , 174.7, 176.4, 178.1, 179.8, 181.5,\n       183.2, 184.9, 186.6, 188.3, 190. , 191.7, 193.4, 195.1, 196.8,\n       198.5, 200.2, 201.9, 203.6, 205.3, 207. , 208.7, 210.4, 212.1,\n       213.8, 215.5, 217.2, 218.9, 220.6, 222.3, 224. , 225.7, 227.4,\n       229.1, 230.8, 232.5, 234.2, 235.9, 237.6, 239.3, 241. , 242.7,\n       244.4, 246.1, 247.8, 249.5, 251.2, 252.9, 254.6, 256.3, 258. ,\n       259.7, 261.4, 263.1, 264.8, 266.5, 268.2, 269.9, 271.6, 273.3,\n       275. , 276.7, 278.4, 280.1, 281.8, 283.5, 285.2, 286.9, 288.6,\n       290.3, 292. , 293.7, 295.4, 297.1, 298.8, 300.5, 302.2, 303.9,\n       305.6, 307.3, 309. , 310.7, 312.4, 314.1, 315.8, 317.5, 319.2,\n       320.9, 322.6, 324.3, 326. , 327.7, 329.4, 331.1, 332.8, 334.5,\n       336.2, 337.9, 339.6, 341.3, 343. , 344.7, 346.4, 348.1, 349.8,\n       351.5, 353.2, 354.9, 356.6, 358.3, 360. , 361.7, 363.4, 365.1,\n       366.8, 368.5, 370.2, 371.9, 373.6, 375.3, 377. , 378.7, 380.4,\n       382.1, 383.8, 385.5, 387.2, 388.9, 390.6, 392.3, 394. , 395.7,\n       397.4, 399.1, 400.8, 402.5, 404.2])\nplt.plot(Y)\nerr = np.random.normal(0, 50, 365)\nY = Y + err\nplt.plot(Y, 'bo')\n#well_df &lt;- as.data.frame(cbind(Y, T, D, P)) \nwell_df = pd.DataFrame({'Y': Y,\n                        'T': T,\n                        'D': D,\n                        'P': P})\nsns.scatterplot(well_df,\n                x = \"T\",\n                y = \"Y\", \n                alpha = 0.75)\nplt.axvline(x=200)\nmod = smf.ols(formula='Y ~ T', data=well_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.817\nModel:                            OLS   Adj. R-squared:                  0.816\nMethod:                 Least Squares   F-statistic:                     1618.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          8.38e-136\nTime:                        16:50:59   Log-Likelihood:                -1986.9\nNo. Observations:                 365   AIC:                             3978.\nDf Residuals:                     363   BIC:                             3986.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -53.6683      5.862     -9.155      0.000     -65.196     -42.141\nT              1.1211      0.028     40.218      0.000       1.066       1.176\n==============================================================================\nOmnibus:                        0.840   Durbin-Watson:                   1.519\nProb(Omnibus):                  0.657   Jarque-Bera (JB):                0.939\nSkew:                          -0.068   Prob(JB):                        0.625\nKurtosis:                       2.792   Cond. No.                         420.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nmod = smf.ols(formula='Y ~ T + D + P', data=well_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.870\nModel:                            OLS   Adj. R-squared:                  0.869\nMethod:                 Least Squares   F-statistic:                     803.5\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          2.35e-159\nTime:                        16:50:59   Log-Likelihood:                -1924.6\nNo. Observations:                 365   AIC:                             3857.\nDf Residuals:                     361   BIC:                             3873.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4776      6.667     -0.372      0.710     -15.588      10.632\nT              0.5594      0.058      9.701      0.000       0.446       0.673\nD             22.9193      9.991      2.294      0.022       3.271      42.568\nP              1.0990      0.097     11.308      0.000       0.908       1.290\n==============================================================================\nOmnibus:                        4.812   Durbin-Watson:                   2.136\nProb(Omnibus):                  0.090   Jarque-Bera (JB):                5.520\nSkew:                           0.134   Prob(JB):                       0.0633\nKurtosis:                       3.539   Cond. No.                         908.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nsm.stats.anova_lm(fit0, fit1)\n\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n363.0\n1.142920e+06\n0.0\nNaN\nNaN\nNaN\n\n\n1\n361.0\n8.122294e+05\n2.0\n330690.71979\n73.488689\n1.679349e-27\nwell_df['yhat'] = fit1.predict()\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(well_df, x = \"T\", y = \"yhat\", ax=ax, color = \"darkorange\", linewidth=5)\nfit1.params\n\nIntercept    -2.477644\nT             0.559382\nD            22.919297\nP             1.098989\ndtype: float64\nb0 = fit1.params['Intercept']\nb1 = fit1.params['T']\nb2 = fit1.params['D']\nb3 = fit1.params['P']\npost_time = 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\nnp.float64(165.4855648977787)\nb0 + b1 * (200 + post_time) \n\nnp.float64(120.58648318541084)\nT_pred = np.concatenate([T, T])\nD_pred = np.concatenate([D, np.repeat(0, len(D))])\nP_pred = np.concatenate([P, np.repeat(0, len(P))])\nnp.repeat([0,1,2,3],2)\n\narray([0, 0, 1, 1, 2, 2, 3, 3])\npred_df = pd.DataFrame({'T': T_pred,\n                        'D': D_pred,\n                        'P': P_pred\n})\npred_df.head()\npred_df['yhat'] = fit1.predict(pred_df)\nplt.plot(pred_df['T'], 'bo')\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(pred_df, x = \"T\", y = \"yhat\", ax=ax, hue = \"D\", linewidth=5)"
  },
  {
    "objectID": "Week3.html#did",
    "href": "Week3.html#did",
    "title": "Week 3: ITS and DID",
    "section": "DID",
    "text": "DID\n\nTime = np.resize([0,1], 1000)\nTreat = np.resize([0,0,1,1], 1000)\n\ny = 50000 + 5000 * Treat + 43000 * Time + 10000 * Treat * Time\n\ne = np.random.normal(0, 10000, 1000)\ny = y + e\n\nhouse_df = pd.DataFrame({'Price': y,\n                         'Treat': Treat,\n                         'Time': Time\n                         })\n\n\nf, ax = plt.subplots()\nsns.stripplot(house_df, x = 'Time', y = 'Price', hue = 'Treat', alpha = 0.25)\nsns.pointplot(house_df, x = 'Time', y = 'Price', hue = 'Treat')\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat', data=house_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.850\nMethod:                 Least Squares   F-statistic:                     2839.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):               0.00\nTime:                        16:51:01   Log-Likelihood:                -10658.\nNo. Observations:                1000   AIC:                         2.132e+04\nDf Residuals:                     997   BIC:                         2.134e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    4.78e+04    564.623     84.657      0.000    4.67e+04    4.89e+04\nTime        4.807e+04    651.970     73.733      0.000    4.68e+04    4.94e+04\nTreat       1.012e+04    651.970     15.525      0.000    8842.575    1.14e+04\n==============================================================================\nOmnibus:                        1.102   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.576   Jarque-Bera (JB):                1.153\nSkew:                           0.039   Prob(JB):                        0.562\nKurtosis:                       2.853   Cond. No.                         3.19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat + Time:Treat', data=house_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.859\nModel:                            OLS   Adj. R-squared:                  0.859\nMethod:                 Least Squares   F-statistic:                     2022.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):               0.00\nTime:                        16:51:01   Log-Likelihood:                -10629.\nNo. Observations:                1000   AIC:                         2.127e+04\nDf Residuals:                     996   BIC:                         2.129e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   5.023e+04    633.857     79.244      0.000     4.9e+04    5.15e+04\nTime        4.321e+04    896.410     48.205      0.000    4.15e+04     4.5e+04\nTreat       5261.6990    896.410      5.870      0.000    3502.631    7020.767\nTime:Treat  9720.5345   1267.715      7.668      0.000    7232.836    1.22e+04\n==============================================================================\nOmnibus:                        0.666   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.717   Jarque-Bera (JB):                0.746\nSkew:                           0.025   Prob(JB):                        0.689\nKurtosis:                       2.876   Cond. No.                         6.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndid_coefs = fit1.params.tolist()\ndid_coefs\n\n[50229.645722472276, 43211.59793472184, 5261.699019136594, 9720.534537801836]\n\n\n\nyhat_control = [did_coefs[0], did_coefs[0] + did_coefs[1]]\nyhat_treatment = [did_coefs[0] + did_coefs[2], \n                   did_coefs[0] + did_coefs[2] + did_coefs[1] + did_coefs[3]]\nyhat_cf = [did_coefs[0] + did_coefs[2], \n            did_coefs[0] + did_coefs[2] + did_coefs[1]]\n\n\nnp.repeat([\"Control\", \"Treat\", \"CF\"], 2)\nnp.resize([0,1], 6)\n\narray([0, 1, 0, 1, 0, 1])\n\n\n\nnp.concatenate([yhat_control, yhat_treatment, yhat_cf])\n\narray([ 50229.64572247,  93441.24365719,  55491.34474161, 108423.47721413,\n        55491.34474161,  98702.94267633])\n\n\n\npred_df = pd.DataFrame({'Label': np.repeat([\"Control\", \"Treat\", \"CF\"], 2),\n                        'Time': np.resize([0,1], 6),\n                        'yhat': np.concatenate([yhat_control, yhat_treatment, yhat_cf])\n                         })\n\n\nsns.lineplot(pred_df, x = \"Time\", y = \"yhat\", hue = \"Label\")"
  },
  {
    "objectID": "Week3.html#first-test",
    "href": "Week3.html#first-test",
    "title": "Week 3: ITS and DID",
    "section": "First test",
    "text": "First test\n\nstudent_df['enem_score1'][student_df['tablet'] == 1]\n\n\n#from scipy import stats\nfrom statsmodels.stats.weightstats import ttest_ind\nt_stat, p_value, df = ttest_ind(student_df[student_df['tablet'] == 1]['enem_score1'], \n                                student_df[student_df['tablet'] == 0]['enem_score1'])\n\n\nprint(f'P-value: {p_value}')\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          hue = \"tablet\")"
  },
  {
    "objectID": "Week3.html#rct",
    "href": "Week3.html#rct",
    "title": "Week 3: ITS and DID",
    "section": "RCT",
    "text": "RCT\n\nschool_df['tablet_rct'] = school_df['tablet'].sample(n_schools).to_numpy()\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet_rct\", order=school_df.sort_values('tuition').id)\n\n\nstudent_df['tablet_rct'] = np.repeat(school_df['tablet_rct'], class_size)\nstudent_df['enem_score2'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet_rct']\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          hue = \"tablet_rct\")\n\n\nmod = smf.ols(formula='enem_score2 ~ tablet_rct', data=student_df)\nfit = mod.fit()\nprint(fit.summary())"
  },
  {
    "objectID": "Week3.html#psm",
    "href": "Week3.html#psm",
    "title": "Week 3: ITS and DID",
    "section": "PSM",
    "text": "PSM\n\nmod = smf.glm(formula='tablet ~ tuition', data=school_df, family=sm.families.Binomial())\nfit = mod.fit()\nprint(fit.summary())\n\n\nfit.predict()\n\n\nprs_df = pd.DataFrame({'prop_score': fit.predict(),\n                        'tablet': school_df['tablet'],\n                        'tuition': school_df['tuition']})\nprs_df.head()\n\n\ntreated_df = prs_df[prs_df['tablet'] == 1].reset_index()\ncontrol_df = prs_df[prs_df['tablet'] == 0].reset_index()\n\n\ntreated_df.iloc[0,:]\n\n\nabs_diff = (treated_df['prop_score'][0] - control_df['prop_score']).abs()\nmatch_id = abs_diff.idxmin()\nprint(match_id)\n\n\ncontrol_df.iloc[match_id,:]\n\n\nschool_df.columns\n\n\nfrom psmpy import PsmPy\npsm = PsmPy(school_df, treatment='tablet', indx='id', exclude = ['tablet_rct'])\n\n\npsm.logistic_ps(balance = False)\n\n\npsm.predicted_data.head()\n\n\npsm.knn_matched(matcher='propensity_logit', replacement=False, caliper=None)\n\n\npsm.plot_match()\n\n\npsm.effect_size_plot(save=False)\n\n\npsm.knn_matched(matcher='propensity_logit', replacement=True, caliper=None)\n\n\npsm.effect_size_plot(save=False)\n\n\npsm.effect_size\n\n\npsm.df_matched\n\n\nfig, axs = plt.subplots(ncols=2)\nsns.histplot(school_df, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[0]).set(title='Before')\nsns.histplot(psm.df_matched, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[1]).set(title='After')\n\n\nfig, axs = plt.subplots(ncols=2)\nsns.ecdfplot(school_df, x = \"tuition\", hue=\"tablet\", ax=axs[0]).set(title='Before')\nsns.ecdfplot(psm.df_matched, x = \"tuition\", hue=\"tablet\", ax=axs[1]).set(title='After')\n\n\nmatch_df = psm.df_matched\n\n\nmatch_df.head()\n\n\nstudent_df.columns\n\n\nmatched_student_df = pd.DataFrame(columns=student_df.columns)\nfor idx, row in match_df.iterrows():\n    print(row['id'])\n    tmp_df = student_df[student_df['school_id'] == row['id']]\n    matched_student_df = pd.concat([matched_student_df, tmp_df], ignore_index = True)\n\n\nmatched_student_df\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=matched_student_df)\nfit = mod.fit()\nprint(fit.summary())"
  },
  {
    "objectID": "Week3.html#ipw",
    "href": "Week3.html#ipw",
    "title": "Week 3: ITS and DID",
    "section": "IPW",
    "text": "IPW\n\nprs_df\n\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\n\nprs_df\n\n\nsns.scatterplot(prs_df, x = \"prop_score\", y = \"ipw\", hue = \"tablet\")\n\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\n\nstudent_df\n\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())"
  },
  {
    "objectID": "GEOG6960_Week5.html",
    "href": "GEOG6960_Week5.html",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "",
    "text": "In this lab, we’re going to explore how to build and analyze directed acyclic graphs (DAGs) with DAGitty. We’ll also briefly introduce R and Python packages that work with DAGs. This tutorial is lightly modified from the DAGitty tutorial located here."
  },
  {
    "objectID": "GEOG6960_Week5.html#introduction",
    "href": "GEOG6960_Week5.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "",
    "text": "In this lab, we’re going to explore how to build and analyze directed acyclic graphs (DAGs) with DAGitty. We’ll also briefly introduce R and Python packages that work with DAGs. This tutorial is lightly modified from the DAGitty tutorial located here."
  },
  {
    "objectID": "GEOG6960_Week5.html#getting-started",
    "href": "GEOG6960_Week5.html#getting-started",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Getting Started",
    "text": "Getting Started\nGo to Dagitty.net and click on “Launch DAGitty Online in your Browser”. This will open the main DAGitty screen, with an example DAG. The screen is split into three panels:\n\nThe left panel has options to modify the existing graph\nThe center contains the main canvas with the DAG\nThe right shows information about the current DAG (independnce, adjustment, etc)\n\n ## Building a Diagram\nTo create a new DAG, go to the [Model] menu at the top of the center panel and select [New Model], to open a blank canvas. You can now add variables to this by clicking on an empty part of the diagram. You will be asked to give the variable a name. For now, create two variables (X and Y):\n\n\n\nAdding variables\n\n\nIf you look at the right panel, you’ll see that the Model code starts to fill out. This is the code description of the model you are creating, and can be used in the offline version of DAGitty. You can also modify this directly, e.g. to set the coordinates of eahc node by hand.\n\n\n\nModel code\n\n\nNow, let’s add an edge connecting X and Y. To add an directional arrow (X-&gt;Y), first click on X and then on Y. You can remove an existing arrow in the same way. Once you have an arrow from X to Y, you can create a double-headed arrow by reversing this, i.e. clicking first on Y then on X.\nIf you hover the cursor over the arrow, you’ll see it change to a 4-way arrow. You can now click and drag the arrow to maked it curved. Similarly, you can click-and-drag the variables themselves."
  },
  {
    "objectID": "GEOG6960_Week5.html#packages",
    "href": "GEOG6960_Week5.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggsci)\nlibrary(sjPlot)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nIf you select the “Adjustment (total effect)” dropdown you’ll see that you can also set it to look for instrumental variables, or to look for the “direct effect”. The direct effect looks only for X -&gt; Y, and not counting for other front-door paths like X -&gt; C -&gt; Y (not pictured)."
  },
  {
    "objectID": "GEOG6960_Week5.html#variable-characteristics",
    "href": "GEOG6960_Week5.html#variable-characteristics",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Variable Characteristics",
    "text": "Variable Characteristics\nOnce you have some variables and arrows on your graph, you may want to manipulate them. If you click on the X variable, it will be highlit with a thick border. You can now use the top-left menu on the left to modify this.\n\n\n\nModel with path\n\n\nYou can delete or rename from this menu, but most importantly for causal models, you can set the exposure or treatment variable and the outcome variable by clicking the appropriate check box in this menu. Use this to set X as the exposure and Y as the outcome:\n\n\n\nCausal Model\n\n\nThe exposure variable is now represented as a green node with an arrow, and the outcome as a blue node with a vertical line. You’ll also notice that the arrow between them turns green, indicating a causal path (you can check this on the legend at the bottom of the panel).\nDAGitty also has a series of hotkeys for working with the variables:\n\ne: sets the selected variable as the exposure\no: sets the selected variable as the outcome\na: sets the selected variable to be controlled or adjusted\nu: defines an unobserved variable (this is something that is theoretically important, but that you don;t have data for)\nd: deletes the selected variable\nr: renames the selected variable\n\nNow let’s add the third variable Z as a confounder. Click on an empty part of the diagram, create a variable Z, and create arrows from it to both X and Y\n\n\n\nConfounder\n\n\nOnce you add both parts, they (and the Z node) will turn purple, indicating a biasing path - a variable that can bias your estimation of a causal relationship. We can get more information about this (and the graph in general) in the right panel."
  },
  {
    "objectID": "GEOG6960_Week5.html#dagitty-diagnostics",
    "href": "GEOG6960_Week5.html#dagitty-diagnostics",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "DAGitty Diagnostics",
    "text": "DAGitty Diagnostics\nThe right bar of the screen contains lots of information about your graph.\nAt the top you’ll see that it gives you the Minimal Adjustment Sets. Given the defined exposure and outcome, this shows if a biasing (or a backdoor) path is open. Here there is one and further down in the panel it shows which variables have to be controlled for (or adjusted) to get an unbiased estimated of the causal effect. By default this should be set to the ‘total effect’, so this shows what adjustment is necessary to obtain the total causal effect. We’ll look below at how to use this to identify direct effects. For this graph, the only required adjustment is on Z, the confounding variable.\nRemember that the exposure and outcome variables must be set properly for this to work. It will also pay attention to the variables you’ve set to be unobserved, or the variables you’ve said you’ve already adjusted for. The diagram itself also has information. Any lines that are purple represent arrows that are a part of an open back door. Lines that are green represent the causal effect of interest (either direct or indirect). And lines that are black are neither. If you forget the colors, check the legend in the bottom-left panel.\nIn the second panel on the right you can see the testable implications of the model. This list all independencies in the model (marginal or conditional) between variables, and so implies any relationships that could be tested. There are no independencies in this graph, but to show what this shows, add a new ancestor variable A, that feeds into X.\n\n\n\nAncestor variable\n\n\nIf you now look at the testable implications panel, it shows two independencies:\n\n\\(A\\perp\\!\\!\\!\\!\\!\\perp Y|X,Z\\): A and Y are conditionally independent if X and Z are controlled/adjusted. So, adjusting for X and Z, A and Y should be unrelated. If you have data for all of these variables, you could then test these with a model (i.e. model Y ~ A + X + Z. If no significant relationship was found between Y ~ A, this would support the DAG. If you found they were related, you’d know the graph was wrong and needed to be updated. This notion of the important of dependencies/independencies will be important in structural equation modeling.\n\\(A\\perp\\!\\!\\!\\!\\!\\perp Z\\): A and Z are fully independent (i.e. there’s no path that links them together)"
  },
  {
    "objectID": "GEOG6960_Week5.html#other-graphs",
    "href": "GEOG6960_Week5.html#other-graphs",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Other graphs",
    "text": "Other graphs\n\nCollider\nNow make a new graph to represent a collider. Use X as the exposure, Y as the outcome, and Z as the collider. As a reminder, the collider has arrows coming in from both the exposure and outcome.\n When you have built it, there should be no open biasing paths. Try adjusting for Z to see the impact on the graph, and on the information on biases.\n\n\nChain\nAnd now make a graph representing a chain:\n\n\n\nChain\n\n\nNote that this graph shows the two causal pathways in green: direct (X-&gt; Y) and indirect (X -&gt; Z -&gt; Y). In the top-right, there is no adjustment set listed, as DAGitty is looking at the total effect (i.e. the direct + the indirect). If you now change the drop-down menu to show the adjustment for the direct effect, it will list Z, and state that there are biasing paths. If you now modify Z so that it is adjusted, the bias is removed, so this model would return an unbiased estimated of the direct effect.\n\n\nGrace and Keely (2006)\nAs a more complex example of a DAG, we’ll now use DAGitty to recreate the DAG from Grace & Kelly’s 2006 paper. Create a new model in DAGitty, andd then start adding the variables and paths in the figure below (feel free to shorten the variable names).\n\n\n\nSEM from Grace and Keely (2006)\n\n\nOnce you have made the DAG, set fire severity as the exposure and richness as the outcome. Before we look at the adjustment sets, take a quick look at the ‘Testable assumptions’. There are a large number listed here, but all of these indicate a potential missing or blocked path. To illustrate why this is useful, let’s consider the first one listed: \\(\\mbox{Landscape Position}\\perp\\!\\!\\!\\!\\!\\perp \\mbox{Fire Severity}|\\mbox{Stand Age}\\). This implies that if we control for stand age, there should be no remaining relationship between fire severity and landscape position. This follows the graph, which indicates that the stand age is a causal result of the position on the landscape (i.e. the spatial context) and the fire severity is a causal result of age. It’s important to note here that this is the assumption made in creating the diagram, but we could now go ahead and test this. If we had data on landscape position, stand age and fire severity, we could build the following model:\nseverity ~ landscape + stand_age\nIf a non-significant relationship was found with landscape, this would support our DAG. We could continue this and explore the other implications in the same way, all of which would help either support the graph, or lead to modifications.\nThe graph also shows a number of biasing paths. Check the top right - what variable(s) do you need to adjust or control for to remove these biases? Try adjusting these and see if it resolves the problem. From this, what variables would you need in a model to estimate the causal effect of fire severity on richness?\nrichness ~ fire_severity + ...\n\n\nMosquito net example\nNext, we’ll reproduce the DAG illustrating the effects of mosquito net use on malaria risk, taken from Andrew Heiss’ blog and shown below.\n\n\n\nMosquito net DAG\n\n\nAs before, start by adding and naming the relevant nodes, then add the links between them. If you are stuck for time, you can also copy-paste the DAG code below into the ‘Model code’ box on the right panel to make the graph.\ndag {\nbb=\"0,0,1,1\"\n\"Household number\" [pos=\"0.140,0.400\"]\n\"Malaria risk\" [pos=\"0.540,0.400\"]\n\"Mosquito net\" [pos=\"0.340,0.400\"]\nEligible [pos=\"0.140,0.200\"]\nHealth [pos=\"0.440,0.600\"]\nIncome [pos=\"0.340,0.201\"]\nResistance [pos=\"0.740,0.400\"]\nT_night [pos=\"0.540,0.200\"]\n\"Household number\" -&gt; \"Mosquito net\"\n\"Household number\" -&gt; Eligible\n\"Mosquito net\" -&gt; \"Malaria risk\"\nEligible -&gt; \"Mosquito net\"\nHealth -&gt; \"Malaria risk\"\nHealth -&gt; \"Mosquito net\"\nIncome -&gt; \"Malaria risk\"\nIncome -&gt; \"Mosquito net\"\nIncome -&gt; Eligible\nIncome -&gt; Health\nResistance -&gt; \"Malaria risk\"\nT_night -&gt; \"Malaria risk\"\nT_night -&gt; \"Mosquito net\"\n}\nNow set the exposure (Mosquito net) and the outcome (malaria risk). Check the top right panel for the adjustment set (this should be Health, Income and Nighttime temp). Each of these is a confounder in the model: - Health: poor health increases both risk and likelihood of net use - Income: increases likelihood of net use but decreases risk (e.g. air conditioner use) - Temperatures: Increases risk (mosquito activity) and may decrease net use\nMake these adjustments, and check that your new, adjusted graph has no more biasing paths (it should say Correctly adjusted in the top right panel). Given these adjustments, what would your model need to include?\nmalaria_risk ~ mosquito_net + ...\nAsa test, you can now see what would happen if we were to update the graph. We’ll imagine that the eligibility for the government funded net program also depends on whether the mosquitos in an areas are resistant. Add a path from resistance to eligibility and see how it changes the model. What would now need to include in your model to correctly adjust it?"
  },
  {
    "objectID": "GEOG6960_Week5.html#grace-and-keely-2006",
    "href": "GEOG6960_Week5.html#grace-and-keely-2006",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Grace and Keely (2006)",
    "text": "Grace and Keely (2006)\nAs a more complex example of a DAG, we’ll now use DAGitty to recreate the DAG from Grace & Kelly’s 2006 paper. Create a new model in DAGitty, andd then start adding the variables and paths in the figure below (feel free to shorten the variable names).\n\n\n\nSEM from Grace and Keely (2006)\n\n\nOnce you have made the DAG, set fire severity as the exposure and richness as the outcome. The graph should show a number of biasing paths. Check the top right - what variable(s) do you need to adjust or control for to remove these biases? Try adjusting these and see if it resolves the problem. What would your model look like to account for this?\nabundance ~ fire_severity + ..."
  },
  {
    "objectID": "GEOG6960_Week5.html#coding-dags",
    "href": "GEOG6960_Week5.html#coding-dags",
    "title": "GEOG 6960 Causality in Geog. Studies 5",
    "section": "Coding DAGs",
    "text": "Coding DAGs\nBoth R and Python have packages that allow to create DAGs programmatically:\n\nR: dagitty and ggdag\nPython: causalgraphicalmodels (note this only seems to work in Python 3.9 or earlier). You will also need graphviz and possibly python-graphviz to render the graphs\n\nWe’ll illustrate these very quickly here with a subset of the Grace and Kelly DAG (stand age, fire severity, plant abundance and richness). Fist load the relevant packages:\n\nRPython\n\n\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n\n\n\nfrom causalgraphicalmodels import CausalGraphicalModel\n\n\n\n\n\nCreating a DAG\n\nRPython\n\n\nWe’ll start by creating the DAG. In R the main function is dagify(). This takes a series of arguments:\n\nThe set of paths to include. These are specified using R’s formula syntax, with the destination node on the left hand side and the origin on the right. So X ~ Y would make a path going from X to Y. Note that if Y has more than one path pointing to it, you should include both in the same formula. So Y ~ X + Z, with have two paths to Y from X and Z respectively\nThe coordinates specifying the position of each node\nWhich variable to use for the exposure and outcome [optional]\nLabels for each node [optional]\n\n\ngrace_dag &lt;- dagify(fire_sev ~ stand_age,\n                    abundance ~ fire_sev,\n                    richness ~ abundance + stand_age,\n                    coords = list(x = c(stand_age = 1,\n                                        fire_sev = 2,\n                                        abundance = 3,\n                                        richness = 4), \n                                  y = c(stand_age = 1,\n                                        fire_sev = 1,\n                                        abundance = 1,\n                                        richness = 2)\n                    ),\n                    exposure = \"fire_sev\",\n                    outcome = \"richness\",\n                    labels = c(stand_age = \"Stand age\", \n                               fire_sev = \"Fire severity\", \n                               abundance = \"Plant abundance\",\n                               richness = \"Richness\")\n)\n\n\n\nIn Python, there are two arguments:\n\nA list of nodes\nA list of tuples defining each path (\"origin\", \"destination\")\n\n\ngrace_dag = CausalGraphicalModel(\n    nodes=[\"stand_age\", \"fire_severity\", \"plant_abundance\", \"richness\"],\n    edges=[\n        (\"stand_age\", \"fire_severity\"), \n        (\"stand_age\", \"richness\"), \n        (\"fire_severity\", \"plant_abundance\"),\n        (\"plant_abundance\", \"richness\")\n    ]\n)\n\n\n\n\n\nRPython\n\n\nThe base function for this in R is ggdag. This is built on top of ggplot2, so you can use functions from the package to update your plot. Here we’ll just add a theme to give a blank background:\n\nggdag(grace_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\nBy default, this just uses the variable names as labels, which (as you can see) don’t fit well with the nodes. We can update this to show the manually defined labels, and the exposure/outcome variables as follows:\n\nggdag_status(grace_dag, use_labels = \"label\", text = FALSE) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nIn Python, the graphical model object has a method to plot it using networkx:\n\ngrace_dag.draw()\n\n&lt;graphviz.graphs.Digraph object at 0x130044880&gt;\n\n\n\n\n\nYou can get the list of conditional independencies as follows:\n\nRPython\n\n\n\nimpliedConditionalIndependencies(grace_dag)\n\nabnd _||_ stn_ | fr_s\nfr_s _||_ rchn | abnd, stn_\n\n\nHere the syntax is var1 is conditionally independent (_||_) on var2 given (|) var3\n\n\n\ngrace_dag.get_all_independence_relationships()\n\n[('stand_age', 'plant_abundance', {'fire_severity'}), ('richness', 'fire_severity', {'stand_age', 'plant_abundance'})]\n\n\nHere the syntax is ((var1, var2, {var3})): var1 is conditionally independent on var2 given var3\n\n\n\nThese show that:\n\nstand age and abundance are conditionally independent given fire severity\nfire severity and richness are conditionally independent given abundance and stand age\n\nThe DAGs can be used to show all possible paths between the exposure (fire) and outcome (richness):\n\nRPython\n\n\n\npaths(grace_dag)\n\n$paths\n[1] \"fire_sev -&gt; abundance -&gt; richness\" \"fire_sev &lt;- stand_age -&gt; richness\"\n\n$open\n[1] TRUE TRUE\n\n\nYou can also visualize these:\n\nggdag_paths(grace_dag)\n\n\n\n\n\n\n\n\n\n\nIn Python, we can extract the backdoor paths - these are usually paths through a confounding variable (here, the stand age)\n\ngrace_dag.get_all_backdoor_paths(\"fire_severity\", \"richness\")\n\n[['fire_severity', 'stand_age', 'richness']]\n\n\n\n\n\nFinally, we can use these DAGs to identify the adjustment sets\n\nRPython\n\n\nIn R, we can extract these directly, as the exposure and outcome were preset when making the DAG. Note that you can specify these in this function, if you want to test a different part of the graph.\n\nadjustmentSets(grace_dag)\n\n{ stand_age }\n\n\nYou can also visualize these:\n\nggdag_adjustment_set(grace_dag)\n\n\n\n\n\n\n\n\n\n\nTo get the adjustment set in Python, we need to specify the exposure and outcome.\n\ngrace_dag.get_all_backdoor_adjustment_sets(\"fire_severity\", \"richness\")\n\nfrozenset({frozenset({'stand_age'})})"
  },
  {
    "objectID": "GEOG6960_Week6.html",
    "href": "GEOG6960_Week6.html",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "",
    "text": "In this lab, we’re going to explore how to build and test structural equation models (SEMs). We’ll use two examples: - The Grace and Keeley fire/plant abundance dataset (keeley.csv) - The mosquito net usage dataset (mosquito_nets.csv)"
  },
  {
    "objectID": "GEOG6960_Week6.html#introduction",
    "href": "GEOG6960_Week6.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "",
    "text": "In this lab, we’re going to explore how to build and test structural equation models (SEMs). We’ll use two examples: - The Grace and Keeley fire/plant abundance dataset (keeley.csv) - The mosquito net usage dataset (mosquito_nets.csv)"
  },
  {
    "objectID": "GEOG6960_Week6.html#coding-dags",
    "href": "GEOG6960_Week6.html#coding-dags",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Coding DAGs",
    "text": "Coding DAGs\nBoth R and Python have packages that allow to create DAGs programmatically:\n\nR: dagitty and ggdag\nPython: causalgraphicalmodels (note this only seems to work in Python 3.9 or earlier). You will also need graphviz and possibly python-graphviz to render the graphs\n\nWe’ll illustrate these very quickly here with a subset of the Grace and Kelly DAG (stand age, fire severity, plant abundance and richness). Fist load the relevant packages:\n\nRPython\n\n\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n\n\n\nimport numpy as np\n\n\n\n\n\nCreating a DAG\n\nRPython\n\n\nWe’ll start by creating the DAG. In R the main function is dagify(). This takes a series of arguments:\n\nThe set of paths to include. These are specified using R’s formula syntax, with the destination node on the left hand side and the origin on the right. So X ~ Y would make a path going from X to Y. Note that if Y has more than one path pointing to it, you should include both in the same formula. So Y ~ X + Z, with have two paths to Y from X and Z respectively\nThe coordinates specifying the position of each node\nWhich variable to use for the exposure and outcome [optional]\nLabels for each node [optional]\n\n\ngrace_dag &lt;- dagify(fire_sev ~ stand_age,\n                    abundance ~ fire_sev,\n                    richness ~ abundance + stand_age,\n                    coords = list(x = c(stand_age = 1,\n                                        fire_sev = 2,\n                                        abundance = 3,\n                                        richness = 4), \n                                  y = c(stand_age = 1,\n                                        fire_sev = 1,\n                                        abundance = 1,\n                                        richness = 2)\n                    ),\n                    exposure = \"fire_sev\",\n                    outcome = \"richness\",\n                    labels = c(stand_age = \"Stand age\", \n                               fire_sev = \"Fire severity\", \n                               abundance = \"Plant abundance\",\n                               richness = \"Richness\")\n)\n\n\n\nIn Python, there are two arguments:\n\nA list of nodes\nA list of tuples defining each path (\"origin\", \"destination\")\n\n\n# grace_dag = CausalGraphicalModel(\n#     nodes=[\"stand_age\", \"fire_severity\", \"plant_abundance\", \"richness\"],\n#     edges=[\n#         (\"stand_age\", \"fire_severity\"), \n#         (\"stand_age\", \"richness\"), \n#         (\"fire_severity\", \"plant_abundance\"),\n#         (\"plant_abundance\", \"richness\")\n#     ]\n# )\n\n\n\n\n\nRPython\n\n\nThe base function for this in R is ggdag. This is built on top of ggplot2, so you can use functions from the package to update your plot. Here we’ll just add a theme to give a blank background:\n\nggdag(grace_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\nBy default, this just uses the variable names as labels, which (as you can see) don’t fit well with the nodes. We can update this to show the manually defined labels, and the exposure/outcome variables as follows:\n\nggdag_status(grace_dag, use_labels = \"label\", text = FALSE) +\n  theme_dag()\n\n\n\n\n\n\n\n\n\n\nIn Python, the graphical model object has a method to plot it using networkx:\n\n# grace_dag.draw()\n\n\n\n\nYou can get the list of conditional independencies as follows:\n\nRPython\n\n\n\nimpliedConditionalIndependencies(grace_dag)\n\nabnd _||_ stn_ | fr_s\nfr_s _||_ rchn | abnd, stn_\n\n\nHere the syntax is var1 is conditionally independent (_||_) on var2 given (|) var3\n\n\n\n# grace_dag.get_all_independence_relationships()\n\nHere the syntax is ((var1, var2, {var3})): var1 is conditionally independent on var2 given var3\n\n\n\nThese show that:\n\nstand age and abundance are conditionally independent given fire severity\nfire severity and richness are conditionally independent given abundance and stand age\n\nThe DAGs can be used to show all possible paths between the exposure (fire) and outcome (richness):\n\nRPython\n\n\n\npaths(grace_dag)\n\n$paths\n[1] \"fire_sev -&gt; abundance -&gt; richness\" \"fire_sev &lt;- stand_age -&gt; richness\"\n\n$open\n[1] TRUE TRUE\n\n\nYou can also visualize these:\n\nggdag_paths(grace_dag)\n\n\n\n\n\n\n\n\n\n\nIn Python, we can extract the backdoor paths - these are usually paths through a confounding variable (here, the stand age)\n\n# grace_dag.get_all_backdoor_paths(\"fire_severity\", \"richness\")\n\n\n\n\nFinally, we can use these DAGs to identify the adjustment sets\n\nRPython\n\n\nIn R, we can extract these directly, as the exposure and outcome were preset when making the DAG. Note that you can specify these in this function, if you want to test a different part of the graph.\n\nadjustmentSets(grace_dag)\n\n{ stand_age }\n\n\nYou can also visualize these:\n\nggdag_adjustment_set(grace_dag)\n\n\n\n\n\n\n\n\n\n\nTo get the adjustment set in Python, we need to specify the exposure and outcome.\n\n# grace_dag.get_all_backdoor_adjustment_sets(\"fire_severity\", \"richness\")"
  },
  {
    "objectID": "Week5.html",
    "href": "Week5.html",
    "title": "Week 5: DAGs",
    "section": "",
    "text": "Also need scipy and the statsmodel\n\nfrom causalgraphicalmodels import CausalGraphicalModel\n\n\nsprinkler = CausalGraphicalModel(\n    nodes=[\"season\", \"rain\", \"sprinkler\", \"wet\", \"slippery\"],\n    edges=[\n        (\"season\", \"rain\"), \n        (\"season\", \"sprinkler\"), \n        (\"rain\", \"wet\"),\n        (\"sprinkler\", \"wet\"), \n        (\"wet\", \"slippery\")\n    ]\n)\n\n\n# draw return a graphviz `dot` object, which jupyter can render\nsprinkler.draw()\n\n\n\n\n\n\n\n\n(stand age, fire severity, plant abundance and richness)\n\ngrace_dag = CausalGraphicalModel(\n    nodes=[\"stand_age\", \"fire_severity\", \"plant_abundance\", \"richness\"],\n    edges=[\n        (\"stand_age\", \"fire_severity\"), \n        (\"stand_age\", \"richness\"), \n        (\"fire_severity\", \"plant_abundance\"),\n        (\"plant_abundance\", \"richness\")\n    ]\n)\ngrace_dag.draw()\n\n\n\n\n\n\n\n\n\n# import required modules \nimport inspect \ninspect.signature(CausalGraphicalModel)\n\n&lt;Signature (nodes, edges, latent_edges=None, set_nodes=None)&gt;\n\n\n\ngrace_dag.get_all_frontdoor_adjustment_sets(\"fire_severity\", \"richness\")\n\nfrozenset({frozenset({'plant_abundance'})})\n\n\n\ngrace_dag.get_all_backdoor_paths(\"fire_severity\", \"richness\")\n\n[['fire_severity', 'stand_age', 'richness']]\n\n\n\n\n\nAttributeError: 'CausalGraphicalModel' object has no attribute 'get_all_frontdoor_paths'\n\n\n\n# get all the conditional independence relationships implied by a CGM\ngrace_dag.get_all_independence_relationships()\n\n[('fire_severity', 'richness', {'plant_abundance', 'stand_age'}),\n ('stand_age', 'plant_abundance', {'fire_severity'})]\n\n\n\n# get all backdoor adjustment sets\ngrace_dag.get_all_backdoor_adjustment_sets(\"fire_severity\", \"richness\")\n\nfrozenset({frozenset({'stand_age'})})\n\n\n\n# get the graph created by intervening on node \"rain\"\ngrace_dag_adj = grace_dag.do(\"stand_age\")\n\ngrace_dag_adj.draw()\n\n\n\n\n\n\n\n\n\n# get all backdoor adjustment sets\ngrace_dag_adj.get_all_backdoor_adjustment_sets(\"fire_severity\", \"richness\")\n\nfrozenset({frozenset({'stand_age'})})\n\n\n\ngrace_dag_adj.get_all_independence_relationships()\n\n[('fire_severity', 'richness', {'plant_abundance', 'stand_age'}),\n ('stand_age', 'plant_abundance', {'fire_severity'})]\n\n\n\n\n\nTypeError: get_all_backdoor_paths() missing 2 required positional arguments: 'x' and 'y'"
  },
  {
    "objectID": "GEOG6960_Week6.html#coding-sems",
    "href": "GEOG6960_Week6.html#coding-sems",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Coding SEMs",
    "text": "Coding SEMs\nBoth R and Python have packages that allow you to create SEMs and estimate coefficients based on a dataset.\n\nR: lavaan and sem\nPython: semopy (pip install semopy)\n\nFirst load (or install and load) the relevant packages. We’ll need some additional packages to explore the data before model building.\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(lavaan)\nlibrary(lavaanPlot)\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport semopy"
  },
  {
    "objectID": "GEOG6960_Week6.html#example-1",
    "href": "GEOG6960_Week6.html#example-1",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Example 1",
    "text": "Example 1\n\nData\nLet’s start by loading the Grace and Keeley dataset and exploring the values. There are (or should be) 8 variables (described in the appendix below). Note that this differs a little from the dataset used in the paper - there is an additional field (elev) which we will drop, and the community type variable is missing. Later, we’ll adjust the DAG that we use as a basis for the model accordingly.\n\nRPython\n\n\n\nkeeley &lt;- read.csv(\"./data/keeley.csv\")\nkeeley &lt;- keeley %&gt;%\n  select(-elev)\nstr(keeley)\n\n'data.frame':   90 obs. of  7 variables:\n $ distance: num  53.4 37 53.7 53.7 52 ...\n $ abiotic : num  60.7 40.9 51 61.2 46.7 ...\n $ age     : int  40 25 15 15 23 24 35 14 45 35 ...\n $ hetero  : num  0.757 0.491 0.844 0.691 0.546 ...\n $ firesev : num  3.5 4.05 2.6 2.9 4.3 4 4.8 4.8 7.25 6.2 ...\n $ cover   : num  1.039 0.478 0.949 1.195 1.298 ...\n $ rich    : int  51 31 71 64 68 34 39 66 25 31 ...\n\n\n\n\n\nkeeley = pd.read_csv(\"./data/keeley.csv\")\nkeeley.drop('elev', axis=1, inplace=True)\nkeeley.describe()\n\n        distance    abiotic        age  ...    firesev      cover       rich\ncount  90.000000  90.000000  90.000000  ...  90.000000  90.000000  90.000000\nmean   49.234583  49.239025  25.566667  ...   4.565000   0.691232  49.233333\nstd     8.829480   7.679109  12.566274  ...   1.652347   0.317235  15.105658\nmin    37.037450  32.593865   3.000000  ...   1.200000   0.055577  15.000000\n25%    39.459800  43.812007  15.000000  ...   3.700000   0.487690  37.000000\n50%    51.770850  48.036046  25.000000  ...   4.300000   0.637118  50.000000\n75%    58.402237  54.898285  35.000000  ...   5.550000   0.914676  62.000000\nmax    60.723000  70.456286  60.000000  ...   9.200000   1.535408  85.000000\n\n[8 rows x 7 columns]\n\n\n\n\n\nLet’s take a quick look at the distribution of the variables in the file. These are generally normally distributed (or at least close enough for our purposes). The one exception is the distance to coast variable which shows three clusters of plots.\n\nRPython\n\n\n\nggpairs(keeley)\n\n\n\n\n\n\n\n\n\n\n\nsns.pairplot(keeley)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at the covariance matrix based on the set of variables. (As a reminder, this is what our SEM will be trying to reproduce.)\n\nRPython\n\n\n\ncov(keeley)\n\n            distance     abiotic         age       hetero      firesev\ndistance  77.9597236  31.1704086 -30.8610652  0.350756757 -4.054309551\nabiotic   31.1704086  58.9687166 -12.9148310  0.243915311 -2.681293440\nage      -30.8610652 -12.9148310 157.9112360 -0.138245879  9.423988764\nhetero     0.3507568   0.2439153  -0.1382459  0.013183203 -0.009932791\nfiresev   -4.0543096  -2.6812934   9.4239888 -0.009932791  2.730250000\ncover      0.6819166   0.2988236  -1.3968842 -0.006133045 -0.229138017\nrich      77.9547539  58.9674057 -57.3359551  0.792607436 -9.180505618\n                cover        rich\ndistance  0.681916629  77.9547539\nabiotic   0.298823649  58.9674057\nage      -1.396884159 -57.3359551\nhetero   -0.006133045   0.7926074\nfiresev  -0.229138017  -9.1805056\ncover     0.100637887   1.5772673\nrich      1.577267311 228.1808989\n\n\n\n\n\nkeeley.cov()\n\n           distance    abiotic         age  ...   firesev     cover        rich\ndistance  77.959724  31.170409  -30.861065  ... -4.054310  0.681917   77.954754\nabiotic   31.170409  58.968717  -12.914831  ... -2.681293  0.298824   58.967406\nage      -30.861065 -12.914831  157.911236  ...  9.423989 -1.396884  -57.335955\nhetero     0.350757   0.243915   -0.138246  ... -0.009933 -0.006133    0.792607\nfiresev   -4.054310  -2.681293    9.423989  ...  2.730250 -0.229138   -9.180506\ncover      0.681917   0.298824   -1.396884  ... -0.229138  0.100638    1.577267\nrich      77.954754  58.967406  -57.335955  ... -9.180506  1.577267  228.180899\n\n[7 rows x 7 columns]\n\n\n\n\n\n\n\nCreating a structural equation model\nWe’ll start by creating a subset model (the same one we looked at in class). This will only use three variables (age, firesev and cover). These are related by the following DAG:\n\n\n\n\n\n\n\n\n\nBuilding a structural equation model usually takes two steps. First, we need to describe the DAG that relates the variables, and second, we use a dataset to estimate values for the paths and other parameters.\n\nModel/DAG description\nBoth lavaan and semopy use a similar syntax to describe the model, based on R’s formula syntax. For each endogenous variable (one that has at least one arrow coming in to it) in the graph, we need a formula to describe the paths. This is written as:\ny ~ x1 + x2 + ...\nWhere y is the variable of interest, and x1, x2, etc are all variables that are at the origin of the path flowing into y. For the DAG above, we need a formula for fire severity (firesev) and plant cover (cover). We do not need one for stand age as there are no incoming arrows in this DAG. The set of formula are specified as a character string. Note that the name of the variables in the formula need to exactly match the names of the data frame column that hold that variable.\n\nRPython\n\n\nIn R, these are generally written as one formula per line:\n\nkeeley_formula = \n'firesev ~ age\ncover ~ age + firesev\n'\n\n\n\nIn Python, these are concatenated into a single line with line returns (\\n) to separate the formula:\n\nkeeley_formula = 'firesev ~ age\\ncover ~ age + firesev'\n\n\n\n\n\n\nModel fitting\n\nRPython\n\n\nIn R, the function is sem(). This needs a minimum of two arguments: the model formula and the data frame containing the variables of interest. We also use the argument meanstructure = TRUE to return the intercepts:\n\nkeeley_sem1 &lt;- sem(keeley_formula, \n                   data = keeley,\n                   meanstructure = TRUE)\n\n\n\nsemopy follows the standard Python approach of first instantiating the semopy model with the formula, then fiting it using the keeley dataframe:\n\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\n\n\n\n\nNow let’s explore the output.\n\nRPython\n\n\n\nsummary(keeley_sem1, \n        standardize = TRUE, \n        rsq = TRUE)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  firesev ~                                                             \n    age               0.060    0.012    4.832    0.000    0.060    0.454\n  cover ~                                                               \n    age              -0.005    0.003   -1.833    0.067   -0.005   -0.191\n    firesev          -0.067    0.020   -3.353    0.001   -0.067   -0.350\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .firesev           3.039    0.351    8.647    0.000    3.039    1.850\n   .cover             1.122    0.090   12.398    0.000    1.122    3.556\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .firesev           2.144    0.320    6.708    0.000    2.144    0.794\n   .cover             0.078    0.012    6.708    0.000    0.078    0.780\n\nR-Square:\n                   Estimate\n    firesev           0.206\n    cover             0.220\n\n\nThere’s quite a lot of output in the model summary. The various sections (from the top) are:\n\nModel fitting details: the fitting and optimization method and the number of parameters estimated\nModel test: the results of the Chi-squared test for goodness-of-fit\nParameter estimates: settings for parameter estimates (incl. standard errors)\nRegressions: the path coefficients. For each one, there is\n\nThe estimate\nThe standard error\nThe estimate as a \\(z\\)-score\nThe \\(p\\)-value\nThe estimate standardized for any latent variables (we don’t have any)\nThe standardized coefficient. This is the coefficient we would obtain if all the variables were \\(z\\)-score transformed, and allows comparison between the strength of the different paths\n\nIntercepts: estimate of intercept/mean values for each endogenous variable. The columns are the same as the previous table\nVariances: the estimate variance of the endogenous variables\nR-Square: the \\(r\\)-squared value for each endogenous variable (how much of the variance of that variable was explained)\n\n\n\nOnce fit, the original model contains details of the fitting process (aglorithm, optimizer, etc):\n\nprint(res)\n\nName of objective: MLW\nOptimization method: SLSQP\nOptimization successful.\nOptimization terminated successfully\nObjective value: 0.000\nNumber of iterations: 13\nParams: 0.060 -0.005 -0.067 0.078 2.145\n\n\nThe estimated model parameters, as well as standard errors and \\(p\\)-values, are in the fitted model object. Path coefficients are noted with ~ and variable variance is noted with ~~:\n\nmod.inspect()\n\n      lval  op     rval  Estimate  Std. Err   z-value       p-value\n0  firesev   ~      age  0.059658  0.012353  4.829254  1.370455e-06\n1    cover   ~      age -0.004827  0.002637 -1.830613  6.715826e-02\n2    cover   ~  firesev -0.067239  0.020052 -3.353280  7.985976e-04\n3  firesev  ~~  firesev  2.144768  0.319723  6.708204  1.970335e-11\n4    cover  ~~    cover  0.077611  0.011570  6.708204  1.970335e-11\n\n\nNote that the intercepts are not returned by default. You can get these with:\n\nfrom semopy.means import estimate_means\nestimate_means(mod)\n\n      lval op rval   Estimate\n0      age  ~    1  25.566667\n1  firesev  ~    1   3.039744\n2    cover  ~    1   1.121591\n\n\nThis returns the intercepts plus the mean of the exogenous variables (age in this example)\n\n\n\n\n\nModel diagnostics\n\nRPython\n\n\nIn addition to the summary output, we can access model diagnostics using other functions. Using anova() will run the Chi-squared test comparing this model to a fully saturated model. In this case, the model we have fit is the saturated model, so the Chi-squared cannot be calculated (but see below for a better test):\n\nanova(keeley_sem1)\n\nChi-Squared Test Statistic (unscaled)\n\n          Df   AIC   BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)\nSaturated  0                 0                              \nModel      0 363.4 380.9     0          0       0           \n\n\nYou can also obtain the model AIC with, not too surprisingly, the AIC() function. There are several other diagnostics that can be obtained with the fitMeasures function. This produces a large number of different test scores, so we’ll limit it here to the root-mean squared error of approximation (RMSEA), which should ideally be below 0.1. Again, with a saturated model, this is zero and not very meaningful.\n\nfitMeasures(keeley_sem1, c('rmsea'))\n\nrmsea \n    0 \n\n\n\n\nIn Python, we can access model diagnostics using the calc_stats function. This produces a large amount of metrics, including the Chi-squared value, CFI, RMSEA, etc\n\nsemopy.calc_stats(mod)\n\n       DoF  DoF Baseline     chi2  ...   AIC        BIC        LogLik\nValue    1             4  0.00002  ...  10.0  22.499048  2.224589e-07\n\n[1 rows x 14 columns]\n\n\nHere’s the results of the Chi-squared test. In this case, the model we have fit is the saturated model, so the Chi-squared cannot be calculated as there are no degrees of freedom. (semopy uses an approximation so the Chi-squared value is not exactly equal to zero.)\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n          chi2  chi2 p-value\nValue  0.00002       0.99643\n\n\n\n\n\n\n\nModel visualization\nAn easier way to portray the results is to plot out the DAG with the SEM estimated coefficients on the paths. This should show significant paths between age and firesev, and between firesev and cover, but the direct path from age to cover is not significant.\n\nRPython\n\n\n\nlavaanPlot(keeley_sem1, coef=TRUE, stars=\"regress\")\n\n\n\n\nlavaanPlot\n\n\n\n\nThe semplot() function provides a wrapper to the graphviz library (you will need to have this installed), and can be used to visualize the model. This will render in a Jupyter notebook or as an external file. (Note that g below is a graphviz object and can be modified.)\n\ng = semopy.semplot(mod, \"test.png\")\ng\n\n&lt;graphviz.graphs.Digraph object at 0x322c5f5f0&gt;\n\n\n\n\n\n\n\nA simpler model\nAs the path between age and cover appears non-significant, we can ask if it is really a necessary part of the model. To do this, we’ll drop the path, rebuild the model and check the results of the Chi-squared test.\n\nRPython\n\n\n\nkeeley_formula = \n'firesev ~ age\ncover ~ firesev\n'\nkeeley_sem2 &lt;- sem(keeley_formula, \n                   data = keeley,\n                   meanstructure = TRUE)\n\n\nsummary(keeley_sem2, \n        standardize = TRUE, \n        rsq = TRUE)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.297\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.069\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  firesev ~                                                             \n    age               0.060    0.012    4.832    0.000    0.060    0.454\n  cover ~                                                               \n    firesev          -0.084    0.018   -4.611    0.000   -0.084   -0.437\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .firesev           3.039    0.351    8.647    0.000    3.039    1.850\n   .cover             1.074    0.088   12.166    0.000    1.074    3.406\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .firesev           2.144    0.320    6.708    0.000    2.144    0.794\n   .cover             0.081    0.012    6.708    0.000    0.081    0.809\n\nR-Square:\n                   Estimate\n    firesev           0.206\n    cover             0.191\n\n\n\n\n\nkeeley_formula = 'firesev ~ age\\ncover ~ firesev'\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nmod.inspect()\n\n      lval  op     rval  Estimate  Std. Err   z-value       p-value\n0  firesev   ~      age  0.059616  0.012352  4.826435  1.389991e-06\n1    cover   ~  firesev -0.083902  0.018201 -4.609770  4.031137e-06\n2  firesev  ~~  firesev  2.144243  0.319645  6.708204  1.970335e-11\n3    cover  ~~    cover  0.080476  0.011997  6.708204  1.970335e-11\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n           chi2  chi2 p-value\nValue  3.297465      0.192294\n\n\n\n\n\nNow we get a value for the Chi-squared test of about 3.3, and a \\(p\\)-value above 0.05, which suggests there is no significant difference between this simpler model and the full covariance matrix. Or to put it more simply, dropping the path from age to cover has not made the model notably worse.\n\n\nThe full model\nNow let’s make the full model, detailed in the DAG below:\n\n\n\n\n\n\n\n\n\nAs before, start by building the paths. Note that, apart from distance, all variables are endogenous and will need a formula.\n\nRPython\n\n\n\nkeeley_formula = \n'\nage ~ distance\nhetero ~ distance\nabiotic ~ distance\nfiresev ~ age\ncover ~ firesev\nrich ~ cover + hetero + abiotic + distance\n'\n\n\n\n\nkeeley_formula = 'age ~ distance\\nhetero ~ distance\\nabiotic ~ distance\\nfiresev ~ age\\ncover ~ firesev\\nrich ~ cover + hetero + abiotic + distance'\n\n\n\n\nNow let’s build the model and check how well it fits.\n\nRPython\n\n\n\nkeeley_sem3 &lt;- sem(keeley_formula, \n                   data = keeley,\n                   meanstructure = TRUE)\n\nWarning: lavaan-&gt;lav_data_full():  \n   some observed variances are (at least) a factor 1000 times larger than \n   others; use varTable(fit) to investigate\n\nsummary(keeley_sem3)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.866\n  Degrees of freedom                                12\n  P-value (Chi-square)                           0.052\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  age ~                                               \n    distance         -0.396    0.144   -2.747    0.006\n  hetero ~                                            \n    distance          0.004    0.001    3.498    0.000\n  abiotic ~                                           \n    distance          0.400    0.081    4.911    0.000\n  firesev ~                                           \n    age               0.060    0.012    4.832    0.000\n  cover ~                                             \n    firesev          -0.084    0.018   -4.611    0.000\n  rich ~                                              \n    cover            13.648    3.405    4.008    0.000\n    hetero           44.512   10.012    4.446    0.000\n    abiotic           0.491    0.158    3.103    0.002\n    distance          0.484    0.145    3.341    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .age              45.057    7.207    6.252    0.000\n   .hetero            0.462    0.064    7.180    0.000\n   .abiotic          29.554    4.072    7.258    0.000\n   .firesev           3.039    0.351    8.647    0.000\n   .cover             1.074    0.088   12.166    0.000\n   .rich            -38.616    9.200   -4.198    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .age             144.076   21.478    6.708    0.000\n   .hetero            0.011    0.002    6.708    0.000\n   .abiotic          45.989    6.856    6.708    0.000\n   .firesev           2.144    0.320    6.708    0.000\n   .cover             0.081    0.012    6.708    0.000\n   .rich            103.534   15.434    6.708    0.000\n\n\n\n\n\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nmod.inspect()\n\n       lval  op      rval    Estimate   Std. Err   z-value       p-value\n0       age   ~  distance   -0.395876   0.144576 -2.738183  6.177962e-03\n1    hetero   ~  distance    0.004499   0.001286  3.498238  4.683427e-04\n2   abiotic   ~  distance    0.399777   0.081389  4.911938  9.018049e-07\n3   firesev   ~       age    0.059678   0.012313  4.846912  1.253983e-06\n4     cover   ~   firesev   -0.083934   0.018190 -4.614244  3.945291e-06\n5      rich   ~     cover   13.626100   3.417470  3.987189  6.686079e-05\n6      rich   ~    hetero   44.282423  10.049588  4.406392  1.051067e-05\n7      rich   ~   abiotic    0.491382   0.158800  3.094346  1.972476e-03\n8      rich   ~  distance    0.485038   0.145445  3.334848  8.534589e-04\n9   abiotic  ~~   abiotic   45.961067   6.851471  6.708204  1.970335e-11\n10      age  ~~       age  145.028612  21.619589  6.708204  1.970335e-11\n11    cover  ~~     cover    0.080498   0.012000  6.708204  1.970335e-11\n12  firesev  ~~   firesev    2.143627   0.319553  6.708204  1.970335e-11\n13   hetero  ~~    hetero    0.011476   0.001711  6.708204  1.970335e-11\n14     rich  ~~      rich  104.311698  15.549870  6.708204  1.970335e-11\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n            chi2  chi2 p-value\nValue  20.871316      0.075513\n\n\n\n\n\nThe goodness-of-fit value indicates that this model fits well (i.e. \\(p&gt;0.05\\)), but it is close to the threshold. We can explore which of the missing paths may be the most useful to include to improve the fit by calculating the modification indices. These indicate the change in Chi-squared value if a path was included in the model.\n\nRPython\n\n\nIn R, we can calculate these with modificationIndices. We’ll get these, sort for the largest index and print the top 5\n\nmodificationIndices(keeley_sem3) %&gt;%\n  arrange(-mi) %&gt;%\n  head(5)\n\n       lhs op    rhs    mi    epc sepc.lv sepc.all sepc.nox\n1   hetero ~~  cover 6.889 -0.008  -0.008   -0.277   -0.277\n2   hetero  ~  cover 6.544 -0.092  -0.092   -0.253   -0.253\n3    cover  ~ hetero 4.078 -0.529  -0.529   -0.192   -0.192\n4 distance  ~  cover 3.404  5.592   5.592    0.201    0.201\n5  firesev  ~  cover 3.238  2.157   2.157    0.414    0.414\n\n\n\n\nThe semopy package unfortunately does not currently have a function to calculate these.\n\n\n\nThe path with the largest impact on the Chi-squared value is between cover and hetero. Physically, the most likely direction for this is that heterogeneity causally affects cover, so we’ll include that (cover ~ firesev + hetero) and update this model.\n\nRPython\n\n\n\nkeeley_formula = \n'\nage ~ distance\nhetero ~ distance\nabiotic ~ distance\nfiresev ~ age\ncover ~ firesev + hetero\nrich ~ cover + hetero + abiotic + distance\n'\nkeeley_sem4 &lt;- sem(keeley_formula, \n                   data = keeley,\n                   meanstructure = TRUE)\n\nWarning: lavaan-&gt;lav_data_full():  \n   some observed variances are (at least) a factor 1000 times larger than \n   others; use varTable(fit) to investigate\n\nsummary(keeley_sem4)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                16.690\n  Degrees of freedom                                11\n  P-value (Chi-square)                           0.117\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  age ~                                               \n    distance         -0.396    0.144   -2.747    0.006\n  hetero ~                                            \n    distance          0.004    0.001    3.498    0.000\n  abiotic ~                                           \n    distance          0.400    0.081    4.911    0.000\n  firesev ~                                           \n    age               0.060    0.012    4.832    0.000\n  cover ~                                             \n    firesev          -0.086    0.018   -4.823    0.000\n    hetero           -0.530    0.256   -2.069    0.039\n  rich ~                                              \n    cover            13.648    3.454    3.952    0.000\n    hetero           44.512   10.178    4.373    0.000\n    abiotic           0.491    0.158    3.103    0.002\n    distance          0.484    0.145    3.340    0.001\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .age              45.057    7.207    6.252    0.000\n   .hetero            0.462    0.064    7.180    0.000\n   .abiotic          29.554    4.072    7.258    0.000\n   .firesev           3.039    0.351    8.647    0.000\n   .cover             1.445    0.198    7.286    0.000\n   .rich            -38.616    9.560   -4.039    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .age             144.076   21.478    6.708    0.000\n   .hetero            0.011    0.002    6.708    0.000\n   .abiotic          45.989    6.856    6.708    0.000\n   .firesev           2.144    0.320    6.708    0.000\n   .cover             0.077    0.011    6.708    0.000\n   .rich            103.534   15.434    6.708    0.000\n\n\n\n\n\nkeeley_formula = 'age ~ distance\\nhetero ~ distance\\nabiotic ~ distance\\nfiresev ~ age\\ncover ~ firesev + hetero\\nrich ~ cover + hetero + abiotic + distance'\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nmod.inspect()\n\n       lval  op      rval    Estimate   Std. Err   z-value       p-value\n0       age   ~  distance   -0.395884   0.144581 -2.738148  6.178627e-03\n1    hetero   ~  distance    0.004500   0.001286  3.498660  4.676024e-04\n2   abiotic   ~  distance    0.399846   0.081395  4.912440  8.994979e-07\n3   firesev   ~       age    0.059678   0.012313  4.846534  1.256371e-06\n4     cover   ~   firesev   -0.085841   0.017788 -4.825676  1.395294e-06\n5     cover   ~    hetero   -0.529902   0.256165 -2.068598  3.858379e-02\n6      rich   ~     cover   13.622034   3.466325  3.929821  8.500921e-05\n7      rich   ~    hetero   44.276342  10.215595  4.334191  1.462971e-05\n8      rich   ~   abiotic    0.491416   0.158783  3.094888  1.968870e-03\n9      rich   ~  distance    0.485102   0.145457  3.335023  8.529224e-04\n10  abiotic  ~~   abiotic   45.967631   6.852450  6.708204  1.970335e-11\n11      age  ~~       age  145.038118  21.621006  6.708204  1.970335e-11\n12    cover  ~~     cover    0.076849   0.011456  6.708204  1.970335e-11\n13  firesev  ~~   firesev    2.144067   0.319619  6.708204  1.970335e-11\n14   hetero  ~~    hetero    0.011476   0.001711  6.708204  1.970335e-11\n15     rich  ~~      rich  104.304465  15.548792  6.708204  1.970335e-11\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n            chi2  chi2 p-value\nValue  16.694584      0.161452\n\n\n\n\n\n\n\n\nlavaanPlot\n\n\nThe new model shows a marginally significant negative relationship between heterogeneity and cover."
  },
  {
    "objectID": "GEOG6960_Week6.html#grace-and-keely-dataset-keeley.csv",
    "href": "GEOG6960_Week6.html#grace-and-keely-dataset-keeley.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Grace and Keely dataset keeley.csv",
    "text": "Grace and Keely dataset keeley.csv\n\n\n\nColumn header\nVariable\n\n\n\n\ndistance\nDistance to coast (m)\n\n\nelev\nElevation a.s.l.\n\n\nabiotic\nAbiotic favorability\n\n\nage\nAge of stand before fire\n\n\nhetero\nPlot heterogeneity\n\n\nfiresev\nSeverity of fire\n\n\ncover\nCover of plants\n\n\nrich\nPlant species richness"
  },
  {
    "objectID": "GEOG6960_Week6.html#mosquito-net-usage-dataset-mosquito_nets.csv",
    "href": "GEOG6960_Week6.html#mosquito-net-usage-dataset-mosquito_nets.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Mosquito net usage dataset mosquito_nets.csv",
    "text": "Mosquito net usage dataset mosquito_nets.csv\nTaken from https://github.com/r-causal/causalworkshop\n\n\n\n\n\n\n\nColumn header\nVariable\n\n\n\n\nid\nobservation ID\n\n\nnet\nDid the household use nets (F/T)\n\n\nnet_num\nDid the household use nets (0/1)\n\n\nmalaria_risk\nlikelihood that someone in the household will be infected (0-100)\n\n\nincome\nMonthly income ($)\n\n\nhealth\nSelf-reported healthiness (0-100)\n\n\nhousehold\nNumber of people living in the household\n\n\neligible\nEligibility for the free net program (0/1)\n\n\ntemperature\nAverage temperature at night (C)\n\n\nresistance\nResistance of mosquito strains to insecticide"
  },
  {
    "objectID": "GEOG6960_Week6.html#example-2",
    "href": "GEOG6960_Week6.html#example-2",
    "title": "GEOG 6960 Causality in Geog. Studies 6",
    "section": "Example 2",
    "text": "Example 2\nFor a second example, you’ll build and fit a structural equation model to the mosquito net data that we briefly introduced in the last lab. This is a simulated dataset on how the use of mosquito nets affects the risk of malaria (see appendix for details). Unlike the previous example, you’ll need to work out the code on your own.\n\nData\n\nRPython\n\n\n\nmosquito &lt;- read.csv(\"./data/mosquito_nets.csv\")\nmosquito &lt;- mosquito %&gt;%\n  select(-id)\nstr(mosquito)\n\n'data.frame':   1752 obs. of  9 variables:\n $ net         : logi  TRUE FALSE FALSE TRUE FALSE FALSE ...\n $ net_num     : int  1 0 0 1 0 0 1 0 0 0 ...\n $ malaria_risk: int  33 42 80 34 44 25 19 35 32 40 ...\n $ income      : int  781 974 502 671 728 1050 1146 1093 1037 828 ...\n $ health      : int  56 57 15 20 17 48 65 75 60 36 ...\n $ household   : int  2 4 3 5 5 1 3 5 3 3 ...\n $ eligible    : logi  FALSE FALSE FALSE TRUE FALSE FALSE ...\n $ temperature : num  21.1 26.5 25.6 21.3 19.2 25.3 27.4 29.8 27.6 21.3 ...\n $ resistance  : int  59 73 65 46 54 34 45 65 55 54 ...\n\n\n\nggpairs(mosquito)\n\n\n\n\n\n\n\n\n\nggplot(mosquito, aes(x = net, y = malaria_risk)) +\n  geom_boxplot() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nmosquito = pd.read_csv(\"./data/mosquito_nets.csv\")\nmosquito.drop('id', axis=1, inplace=True)\nmosquito.describe()\n\n           net_num  malaria_risk  ...  temperature   resistance\ncount  1752.000000    1752.00000  ...  1752.000000  1752.000000\nmean      0.388699      35.58847  ...    23.813128    47.789384\nstd       0.487594      15.45673  ...     4.107322    13.857257\nmin       0.000000      10.00000  ...    15.600000     5.000000\n25%       0.000000      24.00000  ...    20.600000    38.000000\n50%       0.000000      31.00000  ...    23.800000    48.000000\n75%       1.000000      46.00000  ...    26.900000    57.000000\nmax       1.000000      90.00000  ...    32.200000    95.000000\n\n[8 rows x 7 columns]\n\n\n\nsns.pairplot(mosquito)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(mosquito, x=\"net\", y=\"malaria_risk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nDAG\nNow try to fit the full model as described by the DAG below. Start by creating the set of model formulae. A good first order test is to check that the total number of covariates (across all formulae) equals the number of arrows in the DAG.\n\n\n\nlavaanPlot\n\n\nOnce you’ve done this, fit and visualize the model then check the Chis-squared test. Does this give a good fit? Are there any missing paths that should be included?"
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Mosquito example",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport semopy\n\n\nkeeley = pd.read_csv(\"./data/keeley.csv\")\nkeeley.drop('elev', axis=1, inplace=True)\nkeeley.describe()\n\n\n\n\n\n\n\n\n\ndistance\nabiotic\nage\nhetero\nfiresev\ncover\nrich\n\n\n\n\ncount\n90.000000\n90.000000\n90.000000\n90.000000\n90.000000\n90.000000\n90.000000\n\n\nmean\n49.234583\n49.239025\n25.566667\n0.683319\n4.565000\n0.691232\n49.233333\n\n\nstd\n8.829480\n7.679109\n12.566274\n0.114818\n1.652347\n0.317235\n15.105658\n\n\nmin\n37.037450\n32.593865\n3.000000\n0.384182\n1.200000\n0.055577\n15.000000\n\n\n25%\n39.459800\n43.812007\n15.000000\n0.624576\n3.700000\n0.487690\n37.000000\n\n\n50%\n51.770850\n48.036046\n25.000000\n0.684314\n4.300000\n0.637118\n50.000000\n\n\n75%\n58.402237\n54.898285\n35.000000\n0.768369\n5.550000\n0.914676\n62.000000\n\n\nmax\n60.723000\n70.456286\n60.000000\n0.877938\n9.200000\n1.535408\n85.000000\n\n\n\n\n\n\n\n\n\nkeeley_formula = 'firesev ~ age\\ncover ~ age + firesev'\n\n\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley, obj=\"FIML\")\n\n\nprint(res)\n\nName of objective: FIML\nOptimization method: SLSQP\nOptimization successful.\nOptimization terminated successfully\nObjective value: 1083.969\nNumber of iterations: 24\nParams: 0.156 0.005 0.100 0.210 3.925\n\n\n\nmod.param_vals\n\narray([0.15563028, 0.00451447, 0.10025475, 0.21013716, 3.92484989])\n\n\n\nmod.inspect()\n\n\n\n\n\n\n\n\n\nlval\nop\nrval\nEstimate\nStd. Err\nz-value\np-value\n\n\n\n\n0\nfiresev\n~\nage\n0.155630\n0.016711\n9.312882\n0.000000e+00\n\n\n1\ncover\n~\nage\n0.004514\n0.005419\n0.833150\n4.047602e-01\n\n\n2\ncover\n~\nfiresev\n0.100255\n0.024390\n4.110421\n3.949382e-05\n\n\n3\nfiresev\n~~\nfiresev\n3.924850\n0.585082\n6.708204\n1.970335e-11\n\n\n4\ncover\n~~\ncover\n0.210137\n0.031325\n6.708204\n1.970335e-11\n\n\n\n\n\n\n\n\n\nfrom semopy.means import estimate_means\nestimate_means(mod)\n\n\n\n\n\n\n\n\n\nlval\nop\nrval\nEstimate\n\n\n\n\n0\nage\n~\n1\n25.566667\n\n\n1\nfiresev\n~\n1\n0.586052\n\n\n2\ncover\n~\n1\n0.118149\n\n\n\n\n\n\n\n\n\nsemopy.calc_stats(mod)\n\n\n\n\n\n\n\n\n\nDoF\nDoF Baseline\nchi2\nchi2 p-value\nchi2 Baseline\nCFI\nGFI\nAGFI\nNFI\nTLI\nRMSEA\nAIC\nBIC\nLogLik\n\n\n\n\nValue\n1\n4\n12.044105\n0.00052\n14.846056\n-0.01826\n0.188734\n-2.245065\n0.188734\n-3.07304\n0.352266\n7.106071\n19.605119\n1.446964\n\n\n\n\n\n\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n\n\n\n\n\n\n\n\nchi2\nchi2 p-value\n\n\n\n\nValue\n12.044105\n0.00052\n\n\n\n\n\n\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'CFI', 'RMSEA']]\n\n\n\n\n\n\n\n\n\nchi2\nCFI\nRMSEA\n\n\n\n\nValue\n12.044105\n-0.01826\n0.352266\n\n\n\n\n\n\n\n\n\ng = semopy.semplot(mod, \"test.png\")\ng\n\n\n\n\n\n\n\n\n\nkeeley_formula = 'firesev ~ age\\ncover ~ firesev'\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nprint(semopy.calc_stats(mod)[['chi2', 'chi2 p-value']])\n\nTypeError: Solver.__init__() got multiple values for argument 'method'\n\n\n\nkeeley_formula = 'firesev ~ age\\ncover ~ firesev'\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nprint(semopy.calc_stats(mod)[['chi2', 'chi2 p-value']])\n\n           chi2  chi2 p-value\nValue  3.297465      0.192294\n\n\n\nmod.inspect(std_est=True)\n\n\n\n\n\n\n\n\n\nlval\nop\nrval\nEstimate\nEst. Std\nStd. Err\nz-value\np-value\n\n\n\n\n0\nfiresev\n~\nage\n0.059616\n0.453442\n0.012352\n4.826435\n1.389991e-06\n\n\n1\ncover\n~\nfiresev\n-0.083902\n-0.437048\n0.018201\n-4.609770\n4.031137e-06\n\n\n2\nfiresev\n~~\nfiresev\n2.144243\n0.794390\n0.319645\n6.708204\n1.970335e-11\n\n\n3\ncover\n~~\ncover\n0.080476\n0.808989\n0.011997\n6.708204\n1.970335e-11\n\n\n\n\n\n\n\n\n\ng = semopy.semplot(mod, \"test.png\")\ng\n\n\n\n\n\n\n\n\n\nkeeley_formula = 'age ~ distance\\nhetero ~ distance\\nabiotic ~ distance\\nfiresev ~ age\\ncover ~ firesev\\nrich ~ cover + hetero + abiotic + distance'\n\n\nmod = semopy.Model(keeley_formula)\nres = mod.fit(keeley)\nmod.inspect()\n\n\n\n\n\n\n\n\n\nlval\nop\nrval\nEstimate\nStd. Err\nz-value\np-value\n\n\n\n\n0\nage\n~\ndistance\n-0.395876\n0.144576\n-2.738183\n6.177962e-03\n\n\n1\nhetero\n~\ndistance\n0.004499\n0.001286\n3.498238\n4.683427e-04\n\n\n2\nabiotic\n~\ndistance\n0.399777\n0.081389\n4.911938\n9.018049e-07\n\n\n3\nfiresev\n~\nage\n0.059678\n0.012313\n4.846912\n1.253983e-06\n\n\n4\ncover\n~\nfiresev\n-0.083934\n0.018190\n-4.614244\n3.945291e-06\n\n\n5\nrich\n~\ncover\n13.626100\n3.417470\n3.987189\n6.686079e-05\n\n\n6\nrich\n~\nhetero\n44.282423\n10.049588\n4.406392\n1.051067e-05\n\n\n7\nrich\n~\nabiotic\n0.491382\n0.158800\n3.094346\n1.972476e-03\n\n\n8\nrich\n~\ndistance\n0.485038\n0.145445\n3.334848\n8.534589e-04\n\n\n9\nabiotic\n~~\nabiotic\n45.961067\n6.851471\n6.708204\n1.970335e-11\n\n\n10\nage\n~~\nage\n145.028612\n21.619589\n6.708204\n1.970335e-11\n\n\n11\ncover\n~~\ncover\n0.080498\n0.012000\n6.708204\n1.970335e-11\n\n\n12\nfiresev\n~~\nfiresev\n2.143627\n0.319553\n6.708204\n1.970335e-11\n\n\n13\nhetero\n~~\nhetero\n0.011476\n0.001711\n6.708204\n1.970335e-11\n\n\n14\nrich\n~~\nrich\n104.311698\n15.549870\n6.708204\n1.970335e-11\n\n\n\n\n\n\n\n\n\nsemopy.calc_stats(mod)[['chi2', 'chi2 p-value']]\n\n\n# Get modification indices ## GEMINI IS WRONG!!\nmi = mod.inspect(mode='list', what=\"mi\") \n\n# Print the modification indices\nprint(mi)\n\n       lval  op      rval    Estimate   Std. Err   z-value       p-value\n0       age   ~  distance   -0.395876   0.144576 -2.738183  6.177962e-03\n1    hetero   ~  distance    0.004499   0.001286  3.498238  4.683427e-04\n2   abiotic   ~  distance    0.399777   0.081389  4.911938  9.018049e-07\n3   firesev   ~       age    0.059678   0.012313  4.846912  1.253983e-06\n4     cover   ~   firesev   -0.083934   0.018190 -4.614244  3.945291e-06\n5      rich   ~     cover   13.626100   3.417470  3.987189  6.686079e-05\n6      rich   ~    hetero   44.282423  10.049588  4.406392  1.051067e-05\n7      rich   ~   abiotic    0.491382   0.158800  3.094346  1.972476e-03\n8      rich   ~  distance    0.485038   0.145445  3.334848  8.534589e-04\n9   abiotic  ~~   abiotic   45.961067   6.851471  6.708204  1.970335e-11\n10      age  ~~       age  145.028612  21.619589  6.708204  1.970335e-11\n11    cover  ~~     cover    0.080498   0.012000  6.708204  1.970335e-11\n12  firesev  ~~   firesev    2.143627   0.319553  6.708204  1.970335e-11\n13   hetero  ~~    hetero    0.011476   0.001711  6.708204  1.970335e-11\n14     rich  ~~      rich  104.311698  15.549870  6.708204  1.970335e-11\n\n\n\nmosquito = pd.read_csv(\"./data/mosquito_nets.csv\")\nmosquito.describe()\n\n\n\n\n\n\n\n\n\nid\nnet_num\nmalaria_risk\nincome\nhealth\nhousehold\ntemperature\nresistance\n\n\n\n\ncount\n1752.000000\n1752.000000\n1752.00000\n1752.000000\n1752.00000\n1752.000000\n1752.000000\n1752.000000\n\n\nmean\n876.500000\n0.388699\n35.58847\n904.797374\n50.72032\n2.988584\n23.813128\n47.789384\n\n\nstd\n505.903153\n0.487594\n15.45673\n188.750024\n18.21132\n1.412147\n4.107322\n13.857257\n\n\nmin\n1.000000\n0.000000\n10.00000\n301.000000\n5.00000\n1.000000\n15.600000\n5.000000\n\n\n25%\n438.750000\n0.000000\n24.00000\n779.000000\n38.00000\n2.000000\n20.600000\n38.000000\n\n\n50%\n876.500000\n0.000000\n31.00000\n904.500000\n51.00000\n3.000000\n23.800000\n48.000000\n\n\n75%\n1314.250000\n1.000000\n46.00000\n1036.000000\n63.00000\n4.000000\n26.900000\n57.000000\n\n\nmax\n1752.000000\n1.000000\n90.00000\n1469.000000\n100.00000\n9.000000\n32.200000\n95.000000\n\n\n\n\n\n\n\n\n\nsns.pairplot(mosquito)\n\n\n\n\n\n\n\n\n\nsns.histplot(mosquito, x=\"income\")\n\n\n\n\n\n\n\n\n\nsns.boxplot(mosquito, x=\"net\", y=\"malaria_risk\")"
  },
  {
    "objectID": "GEOG6960_Week7.html",
    "href": "GEOG6960_Week7.html",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "",
    "text": "In this lab, we’ll look at an alternative way of fitting structural equation models (SEMs) using a piecewise approach. This method was developed by Shipley and coworkers over the past 15 years, and has a number of advantages, mainly including a broader range of data types and structures within the SEM. We will use the following datasets:\n\nThe Grace and Keeley fire/plant abundance dataset (keeley.csv)\nA data set of tree mortality from Shipley (2009): shipley.csv\n\nAs the name implies, this approach works by piecing together individual models, each of which describes one endogenous variable in the graph. This contrasts with the covariance-based approach of lavaan and semopy that try to estimate a single model of the covariance model underlying the system. As a result, each individual model can be built using different assumptions (generalized linear models, non-linear models, etc), and we will explore some of this here.\nCurrently, this approach is only available through the R library piecewiseSEM developed by Lefcheck (https://doi.org/10.1111/2041-210X.12512). It is possible to carry out a similar in other languages, but some of the tests and visualizations would require additional work."
  },
  {
    "objectID": "GEOG6960_Week7.html#introduction",
    "href": "GEOG6960_Week7.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "",
    "text": "In this lab, we’ll look at an alternative way of fitting structural equation models (SEMs) using a piecewise approach. This method was developed by Shipley and coworkers over the past 15 years, and has a number of advantages, mainly including a broader range of data types and structures within the SEM. We will use the following datasets:\n\nThe Grace and Keeley fire/plant abundance dataset (keeley.csv)\nA data set of tree mortality from Shipley (2009): shipley.csv\n\nAs the name implies, this approach works by piecing together individual models, each of which describes one endogenous variable in the graph. This contrasts with the covariance-based approach of lavaan and semopy that try to estimate a single model of the covariance model underlying the system. As a result, each individual model can be built using different assumptions (generalized linear models, non-linear models, etc), and we will explore some of this here.\nCurrently, this approach is only available through the R library piecewiseSEM developed by Lefcheck (https://doi.org/10.1111/2041-210X.12512). It is possible to carry out a similar in other languages, but some of the tests and visualizations would require additional work."
  },
  {
    "objectID": "GEOG6960_Week7.html#grace-and-keeley-dataset",
    "href": "GEOG6960_Week7.html#grace-and-keeley-dataset",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Grace and Keeley Dataset",
    "text": "Grace and Keeley Dataset\n\nData and libraries\nAs ever, let’s start by loading the libraries we will need:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(correlation)\nlibrary(piecewiseSEM)\n\n\n  This is piecewiseSEM version 2.3.0.\n\n\n  Questions or bugs can be addressed to &lt;LefcheckJ@si.edu&gt;.\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(nlme)\nlibrary(ape)\n\n\nAttaching package: 'ape'\n\nThe following object is masked from 'package:dplyr':\n\n    where\n\n\nNow load the dataset:\n\nkeeley &lt;- read.csv(\"./data/keeley.csv\")\nstr(keeley)\n\n'data.frame':   90 obs. of  8 variables:\n $ distance: num  53.4 37 53.7 53.7 52 ...\n $ elev    : int  1225 60 200 200 970 970 950 740 170 190 ...\n $ abiotic : num  60.7 40.9 51 61.2 46.7 ...\n $ age     : int  40 25 15 15 23 24 35 14 45 35 ...\n $ hetero  : num  0.757 0.491 0.844 0.691 0.546 ...\n $ firesev : num  3.5 4.05 2.6 2.9 4.3 4 4.8 4.8 7.25 6.2 ...\n $ cover   : num  1.039 0.478 0.949 1.195 1.298 ...\n $ rich    : int  51 31 71 64 68 34 39 66 25 31 ...\n\n\nAnd let’s plot the correlation matrix:\n\ncor_res &lt;- correlation(keeley)\nx &lt;- cor_sort(as.matrix(cor_res))\nplot(visualisation_recipe(x))\n\n\n\n\n\n\n\n\n\n\nThree variable model 1\nBefore building the full SEM for these data, we’ll start with the simple three variable model that includes age, firesev and cover. To build the piecewise model, there are a couple of options: you can build the individual models within the piecewise function (psem) or you can build the individual models first, then use psem to link them. We’ll use this second approach as it is a little easier to check what is going on.\nAs a reminder, this is the DAG for the three variable model, as a partially mediated model:\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\nThe two endogenous variables are fire severity (firesev) and canopy cover (cover), so let’s build the two models that describe these according to the DAG using simple OLS linear regression:\n\nmod1 &lt;- lm(firesev ~ age, data = keeley)\nmod2 &lt;- lm(cover ~ age + firesev, data = keeley)\n\nAs these are just basic linear models, you can use all the usual functions to check them and get more information. Here is the summary output for them:\n\nsummary(mod1)\n\n\nCall:\nlm(formula = firesev ~ age, data = keeley)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7199 -1.1131 -0.1747  0.9603  4.6091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.03921    0.35543   8.551 3.45e-13 ***\nage          0.05968    0.01249   4.778 7.03e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.481 on 88 degrees of freedom\nMultiple R-squared:  0.206, Adjusted R-squared:  0.197 \nF-statistic: 22.83 on 1 and 88 DF,  p-value: 7.028e-06\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = cover ~ age + firesev, data = keeley)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49374 -0.22801 -0.06249  0.19985  0.81227 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.121762   0.092030  12.189  &lt; 2e-16 ***\nage         -0.004833   0.002682  -1.802  0.07503 .  \nfiresev     -0.067244   0.020399  -3.296  0.00142 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2833 on 87 degrees of freedom\nMultiple R-squared:  0.2202,    Adjusted R-squared:  0.2023 \nF-statistic: 12.28 on 2 and 87 DF,  p-value: 2.003e-05\n\n\nBoth models explain about 20% of the variance in the outcome, which is not great, but the coefficients and models do show significance.\nNow, we’ll build the piecewise SEM. This does not require refitting the models, but instead links them together to allow for testing.\n\nkeeley_psem1 &lt;- psem(\n  mod1, mod2,\n  data = keeley\n)\nkeeley_psem1\n\nStructural Equations of x :\nlm: firesev ~ age\nlm: cover ~ age + firesev\n\nData:\n  distance elev  abiotic age   hetero firesev     cover rich\n1 53.40900 1225 60.67103  40 0.757065    3.50 1.0387974   51\n2 37.03745   60 40.94291  25 0.491340    4.05 0.4775924   31\n3 53.69565  200 50.98805  15 0.844485    2.60 0.9489357   71\n4 53.69565  200 61.15633  15 0.690847    2.90 1.1949002   64\n5 51.95985  970 46.66807  23 0.545628    4.30 1.2981890   68\n6 51.95985  970 39.82357  24 0.652895    4.00 1.1734866   34\n...with  84  more rows\n\n[1] \"class(psem)\"\n\n\nAs you can see, the model object simply contains a list of the component models and a brief overview of the data. To test the overall SEM, we need to use the summary function:\n\nsummary(keeley_psem1)\n\n\nStructural Equation Model of keeley_psem1 \n\nCall:\n  firesev ~ age\n  cover ~ age + firesev\n\n    AIC\n 363.399\n\n---\nTests of directed separation:\n\n No independence claims present. Tests of directed separation not possible.\n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 0 with P-value = 1 and on 0 degrees of freedom\nFisher's C = NA with P-value = NA and on 0 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n   firesev       age   0.0597    0.0125 88     4.7781  0.0000       0.4539 ***\n     cover       age  -0.0048    0.0027 87    -1.8018  0.0750      -0.1914    \n     cover   firesev  -0.0672    0.0204 87    -3.2965  0.0014      -0.3502  **\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n   firesev   none      0.21\n     cover   none      0.22\n\n\nAnd again, there’s a lot of output. Let’s go through this from top to bottom:\n\nStructural Equation Model: this first section describes the models that were linked, as well as returning the AIC for the full PSEM model (note that this is the just the sum of the individual AICs)\nTests of directed separation: This section tests any missing paths in the model to see if excluding them is justified or not. As the model we have fit is just identified (or saturated), then there are no missing paths to test\nGlobal goodness-of-fit: Two tests are presented here. The first is an approximation of the Chi-squared test used in covariance models. The second (Fisher’s \\(C\\)) is based on the d-separation tests\nCoefficients: a summary of the model coefficients, including standardized coefficients. This should give the same results as the summaries of the two individual lm models above\nIndividual R-squared: the variance explained for each endogenous variable\n\nIn this case, the model is just identified, so the additional work of fitting the SEM cannot provide us with any additional information.\n\n\nThree variable model 2\nNow let’s drop the path between age and cover, refit the models and re-run the PSEM.\n\nmod1 &lt;- lm(firesev ~ age, data = keeley)\nmod2 &lt;- lm(cover ~ firesev, data = keeley)\nkeeley_psem2 &lt;- psem(\n  mod1, mod2,\n  data = keeley\n)\n\nAnd now test the model (we’ll suppresses the progress bar for the purposes of this document):\n\nsummary(keeley_psem2, .progressBar = FALSE)\n\n\nStructural Equation Model of keeley_psem2 \n\nCall:\n  firesev ~ age\n  cover ~ firesev\n\n    AIC\n 364.696\n\n---\nTests of directed separation:\n\n     Independ.Claim Test.Type DF Crit.Value P.Value \n  cover ~ age + ...      coef 87    -1.8018   0.075 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 3.297 with P-value = 0.069 and on 1 degrees of freedom\nFisher's C = 5.18 with P-value = 0.075 and on 2 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n   firesev       age   0.0597    0.0125 88     4.7781       0       0.4539 ***\n     cover   firesev  -0.0839    0.0184 88    -4.5594       0      -0.4371 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n   firesev   none      0.21\n     cover   none      0.19\n\n\nNow building the PSEM makes sense - we’re able to test whether excluding the age to cover path makes sense. Both the Chi-squared and Fisher’s \\(C\\) test are non-significant, which indicates that there is no support for this path in the data, which in turn confirms that excluding it makes sense. This is also shown in the d-separation tests, which shows a test for this excluded path, and the lack of significance further supports the absence of this path. In fact, as we are only excluding a single path, the d-separation and Fisher’s \\(C\\) test are equal.\npiecewiseSEM also comes with visualization function. We’ll use it here to show the paths for this second model, as well as the unstandardized coefficients. (This is built on top of DiagrammeR, a general network visualization package in R, which can be used to further modify the plot.)\n\nplot(keeley_psem2, show = \"unstd\")\n\n\n\n\nPSEM model\n\n\n\n\nFull model\nWe’ll finish this section by building the full Grace and Keeley model. The code below builds the individual models with the psem function to minimize the code, but you can try to build these first then link them together:\n\nkeeley_psem3 &lt;- psem(\n  lm(hetero ~ distance, data = keeley),\n  lm(abiotic ~ distance, data = keeley),\n  lm(age ~ distance, data = keeley),\n  lm(firesev ~ age, data = keeley),\n  lm(cover ~ firesev, data = keeley),\n  lm(rich ~ distance + abiotic + hetero + cover, data = keeley),\n  data = keeley\n)\nsummary(keeley_psem3, .progressBar = FALSE)\n\n\nStructural Equation Model of keeley_psem3 \n\nCall:\n  hetero ~ distance\n  abiotic ~ distance\n  age ~ distance\n  firesev ~ age\n  cover ~ firesev\n  rich ~ distance + abiotic + hetero + cover\n\n    AIC\n 2223.736\n\n---\nTests of directed separation:\n\n            Independ.Claim Test.Type DF Crit.Value P.Value   \n  firesev ~ distance + ...      coef 87    -1.6793  0.0967   \n    cover ~ distance + ...      coef 87     1.3302  0.1869   \n    abiotic ~ hetero + ...      coef 87     1.3296  0.1871   \n        age ~ hetero + ...      coef 87     0.0043  0.9966   \n    firesev ~ hetero + ...      coef 86     0.4923  0.6237   \n      cover ~ hetero + ...      coef 86    -2.7229  0.0078 **\n       age ~ abiotic + ...      coef 87    -0.0652  0.9481   \n   firesev ~ abiotic + ...      coef 86    -0.9713  0.3341   \n     cover ~ abiotic + ...      coef 86    -0.2678  0.7895   \n          rich ~ age + ...      coef 84    -0.8515  0.3969   \n         cover ~ age + ...      coef 86    -1.5896  0.1156   \n      firesev ~ rich + ...      coef 83    -1.2434  0.2172   \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 29.852 with P-value = 0.005 and on 13 degrees of freedom\nFisher's C = 34.017 with P-value = 0.084 and on 24 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n    hetero  distance   0.0045    0.0013 88     3.4593  0.0008       0.3460 ***\n   abiotic  distance   0.3998    0.0823 88     4.8562  0.0000       0.4597 ***\n       age  distance  -0.3959    0.1457 88    -2.7164  0.0079      -0.2781  **\n   firesev       age   0.0597    0.0125 88     4.7781  0.0000       0.4539 ***\n     cover   firesev  -0.0839    0.0184 88    -4.5594  0.0000      -0.4371 ***\n      rich  distance   0.4840    0.1526 85     3.1710  0.0021       0.2829  **\n      rich   abiotic   0.4908    0.1646 85     2.9815  0.0037       0.2495  **\n      rich    hetero  44.5119   10.8473 85     4.1035  0.0001       0.3383 ***\n      rich     cover  13.6481    3.7601 85     3.6297  0.0005       0.2866 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n    hetero   none      0.12\n   abiotic   none      0.21\n       age   none      0.08\n   firesev   none      0.21\n     cover   none      0.19\n      rich   none      0.54\n\n\nAnd we’ll show the plot with standardized coefficients to easier comparison between the paths:\n\nplot(keeley_psem3, show = \"std\", ns_dashed = TRUE)\n\n\n\n\nPSEM model\n\n\nThere is, not surprisingly, much more output here. There are more individual models, more paths, and more missing paths as shown by the large number of d-separation tests. Note that, unlike the covariance approaches, this function uses the structure of the graph ito intuit the direction of the missing paths (i.e. it tests firesev ~ distance not the other way around).\nThe two global tests provide slightly different information this time. The Chi-squared test is significant, whereas \\(C\\) is not (although it is close). From the d-sep tests, you should see that one missing path is flagged as significant. Try not to rebuild and test this SEM with the additional path added back in. Check the AIC of the previous model and this one to see whether including this appears to made an overall improvement.\n\n\nNonlinear models\nnext, we’ll look very quickly at an example of including a non-linear model (a spline-based generalized additive model). We’ll make a new version of the second 3-variable model, which includes paths from age to firesev and firesev to cover. For this, we’ll add a nonlinear relationship for the first of these, based on the following scatterplot which shows a weakening of the link between these variables at higher values of age:\n\nggplot(keeley, aes(x = age, y = firesev)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo do this, we’ll use the gam() function from the library mgcv. We’ll simply replace the first of the two models using this function and the s() to indicate using a smoothing spline to model the response of firesev to age:\n\nmod1 &lt;- gam(firesev ~ s(age), data = keeley)\nmod2 &lt;- lm(cover ~ firesev, data = keeley)\nkeeley_psem4 &lt;- psem(\n  mod1, mod2,\n  data = keeley\n)\nsummary(keeley_psem4, .progressBar = FALSE)\n\nWarning: Basis set includes smoothed terms in independence claim: claim is\nconducted with linear term!\n\n\nWarning: Categorical or non-linear variables detected. Please refer to\ndocumentation for interpretation of Estimates!\n\n\n\nStructural Equation Model of keeley_psem4 \n\nCall:\n  firesev ~ s(age)\n  cover ~ firesev\n\n    AIC\n 361.226\n\n---\nTests of directed separation:\n\n        Independ.Claim Test.Type DF Crit.Value P.Value \n  cover ~ s(age) + ...      coef 87    -1.8018   0.075 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 3.297 with P-value = 0.069 and on 1 degrees of freedom\nFisher's C = 5.18 with P-value = 0.075 and on 2 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error      DF Crit.Value P.Value Std.Estimate\n   firesev    s(age)        -         -  2.7886    10.1726       0            -\n     cover   firesev  -0.0839    0.0184 88.0000    -4.5594       0      -0.4371\n     \n  ***\n  ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n   firesev   none      0.24\n     cover   none      0.19\n\n\nA couple of things to note here. This doesn’t change the global model fit: these tests are based on the same missing path as the previous model. Second, there is no coefficient listed for the first model: as this is a nonlinear model, there is no single coefficient. If you want to see what this looks like, you can simply plot the individual model response:\n\nplot(keeley_psem4[[1]])\n\n\n\n\n\n\n\n\nLet’s finish this section by comparing the AIC of these two models to see if including the smoother improves overall model fit (despite the additional complexity):\n\nAIC(keeley_psem2, keeley_psem4)\n\n      AIC     K  n\n1 364.696 6.000 90\n2 361.226 7.208 90"
  },
  {
    "objectID": "GEOG6960_Week7.html#grace-and-keely-dataset-keeley.csv",
    "href": "GEOG6960_Week7.html#grace-and-keely-dataset-keeley.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Grace and Keely dataset keeley.csv",
    "text": "Grace and Keely dataset keeley.csv\n\n\n\nColumn header\nVariable\n\n\n\n\ndistance\nDistance to coast (m)\n\n\nelev\nElevation a.s.l.\n\n\nabiotic\nAbiotic favorability\n\n\nage\nAge of stand before fire\n\n\nhetero\nPlot heterogeneity\n\n\nfiresev\nSeverity of fire\n\n\ncover\nCover of plants\n\n\nrich\nPlant species richness"
  },
  {
    "objectID": "GEOG6960_Week7.html#mosquito-net-usage-dataset-mosquito_nets.csv",
    "href": "GEOG6960_Week7.html#mosquito-net-usage-dataset-mosquito_nets.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Mosquito net usage dataset mosquito_nets.csv",
    "text": "Mosquito net usage dataset mosquito_nets.csv\nTaken from https://github.com/r-causal/causalworkshop\n\n\n\n\n\n\n\nColumn header\nVariable\n\n\n\n\nid\nobservation ID\n\n\nnet\nDid the household use nets (F/T)\n\n\nnet_num\nDid the household use nets (0/1)\n\n\nmalaria_risk\nlikelihood that someone in the household will be infected (0-100)\n\n\nincome\nMonthly income ($)\n\n\nhealth\nSelf-reported healthiness (0-100)\n\n\nhousehold\nNumber of people living in the household\n\n\neligible\nEligibility for the free net program (0/1)\n\n\ntemperature\nAverage temperature at night (C)\n\n\nresistance\nResistance of mosquito strains to insecticide"
  },
  {
    "objectID": "GEOG6960_Week7.html#grace-and-keeley-dataset-keeley.csv",
    "href": "GEOG6960_Week7.html#grace-and-keeley-dataset-keeley.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Grace and Keeley dataset keeley.csv",
    "text": "Grace and Keeley dataset keeley.csv\n\n\n\nColumn header\nVariable\n\n\n\n\ndistance\nDistance to coast (m)\n\n\nelev\nElevation a.s.l.\n\n\nabiotic\nAbiotic favorability\n\n\nage\nAge of stand before fire\n\n\nhetero\nPlot heterogeneity\n\n\nfiresev\nSeverity of fire\n\n\ncover\nCover of plants\n\n\nrich\nPlant species richness"
  },
  {
    "objectID": "GEOG6960_Week7.html#generalized-and-mixed-effects-models",
    "href": "GEOG6960_Week7.html#generalized-and-mixed-effects-models",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Generalized and mixed effects models",
    "text": "Generalized and mixed effects models\nFor this example, we’ll use a synthetic dataset of tree growth and survival from Shipley’s 2009 paper. These data are longitudinal, with repeated observations for each tree. The trees are also grouped by site (there are 20 sites). The model that we will fit follows this graph:\n\nshipley_dag &lt;- dagify(DD ~ lat,\n                      date ~ DD,\n                      growth ~ date,\n                      live ~ growth,\n                      coords = list(x = c(lat = 1,\n                                          DD = 2,\n                                          date = 3,\n                                          growth = 4,\n                                          live = 5), \n                                    y = c(lat = 1,\n                                          DD = 2,\n                                          date = 3,\n                                          growth = 2,\n                                          live = 1)\n                      ),\n                      labels = c(lat = \"Latitude\",\n                                 DD = \"Degree Day\",\n                                 date = \"Date\",\n                                 growth = \"Growth\",\n                                 live = \"Live\")\n)\nggdag(shipley_dag, use_labels = \"label\", \n      text = FALSE) +\n  theme_dag()\n\n\n\n\n\n\n\n\nThe DAG represents the following processes (taken from Shipley (2009)):\n\nLatitude and year generate the number of degree-days at each site. Degree-days then cause the date of bud burst of a tree species. The date of bud burst causes the amount of diameter growth, and diameter growth determines the survival in the subsequent winter\n\n\nshipley %&gt;%\n  mutate(tree = as.factor(tree),\n         site = as.factor(site)) %&gt;%\n  ggplot(aes(x = year, y = Growth, col = tree)) + \n  geom_line() +\n  facet_wrap(~site) +\n  theme_bw()  + theme(legend.position = \"none\")\n\nWarning: Removed 469 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThere are two additional complexities with these data.\nFirst, the outcome we want to model is a binary variable (1 = live in a given year, 0 = dead). The appropriate model for these data is a binomial model with a logit link function.\nThe second is that the data are grouped: we have repeated observations by tree (for multiple year) and by site (for multiple trees). To account for this structure, we would need to include random effects for both site and tree.\nWe’ll use R’s lme4 library to build these models. For the first three paths, we’ll assume a linear reponse, so we can use the lmer() function. Random effects are included by the following syntax + (1 | g) where g is the grouping variable (site or tree here).\n\nmod1 &lt;- lmer(DD ~ lat + (1 | tree) + (1 | site), data = shipley)\nmod2 &lt;- lmer(Date ~ DD + (1 | tree) + (1 | site), data = shipley)\nmod3 &lt;- lmer(Growth ~ Date + (1 | tree) + (1 | site), data = shipley)\n\nAs before, you can check any of the individual models with the summary() function or any other standard diagnostics.\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: DD ~ lat + (1 | tree) + (1 | site)\n   Data: shipley\n\nREML criterion at convergence: 9157.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.51037 -0.77506 -0.05222  0.77363  2.62452 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n tree     (Intercept)  2.539   1.593   \n site     (Intercept) 20.157   4.490   \n Residual             32.139   5.669   \nNumber of obs: 1431, groups:  tree, 100; site, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 196.6524     7.6606  25.671\nlat          -0.8355     0.1194  -6.996\n\nCorrelation of Fixed Effects:\n    (Intr)\nlat -0.991\n\n\nFor the Live variable, we’ll need to specify a binomial model using glmer, with appropriate family arguments\n\nmod4 &lt;- glmer(Live ~ Growth + (1 | tree) + (1 | site), \n              family = binomial(link = \"logit\"), data = shipley)\n\nWe can now link all of these using psem to create the PSEM:\n\nshipley_psem1 &lt;- psem(\n  mod1, mod2, mod3, mod4,\n  data = shipley\n)\n\nWarning: NAs detected in the dataset. Consider removing all rows with NAs to\nprevent fitting to different subsets of data\n\n\n\nsummary(shipley_psem1, .progressBar = FALSE)\n\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n\n\n\nStructural Equation Model of shipley_psem1 \n\nCall:\n  DD ~ lat\n  Date ~ DD\n  Growth ~ Date\n  Live ~ Growth\n\n    AIC\n 21745.782\n\n---\nTests of directed separation:\n\n      Independ.Claim Test.Type        DF Crit.Value P.Value \n    Date ~ lat + ...      coef   18.0428     0.0064  0.9373 \n  Growth ~ lat + ...      coef   18.2781     0.7972  0.3835 \n    Live ~ lat + ...      coef 1431.0000     1.0282  0.3038 \n   Growth ~ DD + ...      coef  783.7024     0.0861  0.7693 \n     Live ~ DD + ...      coef 1431.0000     1.0036  0.3156 \n   Live ~ Date + ...      coef 1431.0000    -1.5627  0.1181 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = NA with P-value = NA and on 6 degrees of freedom\nFisher's C = 11.532 with P-value = 0.484 and on 12 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error        DF Crit.Value P.Value\n        DD       lat  -0.8355    0.1194   17.9831    48.9350       0\n      Date        DD  -0.4976    0.0049 1337.0585 10171.1806       0\n    Growth      Date   0.3007    0.0266 1400.8657   126.4361       0\n      Live    Growth   0.3479    0.0584 1431.0000     5.9544       0\n  Std.Estimate    \n       -0.7014 ***\n       -0.6281 ***\n        0.3824 ***\n        0.7866 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response      method Marginal Conditional\n        DD        none     0.48        0.69\n      Date        none     0.41        0.98\n    Growth        none     0.11        0.84\n      Live theoretical     0.56        0.63\n\n\nSome things to note in this output:\n\nYou will like have seen some warnings about singular fits: this suggests that some of the models that include the d-separated variable paths could not be fit well, and may suggest that some data standardization is necessary.\nWith that caveat in mind, none of the missing paths were significant\nFisher’s \\(C\\) supports this, suggesting that the model (and DAG) and well supported by the data\nThe Chi-squared test cannot be calculated due to the presence of the binomial model\nThe coefficient shown for the binomial model (~0.348) is on the logit scale (from the binomial model). To convert this to odds, simply take the anti-log (exp), which should give you a value of approximately 1.4, indicating an increasing survival probability with higher growth.\n\n\n\n\nPSEM model"
  },
  {
    "objectID": "GEOG6960_Week7.html#shipley-dataset-shipley.csv",
    "href": "GEOG6960_Week7.html#shipley-dataset-shipley.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Shipley dataset shipley.csv",
    "text": "Shipley dataset shipley.csv\nThis is a synthetic dataset on tree growth and survival.\n\n\n\nColumn header\nVariable\n\n\n\n\nsite\nSite of observation\n\n\ntree\nTree ID\n\n\nlat\nSite latitude\n\n\nyear\nYear of observation\n\n\nDate\nDate of first bud burst\n\n\nDD\nCumulative degree days until first bud burst\n\n\nGrowth\nAnnual increase in stem diameter\n\n\nSurvival\nProportional survival\n\n\nLive\nAlive (1) or dead (0)"
  },
  {
    "objectID": "GEOG6960_Week7.html#spatial-models",
    "href": "GEOG6960_Week7.html#spatial-models",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Spatial models",
    "text": "Spatial models\nAs a last example, we’ll look at the use of spatial models within piecewiseSEM. As a reminder, our concern with spatial models is that the errors or residuals will be autocorrelated, which implies that the standard errors and model \\(p\\)-values will be biased. Incorporating spatial covariance can then help to accoutn for this. To illustrate, we’ll use a data set of (more!) trees taken from a forest plot from the Volzhsko-Kamsky reserve in Russia. This example is lightly modified from Jed Byrnes’ SEM workshop (https://jebyrnes.github.io/semclass). Let’s start by reading in the data:\n\nboreal &lt;- read.csv(\"./data/boreal.csv\")\nhead(boreal)\n\n  point       x       y richness     NDVI   temp        wet\n1     1 2109.70 2093.52       13 0.480180 23.217 -0.0264378\n2     2 2190.18 2105.71       21 0.483990 23.217 -0.0234048\n3     3 2064.48 2052.77       30 0.489213 23.217 -0.0189264\n4     4 2277.34 2103.42       13 0.473226 23.217 -0.0280431\n5     5 2347.91 2074.81       13 0.405898 23.635 -0.0292287\n6     6 2437.21 2086.95        6 0.424769 23.217 -0.0229209\n\n\nAnd let’s convert this to a simple feature (spatial) object for plotting:\n\nboreal_sf &lt;- st_as_sf(boreal, coords = c(\"x\", \"y\"))\n\n\ntm_shape(boreal_sf) +\n  tm_symbols(col = \"NDVI\", size = 0.75, alpha = 0.75, \n             palette = \"Greens\", style = \"fisher\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\nHere’s the DAG for these data (it’s simplifed from the full dataset). This implies some of the following processes:\n\nWarmer temperatures increase richness and productivity (NDVI)\nWetter habitats (more negative wet) increase productivity\nHigher richness increases productivity\n\n\n\n\n\n\n\n\n\n\nLet’s now fit this model. We’ll start with non-spatial models for the two endogenous variables (richness and NDVI)\n\nmod1 &lt;- lm(richness ~ temp, data = boreal)\nmod2 &lt;- lm(NDVI ~ richness + temp + wet, data = boreal)\n\nAnd let’s link these into a PSEM:\n\nboreal_psem1 &lt;- psem(\n  mod1, mod2,\n  data = boreal\n)\nsummary(boreal_psem1, .progressBar = FALSE)\n\n\nStructural Equation Model of boreal_psem1 \n\nCall:\n  richness ~ temp\n  NDVI ~ richness + temp + wet\n\n    AIC\n 2166.646\n\n---\nTests of directed separation:\n\n        Independ.Claim Test.Type  DF Crit.Value P.Value \n  richness ~ wet + ...      coef 530    -1.0421  0.2979 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 1.091 with P-value = 0.296 and on 1 degrees of freedom\nFisher's C = 2.422 with P-value = 0.298 and on 2 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error  DF Crit.Value P.Value Std.Estimate    \n  richness      temp   1.1707    0.5470 531     2.1402  0.0328       0.0925   *\n      NDVI  richness  -0.0004    0.0002 529    -2.0862  0.0374      -0.0440   *\n      NDVI      temp  -0.0355    0.0023 529   -15.6564  0.0000      -0.3456 ***\n      NDVI       wet  -4.2701    0.1329 529   -32.1406  0.0000      -0.7066 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n  richness   none      0.01\n      NDVI   none      0.77\n\n\nThe richness model is not particularly good here, but does show some significance in the relationship with temperature (temp).\nAs these are spatial data, it’s important to check for autocorrelation in the residuals. We can first do this by mapping these out. If there’s little or no autocorrelation, then the pattern should be random. The residuals function will extract the residuals for both models (richness and NDVI):\n\nboreal_sf$richness_res &lt;- residuals(boreal_psem1)[,1]\nboreal_sf$ndvi_res &lt;- residuals(boreal_psem1)[,2]\n\nm1 &lt;- tm_shape(boreal_sf) +\n  tm_symbols(col = \"richness_res\", size = 0.75, alpha = 0.75,\n             style = \"fisher\") +\n  tm_layout(legend.outside = TRUE)\nm2 &lt;- tm_shape(boreal_sf) +\n  tm_symbols(col = \"ndvi_res\", size = 0.75, alpha = 0.75,\n             style = \"fisher\") +\n  tm_layout(legend.outside = TRUE)\ntmap_arrange(m1, m2)\n\n\n\n\n\n\n\n\nWhile there’s some pattern, it’s not particularly clear. Instead, we can use Moran’s \\(I\\) to test if there is any significant pattern that we need to worry about. Moran’s \\(I\\) requires an adjacency matrix: a matrix describing the proximity of each observation to the others. For point data, we can estimate this simply as the inverse distance between observations. (Note that the diagonal needs to be set to zero to avoid self referencing):\n\ndistMat &lt;- as.matrix(dist(\n  cbind(boreal$x, boreal$y))\n)\n\ndistsInv &lt;- 1/distMat\ndiag(distsInv) &lt;- 0\n\nNow we can use the Moran.I function from the ape package to estimate this:\n\nMoran.I(boreal_sf$richness_res, distsInv)\n\n$observed\n[1] 0.03853411\n\n$expected\n[1] -0.001879699\n\n$sd\n[1] 0.003998414\n\n$p.value\n[1] 0\n\n\n\nMoran.I(boreal_sf$ndvi_res, distsInv)\n\n$observed\n[1] 0.08014145\n\n$expected\n[1] -0.001879699\n\n$sd\n[1] 0.003986118\n\n$p.value\n[1] 0\n\n\nIn both cases, the low \\(p\\)-value indicates that the residuals are correlated. To fix this, we can replace the OLS-based lm models with generalized least squares (GLS) models. GLS allows the incorporation of covariance models. These can account for autocorrelation from a variety of sources: space, time and groups. The R library nlme has functions for GLS models, including a range of covariance type (see help(corClasses) for the full range). We’ll use one of the simplest (corExp), which assumes that spatial dependency (or autocorrelation) declines exponentially with distance between observations. We’ll use the same function in both models (richness and NDVI), so let’s create a single object to be used in both:\n\n#Fit using spatial autocorrelation\nspaceCor &lt;- corExp(form =~ x+y, nugget = TRUE)\n\nNow we’ll build the GLS models, including the corvariance functions with with correlation argument:\n\nrichness_gls &lt;- gls(richness ~ temp,\n                    correlation = spaceCor,\n                    data = boreal)\nndvi_gls&lt;- gls(NDVI ~ richness + temp + wet,\n               correlation = spaceCor,\n               data=boreal)\n\nAnd we can now use Moran’s \\(I\\) to test if using the covariance functions has accounted for the autocorrelation. (The argument type=\"normalized\" indicates that we want the adjusted residuals.)\n\nboreal_sf$richness_res &lt;- residuals(richness_gls,\n                                    type = \"normalized\")\nMoran.I(boreal_sf$richness_res, distsInv)\n\n$observed\n[1] -0.004073926\n\n$expected\n[1] -0.001879699\n\n$sd\n[1] 0.003995004\n\n$p.value\n[1] 0.5828389\n\nboreal_sf$ndvi_res &lt;- residuals(ndvi_gls,\n                                    type = \"normalized\")\nMoran.I(boreal_sf$ndvi_res, distsInv)\n\n$observed\n[1] -0.00179094\n\n$expected\n[1] -0.001879699\n\n$sd\n[1] 0.003991756\n\n$p.value\n[1] 0.98226\n\n\nBoth tests are non-significant, indicating that the autocorrelation has been accounted for.\nFinally, we can remake the PSEM model with the two spatial models:\n\nboreal_psem2 &lt;- psem(\n  richness_gls,\n  ndvi_gls,\n  data = boreal\n)\nsummary(boreal_psem2, .progressBar = FALSE)\n\n\nStructural Equation Model of boreal_psem2 \n\nCall:\n  richness ~ temp\n  NDVI ~ richness + temp + wet\n\n    AIC\n 1983.090\n\n---\nTests of directed separation:\n\n        Independ.Claim Test.Type  DF Crit.Value P.Value \n  richness ~ wet + ...      coef 533    -0.8368  0.4031 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 9.99 with P-value = 0.002 and on 1 degrees of freedom\nFisher's C = 1.817 with P-value = 0.403 and on 2 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error  DF Crit.Value P.Value Std.Estimate    \n  richness      temp  -0.0357    0.7765 533    -0.0459  0.9634      -0.0028    \n      NDVI  richness  -0.0002    0.0002 533    -1.6250  0.1047      -0.0301    \n      NDVI      temp  -0.0282    0.0033 533    -8.4356  0.0000      -0.2746 ***\n      NDVI       wet  -3.4060    0.1590 533   -21.4266  0.0000      -0.5636 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n  richness   none      0.00\n      NDVI   none      0.65\n\n\nNow the results show no significance in the richness model, and no significance for the path richness -&gt; NDVI (shown in the plot below as dashed lines). In the first model (above), both of these were found to be significant; accounting for the autocorrelation helps to removeor reduce the bias from this, and (in this case) shows that our assumed DAG is not supported by the data, and a simpler set of paths would be sufficient.\n\nplot(boreal_psem2)\n\n\n\n\nPSEM model"
  },
  {
    "objectID": "GEOG6960_Week7.html#boreal-dataset-boreal.csv",
    "href": "GEOG6960_Week7.html#boreal-dataset-boreal.csv",
    "title": "GEOG 6960 Causality in Geog. Studies 7",
    "section": "Boreal dataset boreal.csv",
    "text": "Boreal dataset boreal.csv\nThis dataset is taken from Alain Zuur’s book “Mixed Effects Models and Extensions in Ecology with R”\n\n\n\nColumn header\nVariable\n\n\n\n\npoint\nPoint ID\n\n\nx\nEasting\n\n\ny\nNorthing\n\n\nrichness\nSpecies richness\n\n\nNDVI\nNDVI\n\n\ntemp\nSoil temperature\n\n\nwet\nMoisture index"
  },
  {
    "objectID": "GEOG6960_Week8.html",
    "href": "GEOG6960_Week8.html",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "",
    "text": "Warning: package 'reticulate' was built under R version 4.4.1"
  },
  {
    "objectID": "GEOG6960_Week8.html#introduction",
    "href": "GEOG6960_Week8.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, we’re going to test methods for causal discovery. We’ll use a synthetic example, and a quick test with the Grace and Keeley fire/plant abundance dataset (keeley.csv)."
  },
  {
    "objectID": "GEOG6960_Week8.html#causal-discovery",
    "href": "GEOG6960_Week8.html#causal-discovery",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Causal discovery",
    "text": "Causal discovery\nThere are a range of packages with algorithms for causal discovery in both R and Python. Here, we’ll use:\n\nR: causalDisco - this packages builds on and integrates functions from a set of other packages, mainly from the Bioconductor repository. Installation instructions are given below.\nPython: causal-learn - we’ll this as it is part of a larger Python ecosystem for causal analysis. Other packages include gcastle and cdt, the Causal Discovery Toolbox\n\nFirst load (or install and load) the relevant packages. We’ll need some additional packages to explore the data before model building.\n\nRPython\n\n\nThe causalDisco library has a fairly large number of dependencies, including packages that are not part of the standard CRAN system. You’ll need to run (at least) the following commands after installing causalDisco to get all the appropriate functions. These are all part of the Bioconductor repository to be installed first from here:\nhttps://www.bioconductor.org/\nOnce you’ve installed this, install the following:\n\nBiocManager::install(\"graph\")\nBiocManager::install(\"RBGL\")\nBiocManager::install(\"Rgraphviz\")\n\nNow try loading the packages. If you get errors, please let me know\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(GGally)\nlibrary(causalDisco)\nlibrary(pcalg)\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "GEOG6960_Week8.html#example-1",
    "href": "GEOG6960_Week8.html#example-1",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Example 1",
    "text": "Example 1\n\nData\nFirst, we’re going to create a synthetic dataset, based on the following ‘true’ DAG. This is the structure that we will try to discover later on:\n\n\n\n\n\n\n\n\n\nNote that this implies the following adjacency matrix, where each row represents an origin node, and each column a destination node:\n\\[\nA_{G} =\n\\begin{bmatrix}\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\nThis will have 4 variables, which we will create as follows:\n\na a randomly distributed exogenous variable: \\(a ~ N(0, 1)\\)\nb a randomly distributed exogenous variable: \\(a ~ N(0, 1)\\)\nc an endogenous variable ‘caused’ by a and b: \\(c = 0.3\\times a + 0.2 \\times b + N(0, 0.01)\\)\nd the outcome variable ‘caused’ by c: \\(d = 0.8 \\times c + N(0, 0.01)\\)\n\nIf you want to make this more complex, uncomment the line that creates the variable e. This will add an additional collider to the DAG.\n\nRPython\n\n\n\nset.seed(1)\nn &lt;- 10000\na &lt;- rnorm(n) \nb &lt;- rnorm(n) \nc &lt;- 0.3*a + 0.2*b + rnorm(n, 0, 0.01)\nd &lt;- 0.8*c + rnorm(n, 0, 0.01)\n# e &lt;- -0.4*a + -0.4*d + rnorm(n, 0, 0.01)\ndf &lt;- data.frame(a,b,c,d)\n\n\n\n\nnp.random.seed(1)\nn = 10000\na = np.random.normal(0, 1, n)\nb = np.random.normal(0, 1, n)\nc = 0.3*a + 0.2*b + np.random.normal(0, 0.01, n)\nd = 0.8*c + np.random.normal(0, 0.01, n)\n\ndf = pd.DataFrame({'a': a,\n                   'b': b,\n                   'c': c,\n                   'd': d})\n\n\n\n\nAs usual, we’ll do a little exploration of the data before moving on.\n\nRPython\n\n\n\nggpairs(df)\n\n\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd here’s the covariance and correlation matrices:\n\nRPython\n\n\n\n# Covariance\ncov(df)\n\n            a           b         c          d\na 1.024865587 0.004859711 0.3084241 0.24677905\nb 0.004859711 0.981640410 0.1977951 0.15809477\nc 0.308424135 0.197795129 0.1321874 0.10573187\nd 0.246779052 0.158094769 0.1057319 0.08467078\n\n\n\n# Correlation\ncor(df)\n\n            a           b         c         d\na 1.000000000 0.004845078 0.8379534 0.8377377\nb 0.004845078 1.000000000 0.5490910 0.5483710\nc 0.837953423 0.549091040 1.0000000 0.9994109\nd 0.837737692 0.548371035 0.9994109 1.0000000\n\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495\n\n\n\ndf.corr()\n\n          a         b         c         d\na  1.000000  0.015684  0.833818  0.833262\nb  0.015684  1.000000  0.564368  0.564101\nc  0.833818  0.564368  1.000000  0.999405\nd  0.833262  0.564101  0.999405  1.000000"
  },
  {
    "objectID": "GEOG6960_Week8.html#peters-clark-algorithm",
    "href": "GEOG6960_Week8.html#peters-clark-algorithm",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Peters-Clark algorithm",
    "text": "Peters-Clark algorithm\nWe’ll first estimate the causal graph using the Peters-Clark (PC) algorithm. This starts by creating a fully connected, but undirected graph. Then:\n\nEdges are removed between variables that are unconditionally independent (i.e. no existing covariance)\nEdges are removed between variables that are conditionally independent (i.e. no existing covariance given other nodes)\nColliders are identified and directed\nThe direction of remaining links is established\n\n\nRPython\n\n\nIn R, the function we will use is pc(). Before running this, we need to create a list with summary statistics for the algorithm. For PC, we need the correlation matrix and the number of observations in the dataset. These will be used to assess the first step:\n\ndf_stats &lt;- list(C = cor(df), n = nrow(df))\n\nWith this, we can then run the PC algorithm. This takes as arguments:\n\nThe list of summary statistics\nA predefined function to test for independence (gaussCItest for normally distributed variables, other functions exist for discrete or binary data)\nThe threshold for inclusion (correlations below this threshold will be excluded)\n\n\ndf_pc &lt;- pc(df_stats, labels = names(df),\n            indepTest = gaussCItest, alpha = 0.01)\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495\n\n\n\n\n\nLet’s examine the output:\n\nRPython\n\n\nThe ‘discovered’ adjacency matrix is shown in the summary() output. Compare this to the known matrix above.\n\nsummary(df_pc)\n\nObject of class 'pcAlgo', from Call:\npc(suffStat = df_stats, indepTest = gaussCItest, alpha = 0.01, \n    labels = names(df))\n\nNmb. edgetests during skeleton estimation:\n===========================================\nMax. order of algorithm:  2 \nNumber of edgetests from m = 0 up to m = 2 :  11 12 3\n\nGraphical properties of skeleton:\n=================================\nMax. number of neighbours:  1 at node(s) 1 2 3 \nAvg. number of neighbours:  0.75 \n\nAdjacency Matrix G:\n  a b c d\na . . 1 .\nb . . 1 .\nc . . . 1\nd . . . .\n\n\nYou can also visualize the resulting graph:\n\nplot(df_pc@graph)\n\n\n\n\n\n\n\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495"
  },
  {
    "objectID": "GEOG6960_Week8.html#greedy-equivalence-search-algorithm",
    "href": "GEOG6960_Week8.html#greedy-equivalence-search-algorithm",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Greedy Equivalence Search algorithm",
    "text": "Greedy Equivalence Search algorithm\nNext we’ll use a greedy search method to find the structure, the Greedy Equivalence Search (GES) algorithm. WStarting from a graph with all the nodes but no edges, this will iterate through three stages:\n\nForwards: edges are added until no further improvement is obtained\nBackwards: edges are removed until no further improvement is obtained\nTurning: edges are reversed until no further improvement is obtained\n\nThe improvement is measured by a score, in this case the Bayesian Information Criterion (BIC).\n\nRPython\n\n\nIn R, this requires first setting up the score function. GaussL0penObsScore is a general function for this, which by default will estimate the Bayesian Information Criterion. This also takes an argument (lambda) to change the weighting (default is \\(log(n)/2\\)). Setting this to higher values will penalize against more complex graphs.\n\nscore &lt;- new(\"GaussL0penObsScore\", df)\n\nNow we can estimate the causal structure using the ges() function (setting verbose=TRUE displays the progress through the different steps described above):\n\ndf_ges &lt;- ges(score, verbose = TRUE)\n\nCasting graph...\nCasting options...\nPerforming GIES...\n== starting forward phase (not adaptive)...\n  inserting edge (2, 3) with C = {}, S = 33716\n== starting forward phase (not adaptive)...\n  inserting edge (2, 0) with C = {}, S = 6051.49\n== starting forward phase (not adaptive)...\n  inserting edge (1, 2) with C = {0}, S = 29796.6\n== starting forward phase (not adaptive)...\n== starting backward phase...\n== starting turning phase...\n== starting forward phase (not adaptive)...\n== starting backward phase...\n== starting turning phase...\n\n\nThe resulting object contains the ‘discovered’ graph:\nWhich can be convert to an adjacency matrix for comparison:\n\nas(as(df_ges$essgraph,\"graphNEL\"),\"Matrix\")\n\n4 x 4 sparse Matrix of class \"ngCMatrix\"\n  a b c d\na . . | .\nb . . | .\nc . . . |\nd . . . .\n\n\nAnd visualized:\n\nplot(df_ges$essgraph)\n\n\n\n\n\n\n\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495"
  },
  {
    "objectID": "GEOG6960_Week8.html#example-data",
    "href": "GEOG6960_Week8.html#example-data",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Example data",
    "text": "Example data\nFirst, we’re going to create a synthetic dataset, based on the following ‘true’ DAG. This is the structure that we will try to discover later on:\n\n\n\n\n\n\n\n\n\nNote that this implies the following adjacency matrix, where each row represents an origin node, and each column a destination node:\n\\[\nA_{G} =\n\\begin{bmatrix}\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\]\nThis will have 4 variables, which we will create as follows:\n\na a randomly distributed exogenous variable: \\(a ~ N(0, 1)\\)\nb a randomly distributed exogenous variable: \\(a ~ N(0, 1)\\)\nc an endogenous variable ‘caused’ by a and b: \\(c = 0.3\\times a + 0.2 \\times b + N(0, 0.01)\\)\nd the outcome variable ‘caused’ by c: \\(d = 0.8 \\times c + N(0, 0.01)\\)\n\nIf you want to make this more complex, uncomment the line that creates the variable e. This will add an additional collider to the DAG.\n\nRPython\n\n\n\nset.seed(1)\nn &lt;- 10000\na &lt;- rnorm(n) \nb &lt;- rnorm(n) \nc &lt;- 0.3*a + 0.2*b + rnorm(n, 0, 0.01)\nd &lt;- 0.8*c + rnorm(n, 0, 0.01)\n# e &lt;- -0.4*a + -0.4*d + rnorm(n, 0, 0.01)\ndf &lt;- data.frame(a,b,c,d)\n\n\n\n\nnp.random.seed(1)\nn = 10000\na = np.random.normal(0, 1, n)\nb = np.random.normal(0, 1, n)\nc = 0.3*a + 0.2*b + np.random.normal(0, 0.01, n)\nd = 0.8*c + np.random.normal(0, 0.01, n)\n\ndf = pd.DataFrame({'a': a,\n                   'b': b,\n                   'c': c,\n                   'd': d})\n\n\n\n\nAs usual, we’ll do a little exploration of the data before moving on.\n\nRPython\n\n\n\nggpairs(df)\n\n\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd here’s the covariance and correlation matrices:\n\nRPython\n\n\n\n# Covariance\ncov(df)\n\n            a           b         c          d\na 1.024865587 0.004859711 0.3084241 0.24677905\nb 0.004859711 0.981640410 0.1977951 0.15809477\nc 0.308424135 0.197795129 0.1321874 0.10573187\nd 0.246779052 0.158094769 0.1057319 0.08467078\n\n\n\n# Correlation\ncor(df)\n\n            a           b         c         d\na 1.000000000 0.004845078 0.8379534 0.8377377\nb 0.004845078 1.000000000 0.5490910 0.5483710\nc 0.837953423 0.549091040 1.0000000 0.9994109\nd 0.837737692 0.548371035 0.9994109 1.0000000\n\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495\n\n\n\ndf.corr()\n\n          a         b         c         d\na  1.000000  0.015684  0.833818  0.833262\nb  0.015684  1.000000  0.564368  0.564101\nc  0.833818  0.564368  1.000000  0.999405\nd  0.833262  0.564101  0.999405  1.000000"
  },
  {
    "objectID": "GEOG6960_Week8.html#causal-discovery-of-keeley-and-grace",
    "href": "GEOG6960_Week8.html#causal-discovery-of-keeley-and-grace",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Causal discovery of Keeley and Grace",
    "text": "Causal discovery of Keeley and Grace\nLet’s finish by running one of these algorithms with a real dataset. We’ll use the Grace and Keeley data set again here. First load it (and remove the elevation column):\n\nRPython\n\n\n\nkeeley &lt;- read.csv(\"data/keeley.csv\")\nkeeley &lt;- keeley %&gt;%\n    select(-elev)\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495\n\n\n\n\n\nAs a reminder, the DAG published in the 2009 paper looks like this:\n\n\n\n\n\n\n\n\n\nSo, let’s see how close the PC algorithm comes to this:\n\nRPython\n\n\n\nkeeley_stats &lt;- list(C = cor(keeley), \n                     n = nrow(keeley))\nkeeley_pc &lt;- pc(keeley_stats, labels = names(keeley),\n                indepTest = gaussCItest, alpha = 0.01)\nplot(keeley_pc@graph)\n\n\n\n\n\n\n\n\n\n\n\ndf.cov()\n\n          a         b         c         d\na  0.997673  0.015685  0.302499  0.241931\nb  0.015685  1.002480  0.205238  0.164176\nc  0.302499  0.205238  0.131922  0.105515\nd  0.241931  0.164176  0.105515  0.084495\n\n\n\n\n\nThe results are a bit of a mixed bag. The returned graph is split into two. In one, the richness is correctly identified as being caused by three of the variables. However, the chain linking age to fire severity and richness, while identified, is not linked."
  },
  {
    "objectID": "GEOG6960_Week8.html#summary",
    "href": "GEOG6960_Week8.html#summary",
    "title": "GEOG 6960 Causality in Geog. Studies 8",
    "section": "Summary",
    "text": "Summary\nThe final results show that caution is required when applying these methods. These cannot incorporate domain knowledge and so can only find the optimal structure according to the rules of the algorithm. With that in mind, these can provide a useful first pass through the data that can then be modified using theory and domain expertise."
  }
]