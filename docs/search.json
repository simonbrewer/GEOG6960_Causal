[
  {
    "objectID": "GEOG6960_Week2.html",
    "href": "GEOG6960_Week2.html",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#introduction",
    "href": "GEOG6960_Week2.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#packages",
    "href": "GEOG6960_Week2.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(MatchIt)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "GEOG6960_Week2.html#simulating-data",
    "href": "GEOG6960_Week2.html#simulating-data",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Simulating data",
    "text": "Simulating data\nFirst, we’re going to create a synthetic dataset for use in the lab. Simulating these types of data can be very useful in understanding how models work, and we’ll use it here to illustrate the difference between a randomized trial and a trial where the treatment (\\(T\\)) is biased. This is particularly useful for causal inference as simulating data allows us to see both the factual (observed) and counterfactual (unobserved) outcomes.\nWe’re going to use the same example shown in the lectures: a study aiming to estimate the effect of computer tablets (\\(T\\)) on student test outcomes (\\(Y\\)). The confounding variable (\\(X\\)) is the school tuition, taken as a proxy for school wealth. One twist here is that we want to assign tablets by school, not by student, which makes this slightly more complicated.\nBefore we start, we need to decide some values for the data. You’re very welcome to change these to different values, but I’d suggest first running this with the values given here, then going back to see how changing these affects your results.\nWe’ll start by setting the random seed (to ensure we get the same results). Again, feel free to change this, but your results will differ slightly from those in this document:\n\nRPython\n\n\n\nset.seed(1242)\n\n\n\n\nnp.random.seed(42)\n\n\n\n\nNext, let’s define the number of observations:\n\nNumber of schools: 50\nNumber of students per school: 20\n\n\nRPython\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\n\nSchools\nWe’ll assign the tuition levels randomly from a normal distribution with a mean of 1000 and s.d. of 300:\n\nRPython\n\n\n\ntuition = round(rnorm(n_schools, 1000, 300))\n\n\n\n\ntuition = np.round(np.random.normal(1000, 300, n_schools))\n\n\n\n\nNow, we’ll use the tuition to decide whether or not a school assigns tablets to students. We’ll do this randomly, using a binomial distribution, where the probability of a school assign tablets is given by first converting the tuition to a \\(z\\)-score:\n\\[\n\\mbox{tuition}_z = (\\mbox{tuition} - mean(\\mbox{tuition}) / sd(\\mbox{tuition})\n\\]\nThen we get \\(p\\) for each school as:\n\\[\np_\\mbox{tablet} = exp(\\mbox{tuition}_z) / (1 + exp(\\mbox{tuition}_z))\n\\]\nPutting this into practice:\n\nRPython\n\n\n\ntuition_z = (tuition - mean(tuition)) / sd(tuition)\ntuition_p = exp(tuition_z)/(1+exp(tuition_z))\ntablet = rbinom(n_schools, 1, tuition_p)\n\nLet’s put all of this into a data frame:\n\nschool_df = data.frame(id = as.factor(1:length(tablet)), \n                       tuition = tuition, \n                       tuition_p = tuition_p,\n                       tablet = as.factor(tablet))\n\n\n\n\ntuition_z = (tuition - tuition.mean()) / tuition.std()\ntuition_p = np.exp(tuition_z)/(1+np.exp(tuition_z))\ntablet = np.random.binomial(1, tuition_p, n_schools)\n\nLet’s put all of this into a data frame:\n\nschool_df = pd.DataFrame({'id': np.arange(n_schools), \n                          'tuition': tuition,\n                          'tablet': tablet})\n\n\n\n\nAnd we can now visualize some of the results (this is a good way to check that we get what we expect):\n\nRPython\n\n\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45)\n\n\n\n\n\n\n\n\n\nggboxplot(school_df, x = \"tablet\", y = \"tuition\") +\n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\nsns.boxplot(school_df, x=\"tablet\", y=\"tuition\", hue=\"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWe can also test for differences in the tuition rates based on whether or not tablets were assigned:\n\nRPython\n\n\n\nt.test(tuition ~ tablet, school_df)\n\n\n    Welch Two Sample t-test\n\ndata:  tuition by tablet\nt = -4.3665, df = 47.53, p-value = 6.79e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -494.1193 -182.4862\nsample estimates:\nmean in group 0 mean in group 1 \n       848.9565       1187.2593 \n\n\n\n\n\nfrom statsmodels.stats.weightstats import ttest_ind\nt_stat, p_value, df = ttest_ind(school_df[school_df['tablet'] == 1]['tuition'], \n                                school_df[school_df['tablet'] == 0]['tuition'])\nprint(f'T: {t_stat}; p-value: {p_value}')\n\nT: 1.8772572008954862; p-value: 0.06656723616720626\n\n\n\n\n\n\n\nStudents\nNow we’ll create class_size students for each school. We first make a data frame of students, by simply repeating the school values for tuition and tablet:\n\nRPython\n\n\n\nstudent_df = data.frame(id = 1:(n_students),\n                        school_id = rep(school_df$id, each = class_size),\n                        tuition = rep(tuition, each = class_size),\n                        tablet = factor(rep(tablet, each = class_size)))\n\n\n\n\nstudent_df = pd.DataFrame({'id': np.arange(n_students),\n                           'school_id': np.repeat(school_df['id'], class_size),\n                           'tuition': np.repeat(school_df['tuition'], class_size),\n                           'tablet': np.repeat(school_df['tablet'], class_size)})\n\n\n\n\nNow we’ll create a test score for each student. This will again be random, but based on the tuition values of the school (to reflect that we expect students at higher funded schools to test better). Student scores will be taken from a random normal distribution with a s.d. of 200 and the mean given by \\(200 + 0.7 \\times \\mbox{tuition}\\). We’ll then rescale the scores so that the maximum is 1000.\n\nRPython\n\n\n\nstudent_df$enem_score0 = rnorm(n_students, 200 +\n                                0.7 * student_df$tuition, 200) \nstudent_df$enem_score0 =\n  (student_df$enem_score0 - min(student_df$enem_score0)) /\n  max(student_df$enem_score0) * 1000\n\n\n\n\nstudent_df['enem_score0'] = np.random.normal(200 + 0.7 * student_df['tuition'], 200, n_students) \n\nstudent_df['enem_score0'] = (student_df['enem_score0'] - student_df['enem_score0'].min()) / student_df['enem_score0'].max() * 1000.0\n\n\n\n\nNote that this score (enem_score0) is the factual for students who were not assigned a tablet, and the counterfactual for students who were.\nFinally, we’ll add a tablet effect. This is the expected change in a student’s score if they were assigned a tablet. For this exercise, we’ll assume that having a tablet reduces scores by 50 points on average, but with a s.d. of 5.\n\nRPython\n\n\n\nstudent_df$tablet_eff = rnorm(n_students, -50, 5)\ngghistogram(student_df, x = \"tablet_eff\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['tablet_eff'] = np.random.normal(-50, 5, n_students)\nsns.histplot(student_df, x = \"tablet_eff\")\n\n\n\n\n\n\n\n\n\n\n\nFinally, add the tablet effect back to the score. We multiply by the binary tablet assignation.\n\nRPython\n\n\n\nstudent_df$enem_score1 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet)-1\nggboxplot(student_df, \n          x = \"tablet\", \n          y = \"enem_score1\", \n          fill = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['enem_score1'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet']\n\nsns.boxplot(student_df, x = \"tablet\", y = \"enem_score1\", \n          hue = \"tablet\")"
  },
  {
    "objectID": "GEOG6960_Week2.html#first-test",
    "href": "GEOG6960_Week2.html#first-test",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "First test",
    "text": "First test\nWith the data in had, we can test for the causal effect of tablet on the observed scores enem_score1. Before running this, just a reminder of two points. Given the way we have created the data set, we know this is expected to be negative and around -50. But we also know that there is a bias in the tablet assignment from the tuition rates.\nAs the test scores are normally distributed, and we have two groups (treated and control), we can use a \\(t\\)-test to explore the differences (as shown above). More usefully, we replace this with a linear model (lm in R or statsmodels.OLS in Python), as this will allow us to test for significance and give us an estimate of the effect in the coefficient \\(\\beta_1\\):\n\\[\n\\mbox{enem\\_score} = \\beta_0 + \\beta_1 \\times \\mbox{tablet}\n\\]\n(As an aside, while they are often taught separately, most statistical tests are just special cases of the linear model…)\n\nRPython\n\n\n\nsummary(lm(enem_score1 ~ tablet, student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\nWhich gives us a highly significant effect of 77.64. Now you should be able to see the impact of the tuition bias: we expected an effect of around -50 and we got 77.64 instead.\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):              0.859\nTime:                        18:57:02   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nYou can also see this effect if you plot the test scores against tuition:\n\nRPython\n\n\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          col = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          hue = \"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWhere you’ll see both the influence of tuition and the asymmetric distribution of tablets."
  },
  {
    "objectID": "GEOG6960_Week2.html#randomized-trial",
    "href": "GEOG6960_Week2.html#randomized-trial",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Randomized trial",
    "text": "Randomized trial\nWe’ll now repeat this test, but by simulating a random trial of tablets across schools. To keep this comparable to the previous (biased) example, we’ll work with the same data. First we assign tablets randomly\n\nRPython\n\n\n\nschool_df$tablet_rct = as.factor(sample(rep(c(0,1), n_schools/2)))\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet_rct\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45) \n\n\n\n\n\n\n\n\n\n\n\nschool_df['tablet_rct'] = school_df['tablet'].sample(n_schools).to_numpy()\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet_rct\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\n\n\nNext we create a new set of test scores by updating the original scores (enem_score0) with tablet effect multiplied by the new tablet assignment. If we then repeat the scatter plot using the new scores and tablet assignments, you should see a more even distribution:\n\nRPython\n\n\n\nstudent_df$tablet_rct = rep(school_df$tablet_rct, each = class_size)\nstudent_df$enem_score2 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet_rct)-1\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          col = \"tablet_rct\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nsummary(lm(enem_score2 ~ tablet_rct, student_df))\n\n\nCall:\nlm(formula = enem_score2 ~ tablet_rct, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-442.06 -115.71   -5.93  118.81  486.75 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  380.065      7.399  51.367  &lt; 2e-16 ***\ntablet_rct1  -33.930     10.464  -3.243  0.00122 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.4 on 998 degrees of freedom\nMultiple R-squared:  0.01043,   Adjusted R-squared:  0.009434 \nF-statistic: 10.51 on 1 and 998 DF,  p-value: 0.001223\n\n\n\n\n\nstudent_df['tablet_rct'] = np.repeat(school_df['tablet_rct'], class_size)\nstudent_df['enem_score2'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet_rct']\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          hue = \"tablet_rct\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nmod = smf.ols(formula='enem_score2 ~ tablet_rct', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score2   R-squared:                       0.037\nModel:                            OLS   Adj. R-squared:                  0.037\nMethod:                 Least Squares   F-statistic:                     38.86\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           6.71e-10\nTime:                        18:57:02   Log-Likelihood:                -6347.3\nNo. Observations:                1000   AIC:                         1.270e+04\nDf Residuals:                     998   BIC:                         1.271e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.5017      6.312     60.281      0.000     368.115     392.888\ntablet_rct   -54.5671      8.753     -6.234      0.000     -71.744     -37.390\n==============================================================================\nOmnibus:                        3.915   Durbin-Watson:                   1.067\nProb(Omnibus):                  0.141   Jarque-Bera (JB):                3.988\nSkew:                           0.145   Prob(JB):                        0.136\nKurtosis:                       2.892   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#propensity-score-matching",
    "href": "GEOG6960_Week2.html#propensity-score-matching",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nIn the previous sections, we looked at the effect of having a randomized or biased design in our data, and how this can impact the conclusions that we draw. But what do you do when you don’t have a randomized trial? In a lot of situations, we have natural experiments; where ‘treatments’ have taken place for other reasons than our tests. This is the case with the first set of test scores - these were created to mimic a natural experiment where schools had decided themselves (and partly based on finances) whether or not to give students tablets. In this case, we can use propensity score matching to try and reduce any biases.\nThe aim here is to create a subset of data with matched treated and control samples, where the confounding variables (e.g. tuition) are used to make the matches. The idea being that if we have treatments and controls for similar tuition levels, then the remaining difference in test scores should be due to the effect of the treatment (the tablets in our example).\nHere, we’ll look briefly at how propensity scores are calculated, then use an add-on package to calculate these for our dataset. Finally, we’ll re-run our model to test for tablet-related test score differences with the new, matched set.\nLet’s remind ourselves of the data we have available:\n\nRPython\n\n\n\nhead(school_df)\n\n  id tuition tuition_p tablet tablet_rct\n1  1    1084 0.5402727      1          1\n2  2     861 0.3714199      1          0\n3  3     936 0.4268085      0          1\n4  4    1620 0.8598539      1          0\n5  5    1428 0.7724307      1          1\n6  6     784 0.3178776      1          1\n\n\n\nhead(student_df)\n\n  id school_id tuition tablet enem_score0 tablet_eff enem_score1 tablet_rct\n1  1         1    1084      1    730.6342  -50.60501    628.4242          1\n2  2         1    1084      1    585.3498  -51.11708    482.1156          1\n3  3         1    1084      1    618.7099  -44.67392    528.3621          1\n4  4         1    1084      1    513.1673  -53.44279    405.2818          1\n5  5         1    1084      1    475.5372  -46.81305    380.9111          1\n6  6         1    1084      1    639.4113  -41.48421    555.4428          1\n  enem_score2\n1    628.4242\n2    482.1156\n3    528.3621\n4    405.2818\n5    380.9111\n6    555.4428\n\n\n\n\n\nschool_df.head()\n\n   id  tuition  tablet  tablet_rct\n0   0   1149.0       1           0\n1   1    959.0       0           1\n2   2   1194.0       1           0\n3   3   1457.0       0           0\n4   4    930.0       1           1\n\n\n\nstudent_df.head()\n\n   id  school_id  tuition  ...  enem_score1  tablet_rct  enem_score2\n0   0          0   1149.0  ...   345.201552           0   396.496762\n0   1          0   1149.0  ...   449.695558           0   500.677307\n0   2          0   1149.0  ...   408.749080           0   459.107087\n0   3          0   1149.0  ...   495.975591           0   546.161703\n0   4          0   1149.0  ...   332.925810           0   379.287662\n\n[5 rows x 9 columns]\n\n\n\n\n\nAlthough we are testing for the differences in students, the assignment (and therefore propensity) needs to be calculated for the schools, so we’ll use schools_df for the next steps. Note that propensity score usually works best with larger datasets, and is somewhat limited with only 50 samples.\nPropensity scores are simply the probability that a given observation was selected for the treatment. The important part is that we want to estimate these probabilities using the same covariate(s) that we think (or know) caused the bias in the treatment. We’ll estimate this here using binomial regression in a generalized linear model, but note you can use any model that works with a binary outcome (random forests, boosted trees, etc).\n\nRPython\n\n\nIn R, we can fit this model using glm and by setting the family to binomial:\n\nfit_ps &lt;- glm(tablet ~ tuition, school_df, family = binomial())\nsummary(fit_ps)\n\n\nCall:\nglm(formula = tablet ~ tuition, family = binomial(), data = school_df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -4.114829   1.321285  -3.114  0.00184 **\ntuition      0.004234   0.001294   3.273  0.00106 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 68.994  on 49  degrees of freedom\nResidual deviance: 53.442  on 48  degrees of freedom\nAIC: 57.442\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df &lt;- data.frame(prop_score = predict(fit_ps, type = \"response\"),\n                     tablet = as.numeric(fit_ps$model$tablet)-1,\n                     tuition = fit_ps$model$tuition)\nhead(prs_df)\n\n  prop_score tablet tuition\n1  0.6165972      1    1084\n2  0.3848259      1     861\n3  0.4621865      0     936\n4  0.9396136      1    1620\n5  0.8734399      1    1428\n6  0.3110631      1     784\n\n\n\n\nIn Python, we can fit this model using the glm function from statsmodels and by setting the family to binomial:\n\nmod = smf.glm(formula='tablet ~ tuition', data=school_df, family=sm.families.Binomial())\nfit = mod.fit()\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 tablet   No. Observations:                   50\nModel:                            GLM   Df Residuals:                       48\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -32.853\nDate:                Sun, 25 Aug 2024   Deviance:                       65.706\nTime:                        18:57:03   Pearson chi2:                     50.1\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.06814\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.7868      1.077     -1.659      0.097      -3.897       0.324\ntuition        0.0020      0.001      1.794      0.073      -0.000       0.004\n==============================================================================\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df = pd.DataFrame({'prop_score': fit.predict(),\n                        'tablet': school_df['tablet'],\n                        'tuition': school_df['tuition']})\nprs_df.head()\n\n   prop_score  tablet  tuition\n0    0.627858       1   1149.0\n1    0.535210       0    959.0\n2    0.648739       1   1194.0\n3    0.758087       0   1457.0\n4    0.520682       1    930.0\n\n\n\n\n\nTo illustrate how a simple match would happen, let’s split this into a treatment and control data set:\n\nRPython\n\n\n\ntreated_df = prs_df %&gt;%\n  filter(tablet == 1)\ncontrol_df = prs_df %&gt;%\n  filter(tablet == 0)\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nmatch_id = which.min(abs(treated_df$prop_score[1] - control_df$prop_score))\nmatch_id\n\n[1] 5\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df[match_id, ]\n\n   prop_score tablet tuition\n15  0.6175977      0    1085\n\n\n\n\n\ntreated_df = prs_df[prs_df['tablet'] == 1].reset_index()\ncontrol_df = prs_df[prs_df['tablet'] == 0].reset_index()\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nabs_diff = (treated_df['prop_score'][0] - control_df['prop_score']).abs()\nmatch_id = abs_diff.idxmin()\nprint(match_id)\n\n4\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df.iloc[match_id,:]\n\nindex            9.00000\nprop_score       0.63441\ntablet           0.00000\ntuition       1163.00000\nName: 4, dtype: float64\n\n\n\n\n\nWe could obviously make this into a loop and get all the matches, but instead we’ll use an external package to carry out the full match.\n\nRPython\n\n\nIn R, the package we will use is called MatchIt. It is pretty well established and allows you to choose different method to calculate the scores and carry out matching.\nTo get an idea of the output, we’ll first run this with no matching (method = NULL). The output will show some summary statistics on the match between the treatment and control. The first line (distance) shows the difference in propensity score between the two groups and the second (and subsequent) line shows the difference in the covariate. A useful index is the standardized mean difference, which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\nmatch0 = matchit(tablet ~ tuition, data = school_df,\n                 method = NULL, distance = \"glm\")\nsummary(match0)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = NULL, \n    distance = \"glm\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.3944          1.0838     1.4057    0.2842\ntuition       1187.259      848.9565          1.0929     1.6977    0.2842\n         eCDF Max\ndistance   0.5427\ntuition    0.5427\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      27\nUnmatched       0       0\nDiscarded       0       0\n\n\nNow, we’ll re-run and use nearest neighbor matching to selected control schools.\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\")\n\nWarning: Fewer control units than treated units; not all treated units will get\na match.\n\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\")\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.7356        0.3944          1.3717     0.8313    0.3617\ntuition      1269.9130      848.9565          1.3599     1.1421    0.3617\n         eCDF Max Std. Pair Dist.\ndistance   0.6522          1.3717\ntuition    0.6522          1.3599\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      23\nUnmatched       0       4\nDiscarded       0       0\n\n\nThe results here are slightly worse (the std. differences have increased). This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\", \n                 replace = TRUE)\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\", replace = TRUE)\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.6353          0.1156     0.9800    0.0585\ntuition       1187.259     1123.9259          0.2046     1.2934    0.0585\n         eCDF Max Std. Pair Dist.\ndistance   0.3704          0.1641\ntuition    0.3704          0.2549\n\nSample Sizes:\n              Control Treated\nAll              23.       27\nMatched (ESS)     3.9      27\nMatched           8.       27\nUnmatched        15.        0\nDiscarded         0.        0\n\n\nNow we obtain a better match as shown by the decrease in std. differences. Note in the sample sizes that the total number of retained control samples is only 8, which is probably too low in practice.\nWe can see the results of the match using the plot() function. For example, this shows the histograms of treated (top) and control (bottom), before (left) and after (right) matching.\n\nplot(match1, type = \"hist\", interactive = FALSE)\n\n\n\n\n\n\n\n\nAnd this shows the same for the empirical cumulative distribution functions:\n\nplot(match1, type = \"ecdf\", interactive = FALSE)\n\n\n\n\n\n\n\n\nWe can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools. First extract the match ‘ids’ (the rows from the original school_df data frame)\n\nmatch_df = get_matches(match1, id = \"mid\")\n\nNow we can loop across these and create a new data frame by appending the students from each matched school in turn:\n\nmatch_student_df = NULL\nfor (i in 1:nrow(match_df)) {\n  tmp_df = student_df %&gt;%\n    filter(school_id == as.numeric(match_df$mid)[i])\n  match_student_df = rbind(match_student_df, tmp_df)\n}\n\nAnd finally, we can repeat our test:\n\nsummary(lm(enem_score1 ~ tablet, match_student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = match_student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -109.72    7.64   95.34  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  427.098      6.451  66.212  &lt; 2e-16 ***\ntablet1      -30.184      9.122  -3.309 0.000968 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 149.9 on 1078 degrees of freedom\nMultiple R-squared:  0.01005,   Adjusted R-squared:  0.009135 \nF-statistic: 10.95 on 1 and 1078 DF,  p-value: 0.0009681\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!\n\n\nIn Python, the package we will use is called psmpy. It is a solid and fairly widely used package, but doesn’t offer quite the same flexibility as R. The main function is PsmPy, and uses a similar format to SciKit-Learn, where methods are initialized then fit to the data. We need to specify:\n\nThe data frame holding the data\nThe treatment (this is the tablet variable)\nA column with observation IDs (these will be used in matching)\nAny varaibles that we want to exclude from the propensity score estimates\n\n\nfrom psmpy import PsmPy\npsm = PsmPy(school_df, treatment='tablet', indx='id', exclude = ['tablet_rct'])\n\nOnce we’ve set this up, we can calculate the propensity score using a binomial (logisitic) model as follows. THe resulting dataframe contains the propensity scores on both a probability and logit scale:\n\npsm.logistic_ps(balance = True)\npsm.predicted_data.head()\n\n   id  tuition  propensity_score  propensity_logit  tablet\n0   0   1149.0          0.582970          0.334979       1\n1   2   1194.0          0.595662          0.387423       1\n2   4    930.0          0.519927          0.079750       1\n3   6   1474.0          0.671227          0.713738       1\n4   8    859.0          0.499251         -0.002996       1\n\n\nOnce this is run, we can use the results to carry out nearest neighbor matching to selected control schools.\n\npsm.knn_matched(matcher='propensity_logit', replacement=False, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\nWe can explore the matches. First, we can plot a histogram of the matched propensity scores. Ideally, these histograms would roughly match, but there is still quite a lot of visible differences\n\npsm.plot_match()\n\n\n\n\n\n\n\n\nA useful index is the standardized mean difference (called the effect_size), which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\n\npsm.effect_size\n\n  Variable matching  Effect Size\n0  tuition   before     0.531394\n1  tuition    after     0.370122\n\n\nThis shows that we have reduced the difference (the after effect_size is lower), but it remains fairly high. This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\npsm.knn_matched(matcher='propensity_logit', replacement=True, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\nNow we obtain a better match as shown by the decrease in effect size. Another useful diagnostic is to plot values of covariates for the matched treated and control samples as histograms:\n\nfig, axs = plt.subplots(ncols=2)\nsns.histplot(school_df, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[0]).set(title='Before')\nsns.histplot(psm.df_matched, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nOr as empirical cumulative distribution functions:\n\nfig, axs = plt.subplots(ncols=2)\nsns.ecdfplot(school_df, x = \"tuition\", hue=\"tablet\", ax=axs[0]).set(title='Before')\nsns.ecdfplot(psm.df_matched, x = \"tuition\", hue=\"tablet\", ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nAs these now align pretty well after the matching, we can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools.\n\nmatch_df = psm.df_matched\nmatched_student_df = pd.DataFrame(columns=student_df.columns)\nfor idx, row in match_df.iterrows():\n    #print(row['id'])\n    tmp_df = student_df[student_df['school_id'] == row['id']]\n    matched_student_df = pd.concat([matched_student_df, tmp_df], ignore_index = True)\n\n&lt;string&gt;:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n    \nmatched_student_df.head()\n\n   id school_id  tuition  ... enem_score1  tablet_rct  enem_score2\n0  20         1    959.0  ...  408.713490           1   355.594645\n1  21         1    959.0  ...  375.552762           1   335.122918\n2  22         1    959.0  ...  191.336345           1   140.382933\n3  23         1    959.0  ...  380.340349           1   331.427514\n4  24         1    959.0  ...  389.004593           1   343.354932\n\n[5 rows x 9 columns]\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=matched_student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.020\nModel:                            OLS   Adj. R-squared:                  0.019\nMethod:                 Least Squares   F-statistic:                     19.11\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           1.37e-05\nTime:                        18:57:06   Log-Likelihood:                -5941.3\nNo. Observations:                 940   AIC:                         1.189e+04\nDf Residuals:                     938   BIC:                         1.190e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept     351.4540      6.146     57.187      0.000     339.393     363.515\ntablet[T.1]   -38.4086      8.785     -4.372      0.000     -55.650     -21.167\n==============================================================================\nOmnibus:                       13.303   Durbin-Watson:                   1.085\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               13.639\nSkew:                           0.295   Prob(JB):                      0.00109\nKurtosis:                       2.992   Cond. No.                         2.59\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!"
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "href": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not recieve a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           1.18e-08\nTime:                        18:12:11   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):              0.859\nTime:                        18:12:11   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-probability-weighting",
    "href": "GEOG6960_Week2.html#inverse-probability-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not recieve a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           1.18e-08\nTime:                        18:57:06   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):              0.859\nTime:                        18:57:06   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "99_html_index.html",
    "href": "99_html_index.html",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "",
    "text": "Often in scientific studies we are interested in establishing a cause and effect, e.g. what is the effect of some policy on health outcomes or the effect of anthropogenic activity on a changing climate. However, most statistical texts and courses are taught using correlative methods, with the mantra that “correlation is not causation” outside of certain strict experimental conditions. Recent work by Judea Pearl and others have developed a framework (Structural Causal Modeling) for causal inference that allows causality to be inferred even when these conditions are not met, allowing this approach in a much broader range of studies. In this seminar, we will review the history and concepts of causal analysis, and go through the steps of Pearl’s causal framework using a set of hands-on examples. The class will largely follow the outline of The Book of Why (Pearl and Mackenzie, 2018; Basic Books, NY). Students will develop their own analysis over the course of the semester through a series of discussions and presentations."
  },
  {
    "objectID": "99_html_index.html#footnotes",
    "href": "99_html_index.html#footnotes",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎"
  }
]