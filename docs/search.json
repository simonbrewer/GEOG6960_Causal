[
  {
    "objectID": "GEOG6960_Week2.html",
    "href": "GEOG6960_Week2.html",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#introduction",
    "href": "GEOG6960_Week2.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week2.html#packages",
    "href": "GEOG6960_Week2.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(MatchIt)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "GEOG6960_Week2.html#simulating-data",
    "href": "GEOG6960_Week2.html#simulating-data",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Simulating data",
    "text": "Simulating data\nFirst, we’re going to create a synthetic dataset for use in the lab. Simulating these types of data can be very useful in understanding how models work, and we’ll use it here to illustrate the difference between a randomized trial and a trial where the treatment (\\(T\\)) is biased. This is particularly useful for causal inference as simulating data allows us to see both the factual (observed) and counterfactual (unobserved) outcomes.\nWe’re going to use the same example shown in the lectures: a study aiming to estimate the effect of computer tablets (\\(T\\)) on student test outcomes (\\(Y\\)). The confounding variable (\\(X\\)) is the school tuition, taken as a proxy for school wealth. One twist here is that we want to assign tablets by school, not by student, which makes this slightly more complicated.\nBefore we start, we need to decide some values for the data. You’re very welcome to change these to different values, but I’d suggest first running this with the values given here, then going back to see how changing these affects your results.\nWe’ll start by setting the random seed (to ensure we get the same results). Again, feel free to change this, but your results will differ slightly from those in this document:\n\nRPython\n\n\n\nset.seed(1242)\n\n\n\n\nnp.random.seed(42)\n\n\n\n\nNext, let’s define the number of observations:\n\nNumber of schools: 50\nNumber of students per school: 20\n\n\nRPython\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\n\n\n\nSchools\nWe’ll assign the tuition levels randomly from a normal distribution with a mean of 1000 and s.d. of 300:\n\nRPython\n\n\n\ntuition = round(rnorm(n_schools, 1000, 300))\n\n\n\n\ntuition = np.round(np.random.normal(1000, 300, n_schools))\n\n\n\n\nNow, we’ll use the tuition to decide whether or not a school assigns tablets to students. We’ll do this randomly, using a binomial distribution, where the probability of a school assign tablets is given by first converting the tuition to a \\(z\\)-score:\n\\[\n\\mbox{tuition}_z = (\\mbox{tuition} - mean(\\mbox{tuition}) / sd(\\mbox{tuition})\n\\]\nThen we get \\(p\\) for each school as:\n\\[\np_\\mbox{tablet} = exp(\\mbox{tuition}_z) / (1 + exp(\\mbox{tuition}_z))\n\\]\nPutting this into practice:\n\nRPython\n\n\n\ntuition_z = (tuition - mean(tuition)) / sd(tuition)\ntuition_p = exp(tuition_z)/(1+exp(tuition_z))\ntablet = rbinom(n_schools, 1, tuition_p)\n\nLet’s put all of this into a data frame:\n\nschool_df = data.frame(id = as.factor(1:length(tablet)), \n                       tuition = tuition, \n                       tuition_p = tuition_p,\n                       tablet = as.factor(tablet))\n\n\n\n\ntuition_z = (tuition - tuition.mean()) / tuition.std()\ntuition_p = np.exp(tuition_z)/(1+np.exp(tuition_z))\ntablet = np.random.binomial(1, tuition_p, n_schools)\n\nLet’s put all of this into a data frame:\n\nschool_df = pd.DataFrame({'id': np.arange(n_schools), \n                          'tuition': tuition,\n                          'tablet': tablet})\n\n\n\n\nAnd we can now visualize some of the results (this is a good way to check that we get what we expect):\n\nRPython\n\n\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45)\n\n\n\n\n\n\n\n\n\nggboxplot(school_df, x = \"tablet\", y = \"tuition\") +\n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\nsns.boxplot(school_df, x=\"tablet\", y=\"tuition\", hue=\"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWe can also test for differences in the tuition rates based on whether or not tablets were assigned:\n\nRPython\n\n\n\nt.test(tuition ~ tablet, school_df)\n\n\n    Welch Two Sample t-test\n\ndata:  tuition by tablet\nt = -4.3665, df = 47.53, p-value = 6.79e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -494.1193 -182.4862\nsample estimates:\nmean in group 0 mean in group 1 \n       848.9565       1187.2593 \n\n\n\n\n\nfrom statsmodels.stats.weightstats import ttest_ind\nt_stat, p_value, df = ttest_ind(school_df[school_df['tablet'] == 1]['tuition'], \n                                school_df[school_df['tablet'] == 0]['tuition'])\nprint(f'T: {t_stat}; p-value: {p_value}')\n\nT: 1.8772572008954862; p-value: 0.06656723616720626\n\n\n\n\n\n\n\nStudents\nNow we’ll create class_size students for each school. We first make a data frame of students, by simply repeating the school values for tuition and tablet:\n\nRPython\n\n\n\nstudent_df = data.frame(id = 1:(n_students),\n                        school_id = rep(school_df$id, each = class_size),\n                        tuition = rep(tuition, each = class_size),\n                        tablet = factor(rep(tablet, each = class_size)))\n\n\n\n\nstudent_df = pd.DataFrame({'id': np.arange(n_students),\n                           'school_id': np.repeat(school_df['id'], class_size),\n                           'tuition': np.repeat(school_df['tuition'], class_size),\n                           'tablet': np.repeat(school_df['tablet'], class_size)})\n\n\n\n\nNow we’ll create a test score for each student. This will again be random, but based on the tuition values of the school (to reflect that we expect students at higher funded schools to test better). Student scores will be taken from a random normal distribution with a s.d. of 200 and the mean given by \\(200 + 0.7 \\times \\mbox{tuition}\\). We’ll then rescale the scores so that the maximum is 1000.\n\nRPython\n\n\n\nstudent_df$enem_score0 = rnorm(n_students, 200 +\n                                0.7 * student_df$tuition, 200) \nstudent_df$enem_score0 =\n  (student_df$enem_score0 - min(student_df$enem_score0)) /\n  max(student_df$enem_score0) * 1000\n\n\n\n\nstudent_df['enem_score0'] = np.random.normal(200 + 0.7 * student_df['tuition'], 200, n_students) \n\nstudent_df['enem_score0'] = (student_df['enem_score0'] - student_df['enem_score0'].min()) / student_df['enem_score0'].max() * 1000.0\n\n\n\n\nNote that this score (enem_score0) is the factual for students who were not assigned a tablet, and the counterfactual for students who were.\nFinally, we’ll add a tablet effect. This is the expected change in a student’s score if they were assigned a tablet. For this exercise, we’ll assume that having a tablet reduces scores by 50 points on average, but with a s.d. of 5.\n\nRPython\n\n\n\nstudent_df$tablet_eff = rnorm(n_students, -50, 5)\ngghistogram(student_df, x = \"tablet_eff\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['tablet_eff'] = np.random.normal(-50, 5, n_students)\nsns.histplot(student_df, x = \"tablet_eff\")\n\n\n\n\n\n\n\n\n\n\n\nFinally, add the tablet effect back to the score. We multiply by the binary tablet assignation.\n\nRPython\n\n\n\nstudent_df$enem_score1 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet)-1\nggboxplot(student_df, \n          x = \"tablet\", \n          y = \"enem_score1\", \n          fill = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nstudent_df['enem_score1'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet']\n\nsns.boxplot(student_df, x = \"tablet\", y = \"enem_score1\", \n          hue = \"tablet\")"
  },
  {
    "objectID": "GEOG6960_Week2.html#first-test",
    "href": "GEOG6960_Week2.html#first-test",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "First test",
    "text": "First test\nWith the data in had, we can test for the causal effect of tablet on the observed scores enem_score1. Before running this, just a reminder of two points. Given the way we have created the data set, we know this is expected to be negative and around -50. But we also know that there is a bias in the tablet assignment from the tuition rates.\nAs the test scores are normally distributed, and we have two groups (treated and control), we can use a \\(t\\)-test to explore the differences (as shown above). More usefully, we replace this with a linear model (lm in R or statsmodels.OLS in Python), as this will allow us to test for significance and give us an estimate of the effect in the coefficient \\(\\beta_1\\):\n\\[\n\\mbox{enem\\_score} = \\beta_0 + \\beta_1 \\times \\mbox{tablet}\n\\]\n(As an aside, while they are often taught separately, most statistical tests are just special cases of the linear model…)\n\nRPython\n\n\n\nsummary(lm(enem_score1 ~ tablet, student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\nWhich gives us a highly significant effect of 77.64. Now you should be able to see the impact of the tuition bias: we expected an effect of around -50 and we got 77.64 instead.\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):              0.859\nTime:                        16:14:47   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nYou can also see this effect if you plot the test scores against tuition:\n\nRPython\n\n\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          col = \"tablet\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          hue = \"tablet\")\n\n\n\n\n\n\n\n\n\n\n\nWhere you’ll see both the influence of tuition and the asymmetric distribution of tablets."
  },
  {
    "objectID": "GEOG6960_Week2.html#randomized-trial",
    "href": "GEOG6960_Week2.html#randomized-trial",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Randomized trial",
    "text": "Randomized trial\nWe’ll now repeat this test, but by simulating a random trial of tablets across schools. To keep this comparable to the previous (biased) example, we’ll work with the same data. First we assign tablets randomly\n\nRPython\n\n\n\nschool_df$tablet_rct = as.factor(sample(rep(c(0,1), n_schools/2)))\n\nggbarplot(school_df, x = \"id\", y = \"tuition\",\n          fill = \"tablet_rct\",\n          palette = \"jco\",\n          sort.val = \"asc\",\n          sort.by.groups = FALSE,\n          x.text.angle = 45) \n\n\n\n\n\n\n\n\n\n\n\nschool_df['tablet_rct'] = school_df['tablet'].sample(n_schools).to_numpy()\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet_rct\", order=school_df.sort_values('tuition').id)\n\n\n\n\n\n\n\n\n\n\n\nNext we create a new set of test scores by updating the original scores (enem_score0) with tablet effect multiplied by the new tablet assignment. If we then repeat the scatter plot using the new scores and tablet assignments, you should see a more even distribution:\n\nRPython\n\n\n\nstudent_df$tablet_rct = rep(school_df$tablet_rct, each = class_size)\nstudent_df$enem_score2 = student_df$enem_score0 + \n  student_df$tablet_eff * as.numeric(student_df$tablet_rct)-1\n\nggscatter(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          col = \"tablet_rct\",\n          palette = \"jco\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nsummary(lm(enem_score2 ~ tablet_rct, student_df))\n\n\nCall:\nlm(formula = enem_score2 ~ tablet_rct, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-442.06 -115.71   -5.93  118.81  486.75 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  380.065      7.399  51.367  &lt; 2e-16 ***\ntablet_rct1  -33.930     10.464  -3.243  0.00122 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.4 on 998 degrees of freedom\nMultiple R-squared:  0.01043,   Adjusted R-squared:  0.009434 \nF-statistic: 10.51 on 1 and 998 DF,  p-value: 0.001223\n\n\n\n\n\nstudent_df['tablet_rct'] = np.repeat(school_df['tablet_rct'], class_size)\nstudent_df['enem_score2'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet_rct']\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          hue = \"tablet_rct\")\n\n\n\n\n\n\n\n\nAnd now if we repeat our linear model, we get an effect that is much closer to the expected value of -50.\n\nmod = smf.ols(formula='enem_score2 ~ tablet_rct', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score2   R-squared:                       0.037\nModel:                            OLS   Adj. R-squared:                  0.037\nMethod:                 Least Squares   F-statistic:                     38.86\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           6.71e-10\nTime:                        16:14:48   Log-Likelihood:                -6347.3\nNo. Observations:                1000   AIC:                         1.270e+04\nDf Residuals:                     998   BIC:                         1.271e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.5017      6.312     60.281      0.000     368.115     392.888\ntablet_rct   -54.5671      8.753     -6.234      0.000     -71.744     -37.390\n==============================================================================\nOmnibus:                        3.915   Durbin-Watson:                   1.067\nProb(Omnibus):                  0.141   Jarque-Bera (JB):                3.988\nSkew:                           0.145   Prob(JB):                        0.136\nKurtosis:                       2.892   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#propensity-score-matching",
    "href": "GEOG6960_Week2.html#propensity-score-matching",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Propensity score matching",
    "text": "Propensity score matching\nIn the previous sections, we looked at the effect of having a randomized or biased design in our data, and how this can impact the conclusions that we draw. But what do you do when you don’t have a randomized trial? In a lot of situations, we have natural experiments; where ‘treatments’ have taken place for other reasons than our tests. This is the case with the first set of test scores - these were created to mimic a natural experiment where schools had decided themselves (and partly based on finances) whether or not to give students tablets. In this case, we can use propensity score matching to try and reduce any biases.\nThe aim here is to create a subset of data with matched treated and control samples, where the confounding variables (e.g. tuition) are used to make the matches. The idea being that if we have treatments and controls for similar tuition levels, then the remaining difference in test scores should be due to the effect of the treatment (the tablets in our example).\nHere, we’ll look briefly at how propensity scores are calculated, then use an add-on package to calculate these for our dataset. Finally, we’ll re-run our model to test for tablet-related test score differences with the new, matched set.\nLet’s remind ourselves of the data we have available:\n\nRPython\n\n\n\nhead(school_df)\n\n  id tuition tuition_p tablet tablet_rct\n1  1    1084 0.5402727      1          1\n2  2     861 0.3714199      1          0\n3  3     936 0.4268085      0          1\n4  4    1620 0.8598539      1          0\n5  5    1428 0.7724307      1          1\n6  6     784 0.3178776      1          1\n\n\n\nhead(student_df)\n\n  id school_id tuition tablet enem_score0 tablet_eff enem_score1 tablet_rct\n1  1         1    1084      1    730.6342  -50.60501    628.4242          1\n2  2         1    1084      1    585.3498  -51.11708    482.1156          1\n3  3         1    1084      1    618.7099  -44.67392    528.3621          1\n4  4         1    1084      1    513.1673  -53.44279    405.2818          1\n5  5         1    1084      1    475.5372  -46.81305    380.9111          1\n6  6         1    1084      1    639.4113  -41.48421    555.4428          1\n  enem_score2\n1    628.4242\n2    482.1156\n3    528.3621\n4    405.2818\n5    380.9111\n6    555.4428\n\n\n\n\n\nschool_df.head()\n\n   id  tuition  tablet  tablet_rct\n0   0   1149.0       1           0\n1   1    959.0       0           1\n2   2   1194.0       1           0\n3   3   1457.0       0           0\n4   4    930.0       1           1\n\n\n\nstudent_df.head()\n\n   id  school_id  tuition  ...  enem_score1  tablet_rct  enem_score2\n0   0          0   1149.0  ...   345.201552           0   396.496762\n0   1          0   1149.0  ...   449.695558           0   500.677307\n0   2          0   1149.0  ...   408.749080           0   459.107087\n0   3          0   1149.0  ...   495.975591           0   546.161703\n0   4          0   1149.0  ...   332.925810           0   379.287662\n\n[5 rows x 9 columns]\n\n\n\n\n\nAlthough we are testing for the differences in students, the assignment (and therefore propensity) needs to be calculated for the schools, so we’ll use schools_df for the next steps. Note that propensity score usually works best with larger datasets, and is somewhat limited with only 50 samples.\nPropensity scores are simply the probability that a given observation was selected for the treatment. The important part is that we want to estimate these probabilities using the same covariate(s) that we think (or know) caused the bias in the treatment. We’ll estimate this here using binomial regression in a generalized linear model, but note you can use any model that works with a binary outcome (random forests, boosted trees, etc).\n\nRPython\n\n\nIn R, we can fit this model using glm and by setting the family to binomial:\n\nfit_ps &lt;- glm(tablet ~ tuition, school_df, family = binomial())\nsummary(fit_ps)\n\n\nCall:\nglm(formula = tablet ~ tuition, family = binomial(), data = school_df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -4.114829   1.321285  -3.114  0.00184 **\ntuition      0.004234   0.001294   3.273  0.00106 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 68.994  on 49  degrees of freedom\nResidual deviance: 53.442  on 48  degrees of freedom\nAIC: 57.442\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df &lt;- data.frame(prop_score = predict(fit_ps, type = \"response\"),\n                     tablet = as.numeric(fit_ps$model$tablet)-1,\n                     tuition = fit_ps$model$tuition)\nhead(prs_df)\n\n  prop_score tablet tuition\n1  0.6165972      1    1084\n2  0.3848259      1     861\n3  0.4621865      0     936\n4  0.9396136      1    1620\n5  0.8734399      1    1428\n6  0.3110631      1     784\n\n\n\n\nIn Python, we can fit this model using the glm function from statsmodels and by setting the family to binomial:\n\nmod = smf.glm(formula='tablet ~ tuition', data=school_df, family=sm.families.Binomial())\nfit = mod.fit()\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 tablet   No. Observations:                   50\nModel:                            GLM   Df Residuals:                       48\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -32.853\nDate:                Thu, 05 Sep 2024   Deviance:                       65.706\nTime:                        16:14:48   Pearson chi2:                     50.1\nNo. Iterations:                     4   Pseudo R-squ. (CS):            0.06814\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.7868      1.077     -1.659      0.097      -3.897       0.324\ntuition        0.0020      0.001      1.794      0.073      -0.000       0.004\n==============================================================================\n\n\nWe can now extract the estimated propensity scores into a new data.frame\n\nprs_df = pd.DataFrame({'prop_score': fit.predict(),\n                        'tablet': school_df['tablet'],\n                        'tuition': school_df['tuition']})\nprs_df.head()\n\n   prop_score  tablet  tuition\n0    0.627858       1   1149.0\n1    0.535210       0    959.0\n2    0.648739       1   1194.0\n3    0.758087       0   1457.0\n4    0.520682       1    930.0\n\n\n\n\n\nTo illustrate how a simple match would happen, let’s split this into a treatment and control data set:\n\nRPython\n\n\n\ntreated_df = prs_df %&gt;%\n  filter(tablet == 1)\ncontrol_df = prs_df %&gt;%\n  filter(tablet == 0)\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nmatch_id = which.min(abs(treated_df$prop_score[1] - control_df$prop_score))\nmatch_id\n\n[1] 5\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df[match_id, ]\n\n   prop_score tablet tuition\n15  0.6175977      0    1085\n\n\n\n\n\ntreated_df = prs_df[prs_df['tablet'] == 1].reset_index()\ncontrol_df = prs_df[prs_df['tablet'] == 0].reset_index()\n\nThen, for the first sample, we can estimate the differences in propensity score and find the closest match:\n\nabs_diff = (treated_df['prop_score'][0] - control_df['prop_score']).abs()\nmatch_id = abs_diff.idxmin()\nprint(match_id)\n\n4\n\n\nAnd show the matching sample (the tuition should be similar to the first treated sample):\n\ncontrol_df.iloc[match_id,:]\n\nindex            9.00000\nprop_score       0.63441\ntablet           0.00000\ntuition       1163.00000\nName: 4, dtype: float64\n\n\n\n\n\nWe could obviously make this into a loop and get all the matches, but instead we’ll use an external package to carry out the full match.\n\nRPython\n\n\nIn R, the package we will use is called MatchIt. It is pretty well established and allows you to choose different method to calculate the scores and carry out matching.\nTo get an idea of the output, we’ll first run this with no matching (method = NULL). The output will show some summary statistics on the match between the treatment and control. The first line (distance) shows the difference in propensity score between the two groups and the second (and subsequent) line shows the difference in the covariate. A useful index is the standardized mean difference, which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\nmatch0 = matchit(tablet ~ tuition, data = school_df,\n                 method = NULL, distance = \"glm\")\nsummary(match0)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = NULL, \n    distance = \"glm\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.3944          1.0838     1.4057    0.2842\ntuition       1187.259      848.9565          1.0929     1.6977    0.2842\n         eCDF Max\ndistance   0.5427\ntuition    0.5427\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      27\nUnmatched       0       0\nDiscarded       0       0\n\n\nNow, we’ll re-run and use nearest neighbor matching to selected control schools.\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\")\n\nWarning: Fewer control units than treated units; not all treated units will get\na match.\n\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\")\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.7356        0.3944          1.3717     0.8313    0.3617\ntuition      1269.9130      848.9565          1.3599     1.1421    0.3617\n         eCDF Max Std. Pair Dist.\ndistance   0.6522          1.3717\ntuition    0.6522          1.3599\n\nSample Sizes:\n          Control Treated\nAll            23      27\nMatched        23      23\nUnmatched       0       4\nDiscarded       0       0\n\n\nThe results here are slightly worse (the std. differences have increased). This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\nmatch1 = matchit(tablet ~ tuition, data = school_df,\n                 method = \"nearest\", distance = \"glm\", \n                 replace = TRUE)\nsummary(match1, un = FALSE)\n\n\nCall:\nmatchit(formula = tablet ~ tuition, data = school_df, method = \"nearest\", \n    distance = \"glm\", replace = TRUE)\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.664        0.6353          0.1156     0.9800    0.0585\ntuition       1187.259     1123.9259          0.2046     1.2934    0.0585\n         eCDF Max Std. Pair Dist.\ndistance   0.3704          0.1641\ntuition    0.3704          0.2549\n\nSample Sizes:\n              Control Treated\nAll              23.       27\nMatched (ESS)     3.9      27\nMatched           8.       27\nUnmatched        15.        0\nDiscarded         0.        0\n\n\nNow we obtain a better match as shown by the decrease in std. differences. Note in the sample sizes that the total number of retained control samples is only 8, which is probably too low in practice.\nWe can see the results of the match using the plot() function. For example, this shows the histograms of treated (top) and control (bottom), before (left) and after (right) matching.\n\nplot(match1, type = \"hist\", interactive = FALSE)\n\n\n\n\n\n\n\n\nAnd this shows the same for the empirical cumulative distribution functions:\n\nplot(match1, type = \"ecdf\", interactive = FALSE)\n\n\n\n\n\n\n\n\nWe can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools. First extract the match ‘ids’ (the rows from the original school_df data frame)\n\nmatch_df = get_matches(match1, id = \"mid\")\n\nNow we can loop across these and create a new data frame by appending the students from each matched school in turn:\n\nmatch_student_df = NULL\nfor (i in 1:nrow(match_df)) {\n  tmp_df = student_df %&gt;%\n    filter(school_id == as.numeric(match_df$mid)[i])\n  match_student_df = rbind(match_student_df, tmp_df)\n}\n\nAnd finally, we can repeat our test:\n\nsummary(lm(enem_score1 ~ tablet, match_student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = match_student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -109.72    7.64   95.34  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  427.098      6.451  66.212  &lt; 2e-16 ***\ntablet1      -30.184      9.122  -3.309 0.000968 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 149.9 on 1078 degrees of freedom\nMultiple R-squared:  0.01005,   Adjusted R-squared:  0.009135 \nF-statistic: 10.95 on 1 and 1078 DF,  p-value: 0.0009681\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!\n\n\nIn Python, the package we will use is called psmpy. It is a solid and fairly widely used package, but doesn’t offer quite the same flexibility as R. The main function is PsmPy, and uses a similar format to SciKit-Learn, where methods are initialized then fit to the data. We need to specify:\n\nThe data frame holding the data\nThe treatment (this is the tablet variable)\nA column with observation IDs (these will be used in matching)\nAny varaibles that we want to exclude from the propensity score estimates\n\n\nfrom psmpy import PsmPy\npsm = PsmPy(school_df, treatment='tablet', indx='id', exclude = ['tablet_rct'])\n\nOnce we’ve set this up, we can calculate the propensity score using a binomial (logisitic) model as follows. THe resulting dataframe contains the propensity scores on both a probability and logit scale:\n\npsm.logistic_ps(balance = True)\npsm.predicted_data.head()\n\n   id  tuition  propensity_score  propensity_logit  tablet\n0   0   1149.0          0.582970          0.334979       1\n1   2   1194.0          0.595662          0.387423       1\n2   4    930.0          0.519927          0.079750       1\n3   6   1474.0          0.671227          0.713738       1\n4   8    859.0          0.499251         -0.002996       1\n\n\nOnce this is run, we can use the results to carry out nearest neighbor matching to selected control schools.\n\npsm.knn_matched(matcher='propensity_logit', replacement=False, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\nWe can explore the matches. First, we can plot a histogram of the matched propensity scores. Ideally, these histograms would roughly match, but there is still quite a lot of visible differences\n\npsm.plot_match()\n\n\n\n\n\n\n\n\nA useful index is the standardized mean difference (called the effect_size), which allows you to compare difference covariates (if you have them). The goal of matching will be to reduce this difference.\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\n\npsm.effect_size\n\n  Variable matching  Effect Size\n0  tuition   before     0.531394\n1  tuition    after     0.370122\n\n\nThis shows that we have reduced the difference (the after effect_size is lower), but it remains fairly high. This is due to the sequential nature of the method used, where the first treated sample is matched to the closest control. This control is then excluded from subsequent matches, even if they are better. We’ll re-run using replacement matching (where each control can be matched to multiple treated samples):\n\npsm.knn_matched(matcher='propensity_logit', replacement=True, caliper=None)\n\n/opt/homebrew/Caskroom/miniforge/base/envs/causal/lib/python3.12/site-packages/psmpy/psmpy.py:363: UserWarning: Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False\n  warnings.warn('Some values do not have a match. These are dropped for purposes of establishing a matched dataframe, and subsequent calculations and plots (effect size). If you do not wish this to be the case please set drop_unmatched=False')\n\n\n\npsm.effect_size_plot()\n\n\n\n\n\n\n\n\nNow we obtain a better match as shown by the decrease in effect size. Another useful diagnostic is to plot values of covariates for the matched treated and control samples as histograms:\n\nfig, axs = plt.subplots(ncols=2)\nsns.histplot(school_df, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[0]).set(title='Before')\nsns.histplot(psm.df_matched, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nOr as empirical cumulative distribution functions:\n\nfig, axs = plt.subplots(ncols=2)\nsns.ecdfplot(school_df, x = \"tuition\", hue=\"tablet\", ax=axs[0]).set(title='Before')\nsns.ecdfplot(psm.df_matched, x = \"tuition\", hue=\"tablet\", ax=axs[1]).set(title='After')\n\n\n\n\n\n\n\n\nAs these now align pretty well after the matching, we can now repeat our test for the effect of the tablets on test scores, but using the matched samples. As we’ve matched the schools, we now need to create a new dataset that includes only the students from these schools.\n\nmatch_df = psm.df_matched\nmatched_student_df = pd.DataFrame(columns=student_df.columns)\nfor idx, row in match_df.iterrows():\n    #print(row['id'])\n    tmp_df = student_df[student_df['school_id'] == row['id']]\n    matched_student_df = pd.concat([matched_student_df, tmp_df], ignore_index = True)\n\n&lt;string&gt;:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n    \nmatched_student_df.head()\n\n   id school_id  tuition  ... enem_score1  tablet_rct  enem_score2\n0  20         1    959.0  ...  408.713490           1   355.594645\n1  21         1    959.0  ...  375.552762           1   335.122918\n2  22         1    959.0  ...  191.336345           1   140.382933\n3  23         1    959.0  ...  380.340349           1   331.427514\n4  24         1    959.0  ...  389.004593           1   343.354932\n\n[5 rows x 9 columns]\n\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=matched_student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.020\nModel:                            OLS   Adj. R-squared:                  0.019\nMethod:                 Least Squares   F-statistic:                     19.11\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           1.37e-05\nTime:                        16:14:53   Log-Likelihood:                -5941.3\nNo. Observations:                 940   AIC:                         1.189e+04\nDf Residuals:                     938   BIC:                         1.190e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept     351.4540      6.146     57.187      0.000     339.393     363.515\ntablet[T.1]   -38.4086      8.785     -4.372      0.000     -55.650     -21.167\n==============================================================================\nOmnibus:                       13.303   Durbin-Watson:                   1.085\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               13.639\nSkew:                           0.295   Prob(JB):                      0.00109\nKurtosis:                       2.992   Cond. No.                         2.59\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWhich shows a similar results to the simulated randomized control above, despite being based on the data set where we know tuition has biased the assignment of tablets!"
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "href": "GEOG6960_Week2.html#inverse-propensity-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse propensity weighting",
    "text": "Inverse propensity weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not recieve a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):           1.18e-08\nTime:                        18:12:11   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Sun, 25 Aug 2024   Prob (F-statistic):              0.859\nTime:                        18:12:11   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "GEOG6960_Week2.html#inverse-probability-weighting",
    "href": "GEOG6960_Week2.html#inverse-probability-weighting",
    "title": "GEOG 6960 Causality in Geog. Studies 2",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nAn alternative approach to working with propensity scores is to use them directly in the test for the causal effect.\nThe scores can be used as weights to indicate that some observations are more important than others for estimating the causal effect. For our example, students with a low likelihood of receiving a tablet who do get one have higher weights that students who follow expectations. Similarly, students with a high likelihood who do not receive a tablet also get higher weights.\nWhy does this work? When we have bias, it indicates that the treatment is not equally (or randomly) distributed across a covariate. This weighting has the effect of making this distribution more equal, removing (or at least reducing) the bias in any test.\nThere are several ways to calculate these weights, but a simple one is:\n\\[\nW_i = \\frac{T_i}{p_i}+ \\frac{1 - T_i}{1 - p_i}\n\\]\n\nRPython\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df &lt;- prs_df %&gt;%\n  mutate(ipw = (tablet / prop_score) + ((1 - tablet) / (1 - prop_score)))\n\nThen assign the relevant weight to each student:\n\nstudent_df$ipw = rep(prs_df$ipw, each = class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df, \n   weights = student_df$ipw))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df, weights = student_df$ipw)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-793.34 -125.67   20.24  162.97  834.34 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  357.057      7.254  49.219  &lt; 2e-16 ***\ntablet1      -26.240      9.970  -2.632  0.00862 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 217.2 on 998 degrees of freedom\nMultiple R-squared:  0.006893,  Adjusted R-squared:  0.005898 \nF-statistic: 6.927 on 1 and 998 DF,  p-value: 0.008623\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nsummary(lm(enem_score1 ~ tablet, \n   data = student_df))\n\n\nCall:\nlm(formula = enem_score1 ~ tablet, data = student_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-432.16 -106.04    1.31  111.37  419.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  319.275      7.130  44.779  &lt; 2e-16 ***\ntablet1       77.639      9.703   8.002 3.39e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 152.9 on 998 degrees of freedom\nMultiple R-squared:  0.06029,   Adjusted R-squared:  0.05935 \nF-statistic: 64.03 on 1 and 998 DF,  p-value: 3.385e-15\n\n\n\n\nIn our example, we need to first calculate this for each school:\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\nThen assign the relevant weight to each student:\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\nAnd finally, we can re-run the linear model with the weights incorporated (remember that the tablet effect was \\(\\sim 50\\):\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n                            WLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.032\nModel:                            WLS   Adj. R-squared:                  0.031\nMethod:                 Least Squares   F-statistic:                     33.07\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):           1.18e-08\nTime:                        16:14:53   Log-Likelihood:                -6366.9\nNo. Observations:                1000   AIC:                         1.274e+04\nDf Residuals:                     998   BIC:                         1.275e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    380.0499      6.187     61.427      0.000     367.909     392.191\ntablet       -50.3483      8.755     -5.751      0.000     -67.528     -33.168\n==============================================================================\nOmnibus:                        9.692   Durbin-Watson:                   1.075\nProb(Omnibus):                  0.008   Jarque-Bera (JB):               10.057\nSkew:                           0.199   Prob(JB):                      0.00655\nKurtosis:                       3.287   Cond. No.                         2.62\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd just as comparison, here are the unweighted results for the same data set\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            enem_score1   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                   0.03153\nDate:                Thu, 05 Sep 2024   Prob (F-statistic):              0.859\nTime:                        16:14:53   Log-Likelihood:                -6329.6\nNo. Observations:                1000   AIC:                         1.266e+04\nDf Residuals:                     998   BIC:                         1.267e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    351.4540      6.201     56.674      0.000     339.285     363.623\ntablet         1.5269      8.600      0.178      0.859     -15.349      18.402\n==============================================================================\nOmnibus:                        4.240   Durbin-Watson:                   1.102\nProb(Omnibus):                  0.120   Jarque-Bera (JB):                4.314\nSkew:                           0.151   Prob(JB):                        0.116\nKurtosis:                       2.888   Cond. No.                         2.67\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "99_html_index.html",
    "href": "99_html_index.html",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "",
    "text": "Often in scientific studies we are interested in establishing a cause and effect, e.g. what is the effect of some policy on health outcomes or the effect of anthropogenic activity on a changing climate. However, most statistical texts and courses are taught using correlative methods, with the mantra that “correlation is not causation” outside of certain strict experimental conditions. Recent work by Judea Pearl and others have developed a framework (Structural Causal Modeling) for causal inference that allows causality to be inferred even when these conditions are not met, allowing this approach in a much broader range of studies. In this seminar, we will review the history and concepts of causal analysis, and go through the steps of Pearl’s causal framework using a set of hands-on examples. The class will largely follow the outline of The Book of Why (Pearl and Mackenzie, 2018; Basic Books, NY). Students will develop their own analysis over the course of the semester through a series of discussions and presentations."
  },
  {
    "objectID": "99_html_index.html#footnotes",
    "href": "99_html_index.html#footnotes",
    "title": "GEOG 6960 Causality in Geographic Studies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎\nUniversity of Utah, simon.brewer@ess.utah.edu↩︎"
  },
  {
    "objectID": "GEOG6960_Week3.html",
    "href": "GEOG6960_Week3.html",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week3.html#introduction",
    "href": "GEOG6960_Week3.html#introduction",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "",
    "text": "In this lab, we’re going to explore what a randomized control trial (RCT) looks like, and the use of propensity score matching to replicate the type of randomization seen in RCTs.\nAs a reminder, the goal of causal inference is to remove any bias related to the treatment: the covariate we are interested in. This is usually expressed as a confounder : one or more additional covariates (\\(X\\)) that affect both the treatment (\\(T\\)) and the outcome (\\(Y\\)). RCTs avoid this problem by trying to ensure that the assignation of \\(T\\) is random relative to \\(X\\). If this is true, then the causal effect (the thing we’re actually interested in) can usually be estimated using simple statistics (\\(t\\)-tests, linear models)."
  },
  {
    "objectID": "GEOG6960_Week3.html#packages",
    "href": "GEOG6960_Week3.html#packages",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Packages",
    "text": "Packages\n\nRPython\n\n\nWe’ll be using the following R packages, so make sure they are installed and then load them:\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(ggsci)\nlibrary(sjPlot)\n\n\n\nWe’ll be using the following Python packages, so install these using your favorite package manage (pip, conda) and import them:\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf"
  },
  {
    "objectID": "GEOG6960_Week3.html#interrupted-time-series",
    "href": "GEOG6960_Week3.html#interrupted-time-series",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Interrupted Time Series",
    "text": "Interrupted Time Series\nInterrupted time series models assess the causal effect of an intervention or treatment by examining changes in the trend of an outcome (\\(Y\\)) before and after the start of the treatment. This is quite widely used in social and economic settings, where often only one set of data can be observed (e.g. tracking GDP before and after the implementation of a fiscal policy).\nThe format for the ITS model is\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 P\n\\]\nWhere:\n\n\\(Y\\) is the outcome\n\\(T\\) is time\n\\(D\\) is a binary indicator (pre vs. post treatment)\n\\(P\\) is a index of time since treatment\n\n\nSimulated data\nFirst, we’re going to create a synthetic dataset, which will include the impact of a treatment. The time series will include a base trend, which will then be modified after the start of the intervention.\nFirst, we’ll set a random seed to make the results repeatable. As before, try changing this to see how the noise we will add changes the reuslts.\n\nRPython\n\n\n\nset.seed(42)\n\n\n\n\nnp.random.seed(42)\n\n\n\n\nNow we’ll simulate the data. The data will represent student outcomes over a full year (365 days), and we’ll use the following equation to represent the base trend (\\(Y\\) is the outcome, \\(T\\) is the time in days). This will give a starting value of 5.4 and an upward trend of 0.5 per day:\n\\[\nY = 5.4 + 0.5 \\times T\n\\] To this we’ll add the following effects: - An immediate effect of the policy change of +20 points - A change in the slope of +1.2\nTo include these, we need to make the two vectors (\\(D\\) and \\(P\\)). With this, the equation to generate the data is:\n\\[\nY = 5.4 + 0.5 \\times T + 20 \\times D + 1.2 \\times P\n\\]\nFinally, we’ll add some noise to the trends to represent individual daily variation (\\(N(0, 50)\\)).\n\nRPython\n\n\nFirst generate the basic equation:\n\nT = rep(1:365)\nD = ifelse(T &gt; 200, 1, 0)\nP = ifelse(T &lt;= 200, 0, rep(1:200))\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\n\nNow add errors and combine everything into a data frame:\n\nerr = rnorm(365, 0, 50)\nY = Y + err\nwell_df &lt;- as.data.frame(cbind(Y, T, D, P)) \n\nAnd finally plot it:\n\nggplot(well_df, aes(x = T, y = Y)) + \ngeom_point(size = 3, alpha = 0.5) +\ngeom_vline(xintercept = 201) +\ntheme_bw() +\ntheme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\nFirst generate the basic equation:\n\nT = np.arange(365)\nD = np.where(T &gt; 200, 1, 0)\nP = T - 200\nP = np.where(T &lt;= 200, 0, P)\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\n\nNow add errors and combine everything into a data frame:\n\nerr = np.random.normal(0, 50, 365)\nY = Y + err\n\nwell_df = pd.DataFrame({'Y': Y,\n                        'T': T,\n                        'D': D,\n                        'P': P})\n\nAnd finally plot it:\n\nsns.scatterplot(well_df,\n                x = \"T\",\n                y = \"Y\", \n                alpha = 0.75)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple model\nBefore fitting the ITS model, we’ll fit a simple trend model of the outcome over time. In this case, this we’ll just use a simple OLS model.\n\nRPython\n\n\n\nfit0 &lt;- lm(Y ~ T, well_df)\ntab_model(fit0)\n\n\n\n\n\n \nY\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-49.30\n-61.27 – -37.32\n&lt;0.001\n\n\nT\n1.08\n1.03 – 1.14\n&lt;0.001\n\n\nObservations\n365\n\n\nR2 / R2 adjusted\n0.796 / 0.795\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Y ~ T', data=well_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.817\nModel:                            OLS   Adj. R-squared:                  0.816\nMethod:                 Least Squares   F-statistic:                     1618.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          8.38e-136\nTime:                        17:06:37   Log-Likelihood:                -1986.9\nNo. Observations:                 365   AIC:                             3978.\nDf Residuals:                     363   BIC:                             3986.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -53.6683      5.862     -9.155      0.000     -65.196     -42.141\nT              1.1211      0.028     40.218      0.000       1.066       1.176\n==============================================================================\nOmnibus:                        0.840   Durbin-Watson:                   1.519\nProb(Omnibus):                  0.657   Jarque-Bera (JB):                0.939\nSkew:                          -0.068   Prob(JB):                        0.625\nKurtosis:                       2.792   Cond. No.                         420.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nNote that the slope we obtain here falls somewhere between the baseline trend (0.5) and the post-treatment trend (0.5 + 1.2), as we have not accounted for this effect as a separate term in the model.\nNow we’ll fit the full ITS model. As a reminder, this extends the basic OLS model by including the two additional vectors described above.\n\n\nITS model\n\nRPython\n\n\n\nfit1 &lt;- lm(Y ~ T + D + P, well_df)\ntab_model(fit1)\n\n\n\n\n\n \nY\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n7.64\n-5.73 – 21.02\n0.262\n\n\nT\n0.46\n0.35 – 0.58\n&lt;0.001\n\n\nD\n25.71\n5.87 – 45.54\n0.011\n\n\nP\n1.20\n1.01 – 1.39\n&lt;0.001\n\n\nObservations\n365\n\n\nR2 / R2 adjusted\n0.862 / 0.860\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Y ~ T + D + P', data=well_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.870\nModel:                            OLS   Adj. R-squared:                  0.869\nMethod:                 Least Squares   F-statistic:                     803.5\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          2.35e-159\nTime:                        17:06:37   Log-Likelihood:                -1924.6\nNo. Observations:                 365   AIC:                             3857.\nDf Residuals:                     361   BIC:                             3873.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4776      6.667     -0.372      0.710     -15.588      10.632\nT              0.5594      0.058      9.701      0.000       0.446       0.673\nD             22.9193      9.991      2.294      0.022       3.271      42.568\nP              1.0990      0.097     11.308      0.000       0.908       1.290\n==============================================================================\nOmnibus:                        4.812   Durbin-Watson:                   2.136\nProb(Omnibus):                  0.090   Jarque-Bera (JB):                5.520\nSkew:                           0.134   Prob(JB):                       0.0633\nKurtosis:                       3.539   Cond. No.                         908.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThe values we used when generating the data should now be a lot closer to the model coefficients (or at least within the confidence intervals).\nOne of the advantages of fitting these models in standard statistical frameworks (like OLS) is that we can use other diagnostics tools. For example, we can use ANOVA to compare the two model, to see if the additional complexity of the ITS model is worthwhile:\n\nRPython\n\n\n\nanova(fit0, fit1)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ T\nModel 2: Y ~ T + D + P\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    363 1222981                                  \n2    361  828505  2    394476 85.941 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(fit0, fit1)\n\n   df_resid           ssr  df_diff       ss_diff          F        Pr(&gt;F)\n0     363.0  1.142920e+06      0.0           NaN        NaN           NaN\n1     361.0  8.122294e+05      2.0  330690.71979  73.488689  1.679349e-27\n\n\n\n\n\nThe low \\(p\\)-value indicates that the more complex ITS model provides a better fit.\nLet’s now use this to visualize the model. First create a new data set to predict for, the plot the results:\n\nRPython\n\n\n\nwell_df$yhat &lt;- predict(fit1)\nhead(well_df)\n\n           Y T D P      yhat\n1  74.447922 1 0 0  8.108687\n2 -21.834909 2 0 0  8.572678\n3  25.056421 3 0 0  9.036669\n4  39.043130 4 0 0  9.500660\n5  28.113416 5 0 0  9.964650\n6   3.093774 6 0 0 10.428641\n\n\n\nggplot(well_df, aes(x = T)) + \n  geom_point(aes(y = Y), size = 3, alpha = 0.5) +\n  geom_line(data = well_df, aes(y = yhat), size = 2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nwell_df['yhat'] = fit1.predict()\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(well_df, x = \"T\", y = \"yhat\",ax=ax,\n            color=\"darkorange\", linewidth=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounterfactual\nWe can use the coefficients from the ITS model to calculate the counterfactual for the post-treatment period. The estimation of this is simple - we just set the values of \\(D\\) and \\(P\\) to zero (rather than the value we set above). In the following code, we first extract the model coefficients, then use these to estimate the factual and counterfactual for 20 days post-treatment.\n\nRPython\n\n\n\nb0 = coef(fit1)[1]\nb1 = coef(fit1)[2]\nb2 = coef(fit1)[3]\nb3 = coef(fit1)[4]\n\nFactual:\n\npost_time &lt;- 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\n(Intercept) \n   159.3938 \n\n\nCounterfactual:\n\nb0 + b1 * (200 + post_time) \n\n(Intercept) \n   109.7227 \n\n\n\n\n\nb0 = fit1.params['Intercept']\nb1 = fit1.params['T']\nb2 = fit1.params['D']\nb3 = fit1.params['P']\n\nFactual:\n\npost_time = 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\nnp.float64(165.4855648977787)\n\n\nCounterfactual:\n\nb0 + b1 * (200 + post_time)\n\nnp.float64(120.58648318541084)\n\n\n\n\n\nWhich should give you a difference of around +40 from the counterfactual. We can also predict these across a range of values, and compare with the factual values to show the effect over time. To do this we need to make a data frame that contains the values of the model variables (T, D, P) for both conditions. For the factual, we just use the values we created earlier. For the counterfactual, we repeat the time variable (T), but set both D and P to zero.\n\nRPython\n\n\n\npred_df &lt;- data.frame(T = rep(T, 2),\n                      D = c(D, rep(0, length(P))),\n                      P = c(P, rep(0, length(P))))\npred_df$yhat &lt;- predict(fit1, newdata = pred_df)\npred_df$D &lt;- as.factor(pred_df$D)\n\n\nggplot(well_df, aes(x = T)) + \n  geom_point(aes(y = Y), size = 3, alpha = 0.5) +\n  geom_line(data = pred_df, aes(y = yhat, col = D), size = 2) +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n\nT_pred = np.concatenate([T, T])\nD_pred = np.concatenate([D, np.repeat(0, len(D))])\nP_pred = np.concatenate([P, np.repeat(0, len(P))])\npred_df = pd.DataFrame({'T': T_pred,\n                        'D': D_pred,\n                        'P': P_pred\n})\n\n\npred_df['yhat'] = fit1.predict(pred_df)\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(pred_df, x = \"T\", y = \"yhat\", ax=ax, hue = \"D\",\n            linewidth=5)"
  },
  {
    "objectID": "GEOG6960_Week3.html#difference-in-differences",
    "href": "GEOG6960_Week3.html#difference-in-differences",
    "title": "GEOG 6960 Causality in Geog. Studies 3",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nDifference-in-difference models are an alternative approach to testing causality with time series data. These improve on the ITS approach by testing for changes in time and comparing these to any change in a control time series.\nThe base model for DID is:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 D\\times T\n\\]\nWhere:\n\n\\(Y\\) is the outcome\n\\(T\\) is time\n\\(D\\) is a binary indicator (control vs. treatment)\n\\(D \\times T\\) is the interaction between \\(T\\) and \\(D\\) and represents the quantity we’re interested in (i.e. the change in slope in the treated group)\n\n\nSimulated data\nAs before, we’ll start by creating a synthetic dataset. This will represent house prices for two locations. Unlike the previous example, where we had observations for multiple time steps, here we’ll just have value pre (0) and post (1) treatment. The treatment here represents the installation of subsidized housing between the two time steps, and the outcome of interest is house prices.\nTo start, we create two vectors of of 1000 binary values representing pre and post treatment (i.e. time) and control (0) or treated (1). We then estimate a house price for each of these using the following equation:\n\\[\nPrice = 50000 + 5000 \\times Treat + 43000 \\times Time +\n10000 \\times Treat \\times Time\n\\]\nThis means that: - Prices for control houses before the treatment are $50K - Prices for treated houses before the treatment are $50K + $5K = $55K - Prices for control houses increase by $43K after the treatment - Prices for treated houses increase by an additional $10K after the treatment\nFinally, we’ll add some noise to represent house-scale variability (\\(N(0, 10000)\\)).\n\nRPython\n\n\n\nTime = rep(c(0,1), 500)\n\nTreat = rep(c(0,0,1,1), 250)\n\ny = 50000 + 5000 * Treat + 43000 * Time + \n  10000 * Treat * Time\n\ne = rnorm(1000, 0, 10000)\ny = y + e\n\nAdd to data frame\n\nhouse_df = data.frame(Price = y,\n  Treat = as.factor(Treat),\n  Time = as.factor(Time))\n\nAnd plot:\n\nggline(house_df, x = \"Time\", y = \"Price\",\n       add = c(\"mean_se\", \"jitter\"), \n       color = \"Treat\", palette = \"jco\") \n\n\n\n\n\n\n\n\n\n\n\nTime = np.resize([0,1], 1000)\nTreat = np.resize([0,0,1,1], 1000)\n\ny = 50000 + 5000 * Treat + 43000 * Time + 10000 * Treat * Time\n\ne = np.random.normal(0, 10000, 1000)\ny = y + e\n\nAdd to data frame\n\nhouse_df = pd.DataFrame({'Price': y,\n                         'Treat': Treat,\n                         'Time': Time\n                         })\n\nAnd plot:\n\nfig, ax = plt.subplots()\nsns.stripplot(house_df, x = 'Time', y = 'Price', hue = 'Treat', alpha = 0.25)\nsns.pointplot(house_df, x = 'Time', y = 'Price', hue = 'Treat')\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple model\nAs before, we’ll start with simple OLS model, with the prices as a function of treatment and time. We’ll exclude the DID effect here, which makes the model:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D\n\\]\n\nRPython\n\n\n\nfit0 &lt;- lm(Price ~ Time + Treat, house_df)\ntab_model(fit0)\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n47048.67\n45919.81 – 48177.54\n&lt;0.001\n\n\nTime [1]\n48215.54\n46912.03 – 49519.04\n&lt;0.001\n\n\nTreat [1]\n10264.85\n8961.35 – 11568.36\n&lt;0.001\n\n\nObservations\n1000\n\n\nR2 / R2 adjusted\n0.847 / 0.846\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat', data=house_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.850\nMethod:                 Least Squares   F-statistic:                     2839.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):               0.00\nTime:                        17:06:40   Log-Likelihood:                -10658.\nNo. Observations:                1000   AIC:                         2.132e+04\nDf Residuals:                     997   BIC:                         2.134e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    4.78e+04    564.623     84.657      0.000    4.67e+04    4.89e+04\nTime        4.807e+04    651.970     73.733      0.000    4.68e+04    4.94e+04\nTreat       1.012e+04    651.970     15.525      0.000    8842.575    1.14e+04\n==============================================================================\nOmnibus:                        1.102   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.576   Jarque-Bera (JB):                1.153\nSkew:                           0.039   Prob(JB):                        0.562\nKurtosis:                       2.853   Cond. No.                         3.19\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nWe get a pretty good model, but note that neither of the coefficients match the expected values from our simulated data (e.g. the effect of time is much larger than the base effect). Again (and I’m sure you’ve already understood this), this is because the model is merging the effects of time for the two groups together.\n\n\nDID model\nLet’s now fit the DID model to see if we get the expected coefficients. To do this, we simply need to add the interaction between Time and Treat to the model:\n\nRPython\n\n\n\nfit1 &lt;- lm(Price ~ Time * Treat, house_df)\ntab_model(fit1)\n\n\n\n\n\n \nPrice\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n49804.99\n48546.68 – 51063.29\n&lt;0.001\n\n\nTime [1]\n42702.91\n40923.39 – 44482.42\n&lt;0.001\n\n\nTreat [1]\n4752.23\n2972.71 – 6531.74\n&lt;0.001\n\n\nTime [1] × Treat [1]\n11025.25\n8508.64 – 13541.87\n&lt;0.001\n\n\nObservations\n1000\n\n\nR2 / R2 adjusted\n0.857 / 0.857\n\n\n\n\n\n\n\n\n\n\n\nmod = smf.ols(formula='Price ~ Time + Treat + Time:Treat', data=house_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.859\nModel:                            OLS   Adj. R-squared:                  0.859\nMethod:                 Least Squares   F-statistic:                     2022.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):               0.00\nTime:                        17:06:40   Log-Likelihood:                -10629.\nNo. Observations:                1000   AIC:                         2.127e+04\nDf Residuals:                     996   BIC:                         2.129e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   5.023e+04    633.857     79.244      0.000     4.9e+04    5.15e+04\nTime        4.321e+04    896.410     48.205      0.000    4.15e+04     4.5e+04\nTreat       5261.6990    896.410      5.870      0.000    3502.631    7020.767\nTime:Treat  9720.5345   1267.715      7.668      0.000    7232.836    1.22e+04\n==============================================================================\nOmnibus:                        0.666   Durbin-Watson:                   1.996\nProb(Omnibus):                  0.717   Jarque-Bera (JB):                0.746\nSkew:                           0.025   Prob(JB):                        0.689\nKurtosis:                       2.876   Cond. No.                         6.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAnd the results should be a much better match, with the model coefficients comparable to the values with used in creating the data. NB: of all the results here, the most important is the coefficient on the Time:Treat interaction. This is the casual effect in this model: the impact on house prices due to the addition of subsidized housing.\nAs in the previous section, we can also compare the two models with ANOVA, to see if including the DID term is helpful\n\nRPython\n\n\n\nanova(fit0, fit1)\n\nAnalysis of Variance Table\n\nModel 1: Price ~ Time + Treat\nModel 2: Price ~ Time * Treat\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1    997 1.0998e+11                                   \n2    996 1.0238e+11  1 7597261695 73.909 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(fit0, fit1)\n\n   df_resid           ssr  df_diff       ss_diff          F        Pr(&gt;F)\n0     997.0  1.059475e+11      0.0           NaN        NaN           NaN\n1     996.0  1.000420e+11      1.0  5.905549e+09  58.794581  4.150257e-14\n\n\n\n\n\nAnd again, the low \\(p\\)-value indicates that the DID model provides a better fit.\n\n\nCounterfactual\nEstimating the counterfactual is pretty straightforward. Here, it is the expected value of the treatment group without the DID effect, or in this case, the intercept plus the time effect plus the treatment effect.\nWe’ll now extract this, plus the estimate of the factual and the control group for plotting. As we only have two values for Time ([0,1]), we can simply work by adding together the model coefficients from the full DID model:\n\\[\nY = \\beta_0 + \\beta_1 T + \\beta_2 D + \\beta_3 D\\times T\n\\]\n\nControl at time 0: \\(\\beta_0\\)\nControl at time 1: \\(\\beta_0 + \\beta_1\\)\nTreatment at time 0: \\(\\beta_0 + \\beta_2\\)\nTreatment at time 1 (factual): \\(\\beta_0 + \\beta_2 + \\beta_1 + \\beta_3\\)\nTreatment at time 1 (counterfactual): \\(\\beta_0 + \\beta_2 + \\beta_1\\)\n\n\nRPython\n\n\nFirst extract the model coefficients:\n\ndid_coefs = coef(fit1)\ndid_coefs\n\n (Intercept)        Time1       Treat1 Time1:Treat1 \n   49804.985    42702.910     4752.228    11025.252 \n\n\nNow, we’ll make up vectors of estimates for the control, treatment, and the treatment with the counterfactual estimate at time = 1:\n\nyhat_control = c(did_coefs[1], did_coefs[1] + did_coefs[2])\nyhat_treatment = c(did_coefs[1] + did_coefs[3], \n                   did_coefs[1] + did_coefs[3] + did_coefs[2] + did_coefs[4])\nyhat_cf = c(did_coefs[1] + did_coefs[3], \n            did_coefs[1] + did_coefs[3] + did_coefs[2])\n\nCreate a data frame:\n\nplot_df = data.frame(Label = factor(rep(c(\"Control\", \"Treat\", \"CF\"), each = 2), \n                                    levels = c(\"Control\", \"Treat\", \"CF\")),\n                     Time = rep(c(0,1), 3),\n                     yhat = c(yhat_control, yhat_treatment, yhat_cf))\n\nAnd now plot:\n\nggplot(plot_df, aes(x = Time, y = yhat, col = Label)) +\n  geom_line(size = 2) +\n  scale_color_jco() +\n  theme_bw() +\n  theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\n\ndid_coefs = fit1.params.tolist()\ndid_coefs\n\n[50229.645722472276, 43211.59793472184, 5261.699019136594, 9720.534537801836]\n\n\n\nyhat_control = [did_coefs[0], did_coefs[0] + did_coefs[1]]\nyhat_treatment = [did_coefs[0] + did_coefs[2], \n                   did_coefs[0] + did_coefs[2] + did_coefs[1] + did_coefs[3]]\nyhat_cf = [did_coefs[0] + did_coefs[2], \n            did_coefs[0] + did_coefs[2] + did_coefs[1]]\n\n\npred_df = pd.DataFrame({'Label': np.repeat([\"Control\", \"Treat\", \"CF\"], 2),\n                        'Time': np.resize([0,1], 6),\n                        'yhat': np.concatenate([yhat_control, yhat_treatment, yhat_cf])\n                         })\n\n\nsns.lineplot(pred_df, x = \"Time\", y = \"yhat\", hue = \"Label\")\n\n\n\n\n\n\n\n\n\n\n\nThe DID effect can be seen here clearly as the difference in the treatment and CF estimates at time = 1."
  },
  {
    "objectID": "Week3.html",
    "href": "Week3.html",
    "title": "Week 3: ITS and DID",
    "section": "",
    "text": "Also need scipy and the statsmodel\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nnp.random.seed(42)\nT = np.arange(365)\n\nD = np.where(T &gt; 200, 1, 0)\nP = T - 200\nP = np.where(T &lt;= 200, 0, P)\n\nY = 5.4 + 0.5 * T + 20 * D + 1.2 * P\nY\n\narray([  5.4,   5.9,   6.4,   6.9,   7.4,   7.9,   8.4,   8.9,   9.4,\n         9.9,  10.4,  10.9,  11.4,  11.9,  12.4,  12.9,  13.4,  13.9,\n        14.4,  14.9,  15.4,  15.9,  16.4,  16.9,  17.4,  17.9,  18.4,\n        18.9,  19.4,  19.9,  20.4,  20.9,  21.4,  21.9,  22.4,  22.9,\n        23.4,  23.9,  24.4,  24.9,  25.4,  25.9,  26.4,  26.9,  27.4,\n        27.9,  28.4,  28.9,  29.4,  29.9,  30.4,  30.9,  31.4,  31.9,\n        32.4,  32.9,  33.4,  33.9,  34.4,  34.9,  35.4,  35.9,  36.4,\n        36.9,  37.4,  37.9,  38.4,  38.9,  39.4,  39.9,  40.4,  40.9,\n        41.4,  41.9,  42.4,  42.9,  43.4,  43.9,  44.4,  44.9,  45.4,\n        45.9,  46.4,  46.9,  47.4,  47.9,  48.4,  48.9,  49.4,  49.9,\n        50.4,  50.9,  51.4,  51.9,  52.4,  52.9,  53.4,  53.9,  54.4,\n        54.9,  55.4,  55.9,  56.4,  56.9,  57.4,  57.9,  58.4,  58.9,\n        59.4,  59.9,  60.4,  60.9,  61.4,  61.9,  62.4,  62.9,  63.4,\n        63.9,  64.4,  64.9,  65.4,  65.9,  66.4,  66.9,  67.4,  67.9,\n        68.4,  68.9,  69.4,  69.9,  70.4,  70.9,  71.4,  71.9,  72.4,\n        72.9,  73.4,  73.9,  74.4,  74.9,  75.4,  75.9,  76.4,  76.9,\n        77.4,  77.9,  78.4,  78.9,  79.4,  79.9,  80.4,  80.9,  81.4,\n        81.9,  82.4,  82.9,  83.4,  83.9,  84.4,  84.9,  85.4,  85.9,\n        86.4,  86.9,  87.4,  87.9,  88.4,  88.9,  89.4,  89.9,  90.4,\n        90.9,  91.4,  91.9,  92.4,  92.9,  93.4,  93.9,  94.4,  94.9,\n        95.4,  95.9,  96.4,  96.9,  97.4,  97.9,  98.4,  98.9,  99.4,\n        99.9, 100.4, 100.9, 101.4, 101.9, 102.4, 102.9, 103.4, 103.9,\n       104.4, 104.9, 105.4, 127.1, 128.8, 130.5, 132.2, 133.9, 135.6,\n       137.3, 139. , 140.7, 142.4, 144.1, 145.8, 147.5, 149.2, 150.9,\n       152.6, 154.3, 156. , 157.7, 159.4, 161.1, 162.8, 164.5, 166.2,\n       167.9, 169.6, 171.3, 173. , 174.7, 176.4, 178.1, 179.8, 181.5,\n       183.2, 184.9, 186.6, 188.3, 190. , 191.7, 193.4, 195.1, 196.8,\n       198.5, 200.2, 201.9, 203.6, 205.3, 207. , 208.7, 210.4, 212.1,\n       213.8, 215.5, 217.2, 218.9, 220.6, 222.3, 224. , 225.7, 227.4,\n       229.1, 230.8, 232.5, 234.2, 235.9, 237.6, 239.3, 241. , 242.7,\n       244.4, 246.1, 247.8, 249.5, 251.2, 252.9, 254.6, 256.3, 258. ,\n       259.7, 261.4, 263.1, 264.8, 266.5, 268.2, 269.9, 271.6, 273.3,\n       275. , 276.7, 278.4, 280.1, 281.8, 283.5, 285.2, 286.9, 288.6,\n       290.3, 292. , 293.7, 295.4, 297.1, 298.8, 300.5, 302.2, 303.9,\n       305.6, 307.3, 309. , 310.7, 312.4, 314.1, 315.8, 317.5, 319.2,\n       320.9, 322.6, 324.3, 326. , 327.7, 329.4, 331.1, 332.8, 334.5,\n       336.2, 337.9, 339.6, 341.3, 343. , 344.7, 346.4, 348.1, 349.8,\n       351.5, 353.2, 354.9, 356.6, 358.3, 360. , 361.7, 363.4, 365.1,\n       366.8, 368.5, 370.2, 371.9, 373.6, 375.3, 377. , 378.7, 380.4,\n       382.1, 383.8, 385.5, 387.2, 388.9, 390.6, 392.3, 394. , 395.7,\n       397.4, 399.1, 400.8, 402.5, 404.2])\nplt.plot(Y)\nerr = np.random.normal(0, 50, 365)\nY = Y + err\nplt.plot(Y, 'bo')\n#well_df &lt;- as.data.frame(cbind(Y, T, D, P)) \nwell_df = pd.DataFrame({'Y': Y,\n                        'T': T,\n                        'D': D,\n                        'P': P})\nsns.scatterplot(well_df,\n                x = \"T\",\n                y = \"Y\", \n                alpha = 0.75)\nplt.axvline(x=200)\nmod = smf.ols(formula='Y ~ T', data=well_df)\nfit0 = mod.fit()\nprint(fit0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.817\nModel:                            OLS   Adj. R-squared:                  0.816\nMethod:                 Least Squares   F-statistic:                     1618.\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          8.38e-136\nTime:                        15:11:26   Log-Likelihood:                -1986.9\nNo. Observations:                 365   AIC:                             3978.\nDf Residuals:                     363   BIC:                             3986.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -53.6683      5.862     -9.155      0.000     -65.196     -42.141\nT              1.1211      0.028     40.218      0.000       1.066       1.176\n==============================================================================\nOmnibus:                        0.840   Durbin-Watson:                   1.519\nProb(Omnibus):                  0.657   Jarque-Bera (JB):                0.939\nSkew:                          -0.068   Prob(JB):                        0.625\nKurtosis:                       2.792   Cond. No.                         420.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nmod = smf.ols(formula='Y ~ T + D + P', data=well_df)\nfit1 = mod.fit()\nprint(fit1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.870\nModel:                            OLS   Adj. R-squared:                  0.869\nMethod:                 Least Squares   F-statistic:                     803.5\nDate:                Fri, 06 Sep 2024   Prob (F-statistic):          2.35e-159\nTime:                        15:11:26   Log-Likelihood:                -1924.6\nNo. Observations:                 365   AIC:                             3857.\nDf Residuals:                     361   BIC:                             3873.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4776      6.667     -0.372      0.710     -15.588      10.632\nT              0.5594      0.058      9.701      0.000       0.446       0.673\nD             22.9193      9.991      2.294      0.022       3.271      42.568\nP              1.0990      0.097     11.308      0.000       0.908       1.290\n==============================================================================\nOmnibus:                        4.812   Durbin-Watson:                   2.136\nProb(Omnibus):                  0.090   Jarque-Bera (JB):                5.520\nSkew:                           0.134   Prob(JB):                       0.0633\nKurtosis:                       3.539   Cond. No.                         908.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nsm.stats.anova_lm(fit0, fit1)\n\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n363.0\n1.142920e+06\n0.0\nNaN\nNaN\nNaN\n\n\n1\n361.0\n8.122294e+05\n2.0\n330690.71979\n73.488689\n1.679349e-27\nwell_df['yhat'] = fit1.predict()\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(well_df, x = \"T\", y = \"yhat\", ax=ax, color = \"darkorange\", linewidth=5)\nfit1.params\n\nIntercept    -2.477644\nT             0.559382\nD            22.919297\nP             1.098989\ndtype: float64\nb0 = fit1.params['Intercept']\nb1 = fit1.params['T']\nb2 = fit1.params['D']\nb3 = fit1.params['P']\npost_time = 20\nb0 + b1 * (200 + post_time) + b2 + b3 * post_time\n\nnp.float64(165.4855648977787)\nb0 + b1 * (200 + post_time) \n\nnp.float64(120.58648318541084)\nT_pred = np.concatenate([T, T])\nD_pred = np.concatenate([D, np.repeat(0, len(D))])\nP_pred = np.concatenate([P, np.repeat(0, len(P))])\nnp.repeat([0,1,2,3],2)\n\narray([0, 0, 1, 1, 2, 2, 3, 3])\npred_df = pd.DataFrame({'T': T_pred,\n                        'D': D_pred,\n                        'P': P_pred\n})\npred_df.head()\npred_df['yhat'] = fit1.predict(pred_df)\nplt.plot(pred_df['T'], 'bo')\nfig, ax = plt.subplots()\nsns.scatterplot(well_df, x = \"T\", y = \"Y\", \n                alpha = 0.75, ax=ax)\nplt.axvline(x=200)\nsns.lineplot(pred_df, x = \"T\", y = \"yhat\", ax=ax, hue = \"D\", linewidth=5)"
  },
  {
    "objectID": "Week3.html#did",
    "href": "Week3.html#did",
    "title": "Week 3: ITS and DID",
    "section": "DID",
    "text": "DID\n\nTime = np.resize([0,1], 1000)\nTreat = np.resize([0,0,1,1], 1000)\n\ny = 50000 + 5000 * Treat + 43000 * Time + 10000 * Treat * Time\n\ne = np.random.normal(0, 10000, 1000)\ny = y + e\n\nhouse_df = pd.DataFrame({'Price': y,\n                         'Treat': Treat,\n                         'Time': Time\n                         })\n\n\nf, ax = plt.subplots()\nsns.stripplot(house_df, x = 'Time', y = 'Price', hue = 'Treat', alpha = 0.25)\nsns.pointplot(house_df, x = 'Time', y = 'Price', hue = 'Treat')\n\n\n\n\n\n\n\n\n\nn_schools = 50\nclass_size = 20\nn_students = n_schools * class_size\n\n\ntuition = np.round(np.random.normal(1000, 300, n_schools))\ntuition\n\n\ntuition.std()\n\n\ntuition_z = (tuition - tuition.mean()) / tuition.std()\ntuition_p = np.exp(tuition_z)/(1+np.exp(tuition_z))\ntablet = np.random.binomial(1, tuition_p, n_schools)\n\n\ntuition_p\n\n\ntablet\n\n\nnp.arange(n_schools)\n\n\n# school_df = data.frame(id = as.factor(1:length(tablet)), \n#                        tuition = tuition, \n#                        tablet = as.factor(tablet))\n\n\nschool_df = pd.DataFrame({'id': np.arange(n_schools), \n                          'tuition': tuition,\n                          'tablet': tablet})\n\n\nsns.boxplot(school_df, x=\"tablet\", y=\"tuition\", hue=\"tablet\")\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet\", order=school_df.sort_values('tuition').id)\n\n\n## Make up students\n\nstudent_df = pd.DataFrame({'id': np.arange(n_students),\n                           'school_id': np.repeat(school_df['id'], class_size),\n                           'tuition': np.repeat(school_df['tuition'], class_size),\n                           'tablet': np.repeat(school_df['tablet'], class_size)})\n\n\nstudent_df['enem_score0'] = np.random.normal(200 + 0.7 * student_df['tuition'], 200, n_students) \nstudent_df['enem_score0'] = (student_df['enem_score0'] - student_df['enem_score0'].min()) / student_df['enem_score0'].max() * 1000.0\n\n#student_df$enem_score0 =\n#  (student_df$enem_score0 - min(student_df$enem_score0)) /\n#  max(student_df$enem_score0) * 1000\n\n\nstudent_df['tablet_eff'] = np.random.normal(-50, 5, n_students)\nsns.histplot(student_df, x = \"tablet_eff\")\n\n\nstudent_df['enem_score1'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet']\n\n\nsns.boxplot(student_df, x = \"tablet\", y = \"enem_score1\")"
  },
  {
    "objectID": "Week3.html#first-test",
    "href": "Week3.html#first-test",
    "title": "Week 3: ITS and DID",
    "section": "First test",
    "text": "First test\n\nstudent_df['enem_score1'][student_df['tablet'] == 1]\n\n\n#from scipy import stats\nfrom statsmodels.stats.weightstats import ttest_ind\nt_stat, p_value, df = ttest_ind(student_df[student_df['tablet'] == 1]['enem_score1'], \n                                student_df[student_df['tablet'] == 0]['enem_score1'])\n\n\nprint(f'P-value: {p_value}')\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score1\", \n          hue = \"tablet\")"
  },
  {
    "objectID": "Week3.html#rct",
    "href": "Week3.html#rct",
    "title": "Week 3: ITS and DID",
    "section": "RCT",
    "text": "RCT\n\nschool_df['tablet_rct'] = school_df['tablet'].sample(n_schools).to_numpy()\n\n\nsns.barplot(school_df, x=\"id\", y=\"tuition\", \n            hue=\"tablet_rct\", order=school_df.sort_values('tuition').id)\n\n\nstudent_df['tablet_rct'] = np.repeat(school_df['tablet_rct'], class_size)\nstudent_df['enem_score2'] = student_df['enem_score0'] + student_df['tablet_eff'] * student_df['tablet_rct']\n\n\nsns.scatterplot(student_df, \n          x = \"tuition\", \n          y = \"enem_score2\", \n          hue = \"tablet_rct\")\n\n\nmod = smf.ols(formula='enem_score2 ~ tablet_rct', data=student_df)\nfit = mod.fit()\nprint(fit.summary())"
  },
  {
    "objectID": "Week3.html#psm",
    "href": "Week3.html#psm",
    "title": "Week 3: ITS and DID",
    "section": "PSM",
    "text": "PSM\n\nmod = smf.glm(formula='tablet ~ tuition', data=school_df, family=sm.families.Binomial())\nfit = mod.fit()\nprint(fit.summary())\n\n\nfit.predict()\n\n\nprs_df = pd.DataFrame({'prop_score': fit.predict(),\n                        'tablet': school_df['tablet'],\n                        'tuition': school_df['tuition']})\nprs_df.head()\n\n\ntreated_df = prs_df[prs_df['tablet'] == 1].reset_index()\ncontrol_df = prs_df[prs_df['tablet'] == 0].reset_index()\n\n\ntreated_df.iloc[0,:]\n\n\nabs_diff = (treated_df['prop_score'][0] - control_df['prop_score']).abs()\nmatch_id = abs_diff.idxmin()\nprint(match_id)\n\n\ncontrol_df.iloc[match_id,:]\n\n\nschool_df.columns\n\n\nfrom psmpy import PsmPy\npsm = PsmPy(school_df, treatment='tablet', indx='id', exclude = ['tablet_rct'])\n\n\npsm.logistic_ps(balance = False)\n\n\npsm.predicted_data.head()\n\n\npsm.knn_matched(matcher='propensity_logit', replacement=False, caliper=None)\n\n\npsm.plot_match()\n\n\npsm.effect_size_plot(save=False)\n\n\npsm.knn_matched(matcher='propensity_logit', replacement=True, caliper=None)\n\n\npsm.effect_size_plot(save=False)\n\n\npsm.effect_size\n\n\npsm.df_matched\n\n\nfig, axs = plt.subplots(ncols=2)\nsns.histplot(school_df, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[0]).set(title='Before')\nsns.histplot(psm.df_matched, x=\"tuition\", hue=\"tablet\", binwidth=100, ax=axs[1]).set(title='After')\n\n\nfig, axs = plt.subplots(ncols=2)\nsns.ecdfplot(school_df, x = \"tuition\", hue=\"tablet\", ax=axs[0]).set(title='Before')\nsns.ecdfplot(psm.df_matched, x = \"tuition\", hue=\"tablet\", ax=axs[1]).set(title='After')\n\n\nmatch_df = psm.df_matched\n\n\nmatch_df.head()\n\n\nstudent_df.columns\n\n\nmatched_student_df = pd.DataFrame(columns=student_df.columns)\nfor idx, row in match_df.iterrows():\n    print(row['id'])\n    tmp_df = student_df[student_df['school_id'] == row['id']]\n    matched_student_df = pd.concat([matched_student_df, tmp_df], ignore_index = True)\n\n\nmatched_student_df\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=matched_student_df)\nfit = mod.fit()\nprint(fit.summary())"
  },
  {
    "objectID": "Week3.html#ipw",
    "href": "Week3.html#ipw",
    "title": "Week 3: ITS and DID",
    "section": "IPW",
    "text": "IPW\n\nprs_df\n\n\nprs_df['ipw'] = (prs_df['tablet'] / prs_df['prop_score']) + ((1 - prs_df['tablet']) / (1 - prs_df['prop_score']))\n\n\nprs_df\n\n\nsns.scatterplot(prs_df, x = \"prop_score\", y = \"ipw\", hue = \"tablet\")\n\n\nstudent_df['ipw'] = np.repeat(prs_df['ipw'], class_size)\n\n\nstudent_df\n\n\nmod = smf.wls(formula='enem_score1 ~ tablet', data=student_df, weights=student_df['ipw'])\nfit = mod.fit()\nprint(fit.summary())\n\n\nmod = smf.ols(formula='enem_score1 ~ tablet', data=student_df)\nfit = mod.fit()\nprint(fit.summary())"
  }
]